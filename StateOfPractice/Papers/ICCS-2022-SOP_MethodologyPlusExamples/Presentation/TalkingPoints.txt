Digging Deeper into the State of the Practice for Domain Specific Research Software

01. Title Slide
	- thank you for your attention, aim to be done on time
	- need to understand state of the practice to improve the practice
	- focus on specific domains, since helps the chance of having an impact

02. Outline
	- go through RQs
	- measure software through template and through interviews, then compare to other research software, including software guidelines for artifacts, tools, process, pain points

03. Motivation
	- continuum of different ways to assess from surveys to case studies, general populations, specific populations
	- we focus not on surveys, but on manual/automated mining of repos
	- interview developers
	- one domain at a time

04. LBM Running Example
	- easier to explain with an example
	- example for blood flow in an aorta with a coarctation an aneurism, pre and post treatment
	- LBM solvers consider the behaviour of a collection of particles as a single unit at the mesoscopic scale - predict the positional probability of a collection of particles moving through a lattice structure following a two-step process: i) streaming, ii) colliding

05. Methodology
	- identify software
	- measurements template
	- AHP for pair wise comparison, usability of package A versus usability of package B
	- interviews
	- domain analysis

06. Identifying Software (RQ1)
	- emphasize domain expert

07. Domain Analysis
	- by commonalities and variabilities
	- help select correct software
	- dimension, parallel, compressible, reflexive boundary, multi-fluid, turbulence, complex geom

08. Measurement Template
	- by qualities: installability, correctness, reliability, robustness, performance, usability, maintainability, reusability, understandability, visibility, reproducibility
	- some measures are shallow by necessity
	- installation instructions
	- linear instructions
	- number of steps
	- what artifacts
	- what tools
	- response to unexpected inputs
	- percentage of code comments
	- number of code files
	- etc.

09. Ranking By Best Practices (RQ2)
	- pairwise comparison on each quality
	- equal weight of qualities for overall ranking (threat to validity)

10. Compare to Community Ranking (RQ3)
	- highly ranked by our method tend to have more stars/watches
	- our highest ranked has the second most stars
	- ninth ranked has highest number of stars

11. Compare Artifacts to Recommendations: List of Guidelines 
	- not an exhaustive list of software guidelines, but believe representative

12. Compare to Guidelines (RQ4) (draw on screen)
	- top packages have most of the common and uncommon artifacts
	- often missing: code of conduct, style guide, roadmap, API documentation, requirements

13. Tools (RQ5)
	- say how find
	- focus on version control and CI in particular

14. Processes (RQ6)
	- literature suggestions
	- observations for LBM

15. Pain Points (RQ7, RQ8)
	- matches pain points from literature

16. SOP for LBM (would like some time on this slide)
	- design for change - modules that encapsulate likely changes 
	- oracle problem
		- simpler cases with analytical solutions
		- verify mathematical foundations via peer review
		- comparison to pseudo-oracles
	- prioritize doc and usability
		- came up multiple times in interviews
		- explicitly state software limitations
		- identify expected user characteristics
		- documentation generators
		- guards in code to test user input
	- more papers - JOSS, citable software
	- more contributors - advice from open source community
	- write theory manuals, but not requirements, add req info:
		- user characteristics
		- input data requirements
		- likely and unlikely changes
		- prioritize nonfunctional req
		- traceability info to show consequences of changing assumptions

17. Threats to Validity
	- follow slide as time permits

18. Concluding Remarks
	- follow slide as time permits