----------------------- REVIEW 3 ---------------------

The methodology is extremely weak and does not appear to be appropriate for the stated purpose. The methodology does not describe in sufficient detail what, where, when and how data were collected or how the data was analysed. NOT POSSIBLE TO ADDRESS IN SPACE AVAILABLE, SEE REFERENCES

The paper presents some results, which do not appear to directly address the research questions under investigation. DISAGREE

The paper does not summarize and synthesizes a reasonable selection of related work and there is no real attempt to clearly describes the relationship between the author's contribution(s) and related work. DONE (by addressing other reviewers comments)

While the paper discusses implications of the results and the study's limitations including threats to validity it is not clear that this is underpinned by a robust methodology. NOT POSSIBLE TO ADDRESS IN SPACE AVAILABLE, SEE REFERENCES

The paper states several conclusions but is not supported by explicit evidence since they are nothing valid to compare the results against. SEE REFERENCES

There is a lack of rigour in the development of their proposed methodology and its evaluation. In addition, the paper ignores established profiling tools such as SonarGraph or SonarCloud which provide a deep analysis of VCS information. ADDED LINKS and EXPLANATION FOR WHY EXCLUDED



----------------------- REVIEW 1 ---------------------

- The Abstract lacks the motivation for the problem and the implications of the results. DONE

- The Introduction section could be a lot better. It should have described the problem and motivation behind the solution clearly. In addition, since there is no Background/Related work section, the introduction section should have discussed some of the related works more to provide a foundation of the study. DONE

- Some of the research questions are vague. For example, the authors talked about "current best practices" without spelling out or citing current best practices, "compare to research software in general," without providing the characteristics of research software in general. It was unclear what was going to be comparedâ€”similarly, tools and pain points of research software in general. The paper should specifically talk about all these points with proper citation. -DONE

- The presentation of the results and the mapping between the research questions and findings could have been presented better. For example, while the answers to the other RQs have their own section, RQ1 and RQ2 are blended with the methodology section. DONE

- RQ8 should have been specifically called out in section 7. It was mentioned in the introduction section but never called out after that. DONE


----------------------- REVIEW 2 ---------------------

The mix of repository mining and developer interviews is a great approach. Ideally, it could be used to also cross-check the assessment. For example, it is stated that some interviews suggested that CI is more commonly used as the analysis of the repository predicted, but it is not stated if this was used to adapt the existing assessment of the software. DONE

Also mixing automated analysis with manual evaluation is a good approach. Since the required time for an assessment of a whole domain is still high, trying to introduce more automatic analysis tools should be a next step. DONE
	
Despite all this praise, I discovered one flaw: In "Table 3 - Recommended artifacts in software development guidelines" at least for [16] the table is not correct. LICENSE, README, CONTRIBUTING, and CHANGELOG are required by the guideline as well. While the file names are not all stated explicitly, it is implicitly stated that these artifacts needs to be provided. README and CONTRIBUTING are even mentioned explicitly as possible implementations for such artifacts. DONE

