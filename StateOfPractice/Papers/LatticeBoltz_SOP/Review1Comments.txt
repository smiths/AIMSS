Dear Dr. Smith,

We have received the reports from our advisors on your manuscript, "State of the Practice for Lattice Boltzmann Method Software", submitted to
Archives of Computational Methods in Engineering

Based on the advice received, I have decided that your manuscript can be accepted for publication after you have carried out the corrections as suggested by the reviewer(s).

Below, please find the reviewers' comments for your perusal. You are kindly requested to also check the website for possible reviewer attachment(s).

Please submit your revised manuscript online by using the Editorial Manager system.

Your username is: smiths 
If you forgot your password, you can click the 'Send Login Details' link on the EM Login page at https://www.editorialmanager.com/arco/

Please make sure to submit your editable source files (i. e. Word, TeX).

In order to add the due date to your electronic calendar, please open the attached file.

I am looking forward to receiving your revised manuscript before 10 Oct 2022.

Thank you very much.

With kind regards,

Eugenio OÃ±ate
Editor in Chief
Archives of Computational Methods in Engineering

COMMENTS TO THE AUTHOR:

Reviewer #1: This is an interesting, useful, thorough, and well-written paper.  

The anonymization of the authors poses some challenges in evaluating the paper because the citations for the methodology and the raw data underlying the study have been anonymized away.  Even though those references will be available to readers in the published version, I think it would be useful to have slightly more by way of explanation here -- to provide a bit more understanding of the process followed for readers who choose not to pursue the details provided in the reference.  Two key steps to put a little more flesh around would be (1) how are the entries  of the measurement template (Fig 2) turned into a single numerical score for the section, and (2) how is the aggregate AHP score obtained?  As to the first point, for example, I look at the Installability Overall Impression scores, which I initially took to be a numerical score for installability, but in the AHP Installability scores shown in Fig 3, there is no correlation between the number in the measurement
template and the final score.  Also, in Fig 2, I see responses for "Descriptive error messages?" of yes, no, and n/a.  I have a hard time imagining what n/a could mean in this context.  Presumably the installer either provides descriptive error messages or it does not?  Is this explained in one of the anonymized references or in the raw dataset, which I can't access either due to anonymization.  As to the second point, AHP is left completely to outside references, and I think it would be beneficial to have at least a brief description here.  I think there was a statement that AHP weights all of the qualities equally, but in the graphs of the scores for various qualities (e.g., Figs 3-6), there are dramatic differences in the ranges of the scores given to the packages -- anywhere from 0.06 as the top score to nearly 0.16.  The highest overall score is ~0.075 (Fig 12). So what is the *theoretical* top score in any category and overall?  Are there some categories in which even
the highest-scoring packages are quite low on the absolute scale?  If so, I would think such situations deserve some discussion.  I really would have liked to be able to look over the whole set of measurement template results and a thorough description of the scoring and AHP processes in order to better understand the paper and the approach.  That's partly my duty as a reviewer and partly my interest/curiosity about the results.  To be clear, I'm *not* penalizing the authors for following the journal's policy on double-blind review. And in principle, I applaud the idea.  But it is not without its challenges for the reviewer. I'm taking a bit of a leap of faith here. For future papers submitted for double-blind review, the authors might want to consider whether they could approach their presentation in a somewhat different way that might make it easier for a reviewer access more of the details of the study.  The authors also may wish to know that this journal's editorial
management system exposes the name of the corresponding author to the reviewer, so I'm not entirely blind to the authorship of the paper.

As to the rest of the paper, I think there are a lot of details that would make for interesting discussions and debates -- exactly how each of the qualities are evaluated, etc. But (particularly once the key references are visible), the approach appears to be fairly well defined, which should suffice for a paper like this.  The recommendations to improve software practices and address pain points are mostly reasonable.  I might quibble with some in small ways (for instance, the idea that domain-specific modeling effectively eliminates technical debt), but I think they're largely within the authors' license.  

I appreciate the thorough discussion of threats to validity of this study.

Overall, it is an interesting an useful paper, and I appreciate the work the authors have put into it.

__


