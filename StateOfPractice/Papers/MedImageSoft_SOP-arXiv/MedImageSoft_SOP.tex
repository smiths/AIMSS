%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with Harvard style bibliographic references

%\documentclass[preprint,12pt,authoryear]{elsarticle}
%\documentclass[3p, 12pt,authoryear]{elsarticle}
\documentclass[final, 3p, times, authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{amsmath,amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

%\usepackage{tikz}
%\usetikzlibrary{mindmap}
\usepackage{multirow}
\usepackage{pgfplots}
\usepackage{arydshln}
\usepackage{paralist}
\usepackage{booktabs}

%% Comments
\newif\ifcomments\commentstrue

\ifcomments
\newcommand{\authornote}[3]{\textcolor{#1}{[#3 ---#2]}}
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}
\else
\newcommand{\authornote}[3]{}
\newcommand{\todo}[1]{}
\fi

\newcommand{\wss}[1]{\authornote{blue}{SS}{#1}} %Spencer Smith
\newcommand{\jc}[1]{\authornote{red}{JC}{#1}} %Jacques Carette
\newcommand{\mn}[1]{\authornote{magenta}{MN}{#1}} %Mike Noseworthy
\newcommand{\ad}[1]{\authornote{cyan}{AD}{#1}} %Ao Dong

\newcommand{\notdone}[1]{\textcolor{red}{#1}}
\newcommand{\done}[1]{\textcolor{black}{#1}}

\newcounter{rqnum} %research question number
\newcommand{\rqtherqnum}{RQ`'\therqnum}
\newcommand{\rqref}[1]{RQ\ref{#1}}

\newcounter{pnum} %pain point number
\newcommand{\ppthepnum}{P`'\thepnum}
\newcommand{\ppref}[1]{P\ref{#1}}

\newcounter{qnum} %quality number
\newcommand{\qthepnum}{Q`'\theqnum}
\newcommand{\qref}[1]{Q\ref{#1}}

\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\small\bf
+}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\small\bf +}}

\journal{Academic Radiology}

\pgfplotsset{compat=1.13}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%            addressline={}, 
%%            city={},
%%            postcode={}, 
%%            state={},
%%            country={}}
%% \fntext[label3]{}

\title{State of the Practice for Medical Imaging Software}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author[CAS]{Spencer Smith}
\author[CAS]{Ao Dong}
\author[CAS]{Jacques Carette}
\author[ECE]{Mike Noseworthy}

\affiliation[CAS]{organization={McMaster University, Computing and Software Department},%Department and Organization
            addressline={1280 Main Street West}, 
            city={Hamilton},
            postcode={L8S 4K1}, 
            state={Ontario},
            country={Canada}}

\affiliation[ECE]{organization={McMaster University, Electrical Engineering},%Department and Organization
            addressline={1280 Main Street West}, 
            city={Hamilton},
            postcode={L8S 4K1}, 
            state={Ontario},
            country={Canada}}

\begin{abstract}

We present the state of the practice for Medical Imaging software. We selected
29 medical imaging projects from 48 candidates, assessed 10 software qualities
(installability, correctness/ verifiability, reliability, robustness, usability,
maintainability, reusability, understandability, visibility/transparency and
reproducibility) by answering 108 questions for each software project, and
interviewed 8 of the 29 development teams. Based on the quantitative data for
the first 9 qualities, we ranked the MI software with the Analytic Hierarchy
Process (AHP). The four top ranked software products are: \textit{3D Slicer},
\textit{ImageJ}, \textit{Fiji}, and \textit{OHIF Viewer}. Our ranking is mostly
consistent with the community's ranking, with four of our top five projects also
appearing in the top five of a list ordered by stars-per-year. \textit{DICOM
Viewer} and \textit{dmv} are exceptions; we ranked them lower than their
estimated popularity because of difficulty with their installation.  Comparison
for tool usage. Comparison for process. From interviewing the developers, we
identified five pain points and two qualities of potential concern:: i) lack of
resources; ii) difficulty balancing between compatibility, maintainability,
performance, and security; and, iii) lack of access to real-world datasets for
testing. For future MI software projects, we propose adopting test-driven
development, using continuous integration and continuous delivery (CI/CD), using
git and GitHub, maintaining good documentation, supporting third-party plugins
or extensions, considering web application solutions, and improving
collaboration between different MI software projects. \wss{Update after the
paper has been revised.}

\end{abstract}

%%Graphical abstract
%\begin{graphicalabstract}
%\includegraphics{grabs}
%\end{graphicalabstract}

%%Research highlights
%\begin{highlights}
%\item Research highlight 1
%\item Research highlight 2
%\end{highlights}

\begin{keyword}
	medical imaging, research software, software engineering, software
	quality, Analytic Hierarchy Process, developer interviews
\end{keyword}

\end{frontmatter}

%% \linenumbers

\section{Introduction} \label{ch_intro}

We aim to study the state of software development practice for Medical Imaging
(MI) software.  MI tools use images of the interior of the body (from sources
such as Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron
Emission Tomography (PET) and Ultrasound) to provide information for diagnostic,
analytic, and medical applications \citep{FDA2021, enwiki:1034887445,
Zhang2008}.  Figure~\ref{Fig_Example}, which shows an image of the brain,
highlights the importance and value of MI. Through MI medical practitioners and
researchers can noninvasively gain insights into the human body, including
information on injuries and illnesses. Given the importance of MI software and
the high number of competing software projects, we wish to understand the merits
and drawbacks of the current development processes, tools, and methodologies. We
aim to assess through a software engineering lens the quality of the existing
software with the hope of highlighting standout examples, understanding current
pain points and providing guidelines and recommendations for future development.

\begin{figure}[!ht]
    \begin{center}
        \includegraphics[scale=0.25]{figures/MPR.png}        
    \end{center}
    \caption{Example brain image showing a multi-planar reformat using Horos
	(free open-source medical imaging/DICOM viewer for OSX, based on OsiriX)}
    \label{Fig_Example}
\end{figure}
    
\subsection{Research Questions} \label{sec_motivation}

Not only do we wish to gain insight into the state of the practice for MI
software, we also wish to understand the development of research software in
general. We wish to understand the impact of the often cited gap, or chasm,
between software engineering and research software \citep{Kelly2007,
Storer2017}. Although scientists spend a substantial proportion of their working
hours on software development \citep{Hannay2009, Prabhu2011}, many developers
learn software engineering skills by themselves or from their peers, instead of
from proper training \citep{Hannay2009}. \citet{Hannay2009} observe that many
scientists showed ignorance and indifference to standard software engineering
concepts. For instance, according to a survey by \citet{Prabhu2011}, more than
half of their 114 subjects did not use a proper debugger when coding.

To gain insights, we devised 10 research questions, which can be applied to MI,
as well as to other domains, of research software \citep{SmithEtAl2021,
SmithAndMichalski2022}.  We designed the questions to learn about the
community's interest in, and experience with, software artifacts, tools,
principles, processes, methodologies, and qualities.  When we mention artifacts
we mean the documents, scripts and code that constitutes a software development
project. Example artifacts include requirements, specifications, user manuals,
unit tests, system tests, usability tests, build scripts, API (Application
Programming Interface) documentation, READMEs, license documents, process
documents, and code.  Once we have learned what MI developers do, we then put
this information in context by contrasting MI software against the trends shown
by developers in other research software communities.  Our aim is to collect
enough information to understand the current pain points experienced by the MI
software development community so that we can make some preliminary
recommendations for future improvements. 

We based the structure of the paper on the research questions, so for each
research question below we point to the section that contains our answer.  We
start with identifying the relevant examples of MI software for the assessment
exercise:

\begin{enumerate}
	\item[RQ\refstepcounter{rqnum}\therqnum \label{RQ_WhatProjects}:] What MI
	software projects exist, with the constraint that the source code must be
	available for all identified projects? (Section~\ref{ch_results})
	\item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_HighestQuality}:] Which
	of the projects identified in \rqref{RQ_WhatProjects} follow current best
	practices, based on evidence found by experimenting with the software and
	searching the artifacts available in each project's repository?
	(Section~\ref{ch_results})
	\item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_CompareHQ2Popular}:] How
	similar is the list of top projects identified in \rqref{RQ_HighestQuality}
	to the most popular projects, as viewed by the scientific community?
	(Section~\ref{Sec_VsCommunityRanking})
    \item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_CompareArtifacts}:] How
	do MI projects compare to research software in general with respect to the
	artifacts present in their repositories?
	(Section~\ref{Sec_CompareArtifacts})
	\item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_CompareToolsProjMngmnt}:]
	How do MI projects compare to research software in general with respect to
	the use of tools (Section~\ref{Sec_CompareTools}) for:
	\begin{enumerate} 
		\item [\rqref{RQ_CompareToolsProjMngmnt}.a] development; and,
		\item [\rqref{RQ_CompareToolsProjMngmnt}.b] project management?
	\end{enumerate}
	\item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_CompareMethodologies}:]
	How do MI projects compare to research software in general with respect to
	principles, processes, and methodologies used?
	(Section~\ref{Sec_CompareMethodologies})
	\item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_PainPoints}:] What are
	the pain points for developers working on MI software projects?
	(Section~\ref{painpoints})
	\item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_ComparePainPoints}:] How
	do the pain points of developers from MI compare to the pain points
	for research software in general? (Section~\ref{painpoints})
	\item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_Concerns}:] For MI
	developers what specific best practices are taken to address the pain points
	and software quality concerns? (Section~\ref{painpoints})
	\item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_Recommend}:]
	What research software development practice could potentially address the
	pain point concerns identified in \rqref{RQ_PainPoints}?
	(Section~\ref{ch_recommendations})

\end{enumerate}

\subsection{Scope} \label{sec_scope}

To make the project feasible, we only cover MI visualization software.  As a
consequence we are excluding many other categories of MI software, including
Segmentation, Registration, Visualization, Enhancement, Quantification,
Simulation, plus MI archiving and telemedicine systems (Compression, Storage,
and Communication) (as summarized by \citet{Bankman2000} and
\citet{Angenent2006}).  We also exclude Statistical Analysis and Image-based
Physiological Modelling \citep{enwiki:1034877594} and Feature Extraction,
Classification, and Interpretation \citep{Kim2011}. Software that provides MI
support functions is also out of scope; therefore, we have not assessed the
toolkit libraries VTK \citep{SchroederEtAl2006} and ITK \citep{McCormick2014}.
Finally, Picture Archiving and Communication System (PACS), which helps users to
economically store and conveniently access images \citep{Choplin1992}, are
considered out of scope. 

\subsection{Methodology Overview}

We designed a general methodology to assess the state of the practice for
research software \citep{SmithEtAl2021, SmithAndMichalski2022}. Details can be
found in Section~\ref{ch_methods}.  Our methodology has been applied to MI
software \citep{Dong2021} and Lattice Boltzmann Solvers \citep{SmithEtAl2022,
Michalski2021}.  This methodology builds off prior work to assess the state of
the practice for such domains as Geographic Information Systems
\citep{smith2018state}, Mesh Generators \citep{smith2016state}, Seismology
software \citep{Smith2018Seismology}, and Statistical software for psychology
\citep{smith2018statistical}.  In keeping with the previous methodology, we have
maintained the constraint that the work load for measuring a given domain should
be feasible for a team as small as one person, and for a short time, ideally
around a person month of effort. We consider a person month as $20$ working days
($4$ weeks in a month, with $5$ days of work per week) at $8$ person hours per
day, or $20 \times 8 = 160$ person hours.

With our methodology, we first choose a research software domain (in the current
case MI) and identify a list of about 30 software packages. (For measuring MI we
used 29 software packages.)  We approximately measure the qualities of each
package by filling in a grading template. Compared with our previous
methodology, the new methodology also includes repository based metrics, such as
the number of files, number of lines of code, percentage of issues that are
closed, etc.  With the quantitative data in the grading template, we rank the
software with the Analytic Hierarchy Process (AHP) (Section~\ref{ch_background}
provides details). After this, as another addition to our previous methodology,
we interview some development teams to further understand the status of their
development process.

\section{Background} \label{ch_background}

To measure the existing MI software we need two sets of definitions: i) the
definitions of relevant software license models (Section
\ref{sec_software_categories}); and, ii) the definitions of the software
qualities that we will be assessing (Section \ref{sec_software_quality}). In our
assessment we rank the software packages for each quality; therefore, this
section also provides the background on our ranking process --- the Analytic
Hierarchy Process (Section \ref{sec_AHP}).

\subsection{Software Categories} \label{sec_software_categories}

When assessing software packages, we need to know the software's license.  In
particular, we need to know whether the source code will be available to us or
not.  We define three common software categories.  We will only assess software
that fits under the Open Source Software license.

\begin{itemize}

\item \textbf{Open Source Software (OSS)} For OSS, the source code is openly
accessible. Users have the right to study, change and distribute it under a
license granted by the copyright holder. For many OSS projects, the development
process relies on the collaboration of different contributors worldwide
\citep{Corbly2014}. Accessible source code usually exposes more ``secrets'' of a
software project, such as the underlying logic of software functions, how
developers achieve their works, and the flaws and potential risks in the final
product. Thus, OSS is suitable for researchers analyzing the qualities of a
project.

\item \textbf{Freeware} Freeware is software that can be used free of charge.
Unlike OSS, the authors of do not allow access or modify the source code
\citep{LINFO2006}. To many end-users, the differences between freeware and OSS
may not be relevant. However, software developers who wish to modify the source
code, and researchers looking for insight into software development process may
find the inaccessible source code a problem. 

\item \textbf{Commercial Software} ``Commercial software is software developed
by a business as part of its business'' \citep{GNU2019}. Typically speaking,
commercial software requires users to pay to access all of its features,
excluding access to the source code. However, some commercial software is also
free of charge \citep{GNU2019}. Based on our experience, most commercial
software products are not OSS.

\end{itemize}

\subsection{Software Quality Definitions} \label{sec_software_quality}

Quality is defined as a measure of the excellence or worth of an entity.  As is
common practice, we do not think of quality as a single measure, but rather as a
set of measures.  That is, quality is a collection of different qualities, often
called ``ilities.''  Below we list the 10 qualities of interest for this study.
The order of the qualities follows the order used in \citet{GhezziEtAl2003},
which puts related qualities (like correctness and reliability) together.
Moreover, the order is roughly the same as the order developers consider
qualities in practice.

\begin{itemize}
	\item \textbf{Installability} The effort required for the installation
    and/or uninstallation of software in a specified environment
    \citep{ISO/IEC25010, lenhard2013measuring}.

	\item \textbf{Correctness \& Verifiability} A program is correct if it
    matches its specification \citep[p.\ 17]{GhezziEtAl2003}.  The specification
    can either be explicitly or implicitly stated.  The related quality of
    verifiability is the ease with which the software components or the
    integrated product can be checked to demonstrate its correctness. 

	\item \textbf{Reliability} The probability of failure-free operation of a
	computer program in a specified environment for a specified time \citep{musa1987software}, \citep[p.\ 357]{GhezziEtAl2003}.

	\item \textbf{Robustness} Software possesses the characteristic of
	robustness if it behaves ``reasonably'' in two situations: i) when it
	encounters circumstances not anticipated in the requirements specification,
	and ii) when users violate the assumptions in its requirements specification 
	\citep[p.\ 19]{GhezziEtAl2003}, \citep{boehm2007software}.

	\item \textbf{Usability} ``The extent to which a product can be used by
	specified users to achieve specified goals with effectiveness, efficiency,
	and satisfaction in a specified context of use'' \citep{ISO/TR16982:2002}
	\citep{ISO9241-11:2018}.

	\item \textbf{Maintainability} The effort with which a software system or
	component can be modified to i) correct faults; ii) improve performance or
	other attributes; iii) satisfy new requirements
	\citep{IEEEStdGlossarySET1990}, \citep{boehm2007software}.

	\item \textbf{Reusability} ``The extent to which a software component can be
	used with or without adaptation in a problem solution other than the one for
	which it was originally developed'' \citep{kalagiakos2003non}.

	\item \textbf{Understandability} ``The capability of the software product to
	enable the user to understand whether the software is suitable, and how it
	can be used for particular tasks and conditions of use'' \citep{iso2001iec}.

	\item \textbf{Visibility/Transparency} The extent to which all the steps
	of a software development process and the current status of it are conveyed
	clearly \citep[p.\ 32]{GhezziEtAl2003}.

	\item \textbf{Reproducibility} ``A result is said to be reproducible if
	another researcher can take the original code and input data, execute it,
	and re-obtain the same result'' \citep{BenureauAndRougier2017}.
\end{itemize}

\subsection{Analytic Hierarchy Process (AHP)} \label{sec_AHP}

Saaty developed AHP in the 1970s, and people have widely used it since to make
and analyze multiple criteria decisions \citep{VaidyaEtAl2006}. AHP organizes
multiple criteria in a hierarchical structure and uses pairwise comparisons
between alternatives to calculate relative ratios \citep{Saaty1990}. AHP works
with sets of $n$ \textit{options} and $m$ \textit{criteria}.  In our project
$n=29$ and $m=9$ since there are 29 options (software products) and 9 criteria
(qualities). We rank the software for each of the qualities, and then we combine
the quality rankings into an overall ranking based on the relative priorities
between qualities.

The first step for ranking the software choices for a given quality involves a
pairwise comparison between each of the $n$ software options for that quality.
AHP expresses the comparison through an $n \times n$ matrix $A$. When comparing
option $i$ and option $j$, the value of $A_{ij}$ is decided as follows, with the
value of $A_{ji}$ generally equal to $1/A_{ij}$ \citep{Saaty1990}: $A_{ij} = 1$
if criterion $i$ and criterion $j$ are equally important, while $A_{ij} = 9$ if
criterion $i$ is extremely more important than criterion $j$.  The natural
numbers between 1 and 9 are used to show the different levels of relative
importance between these two extremes. The above assumes that option $i$ is of
equal, or more, importance compared to option $j$ ($i \geq j$).  If that is not
the case, we reverse $i$ and $j$ and determine $A_{ji}$ first, then $A_{ij} =
1/A_{ji}$.

Section~\ref{sec_grading_software} shows how we measure the software via a
grading template.  For the AHP process, the relevant measure is the subjective
score from $1$ to $10$ for each quality for each package. To turn these
subjective measures $x_{\text{sub}}$ and $y_{\text{sub}}$ into Saaty's
pair-wise scores for option $x$ versus option $y$, respectively, we use the
following calculation:
\[
\begin{cases}
\min\{9, x_{\text{sub}} - y_{\text{sub}} + 1\} & x_{\text{sub}} \geq y_{\text{sub}} \\
1 / \min\{9, y_{\text{sub}} - x_{\text{sub}} + 1\} & x_{\text{sub}} < y_{\text{sub}}
\end{cases}
\]

\noindent For example, we measured the usability for 3D Slicer and Ginkgo CADx
as $8$ and $7$, respectively; therefore, on the 9-point scale, 3D Slicer compared
to Ginkgo CADx is 2 and Ginkgo CADx versus 3D Slicer is 1/2, as shown in the
sample AHP calculations (Table~\ref{Tbl_SampleAHP}).

The second step is to calculate the priority vector $w$ from $A$.  The
vector $w$ ranks the software options by how well they achieve the given
quality.  The priority vector can be calculated by solving the equation
\citep{Saaty1990}:
\begin{equation} 
    A w = \lambda_{\text{max}} w,
\end{equation}
where $\lambda_{\text{max}}$ is the maximal eigenvalue of $A$.  In this project,
$w$ is approximated with the classic \textit{mean of normalized values} approach
\citep{AlessioEtAl2006}:

\begin{equation}
w_i = \frac{1}{n}\sum_{j=1}^{n}\frac{A_{ij}}{\sum_{k=1}^{n}A_{kj}}
\end{equation}

Table~\ref{Tbl_SampleAHP} summarizes the above two steps for the quality of
installability.  The matrix $A$ is shown in the first set of columns, then the
normalized version of $A$ and finally the average of the normalized values to
form the vector $w$ in the last column.

\begin{table}[h!]
\begin{center}
\begin{tabular}{ l c c c c c | c c c c c | c }
 \toprule
 ~ & \multicolumn{5}{c|}{$A_{ij}$} & \multicolumn{5}{c|}{${A_{ij}}/{\sum_{k=1}^{n}A_{kj}}$} & ~\\
 \midrule
 ~ & \rotatebox{90}{3D Slicer} & \rotatebox{90}{Ginkgo} & \rotatebox{90}{XMedCon} & $\cdots$ & \rotatebox{90}{Gwyddion} & \rotatebox{90}{3D Slicer} & \rotatebox{90}{Ginkgo} & \rotatebox{90}{XMedCon} & $\cdots$ & \rotatebox{90}{Gwyddion} & AVG \\
 \midrule
 3D Slicer & 1 & 2 & 4 & $\cdots$ & 2 & 0.071 & 0.078 & 0.060 & $\cdots$ & 0.078 & 0.068\\
 Ginkgo & 1/2 & 1 & 3 & $\cdots$ & 1 & 0.036 & 0.039 & 0.045 & $\cdots$ & 0.039 & 0.041\\
 XMedCon & 1/4 & 1/3 & 1 & $\cdots$ & 1/3 & 0.018 & 0.013 & 0.015 & $\cdots$ & 0.013 & 0.015\\
 $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$\\
 Gwyddion & 1/2 & 1 & 3 & $\cdots$ & 1 & 0.036 & 0.039 & 0.045 & $\cdots$ & 0.039 & 0.041\\  
 \midrule
 SUM = & 14.01 & 25.58 & 66.75 & $\cdots$ & 25.58 & 1.000 & 1.000 & 1.000 & $\cdots$ & 1.000 & 1.000\\
 \bottomrule
\end{tabular}
\end{center}
\caption{Sample AHP Calculations for the Quality of Usability} \label{Tbl_SampleAHP}
\end{table}

We repeat the first and second steps for each of the qualities.  The third step
combines the quality rankings into an overall ranking.  Following AHP, we need
to first prioritize the qualities.  The AHP method finds the priority of quality
$i$ ($p_i$) in the same way that the score ($w_j$) was found for software
package $j$ evaluated for a given quality (as shown above).  That is, we
conduct a pairwise comparison between the priority of different qualities to
construct the $m \times m$ matrix $A$, and then we take the mean of normalized
values for row $i$ to find the priority value $p_i$ for quality $i$.  If we
introduce the notation that $w^i_j$ is the score for quality $i$ for package
$j$, then the overall score $S_j$ for package $j$ is found via:

$$S_j = \sum_{i=1}^m w^i_j p_i$$ 

\section{Methodology} \label{ch_methods}

We developed a methodology for evaluating the state of the practice of research
software \citep{SmithEtAl2021, SmithAndMichalski2022}.  The methodology can be
instantiated for a specific domain of scientific software, which in the current
case is medical imaging software for visualization.  Our methodology involves
and engages a domain expert partner throughout, as discussed in
Section~\ref{sec_vet_software_list}.  The four main steps of the methodology
are:

\begin{enumerate}
\item Identify list of representative software packages
(Section~\ref{sec_software_selection});
\item Measure (or grade) the selected software
(Section~\ref{sec_grading_software});
\item Interview developers (Section~\ref{sec_interview_methods});
\item Answer the research questions (as given in Section~\ref{sec_motivation}).
\end{enumerate}

In the sections below we provide additional detail on the above steps, while
concurrently giving examples of how we applied the methodology to the MI domain.

\subsection{Interaction With Domain Expert} \label{sec_vet_software_list}

The Domain Expert is an important member of the state of the practice assessment
team. Pitfalls exist if non-experts attempt to acquire an authoritative list of
software, or try to definitively rank the software. Non-experts have the problem
that they can only rely on information available on-line, which has the
following drawbacks:
\begin{inparaenum}[i)]
  \item the on-line resources could have false or inaccurate information; and,
  \item the on-line resources could leave out relevant information that is so
in-grained with experts that nobody thinks to explicitly record it.
\end{inparaenum}

Domain experts may be recruited from academia or industry.  The only
requirements are knowledge of the domain and a willingness to be engaged in the
assessment process.  The Domain Expert does not have to be a software developer,
but they should be a user of domain software.  Given that the domain experts are
likely to be busy people, the measurement process cannot put too much of a burden
on their time.  For the current assessment, our Domain Expert (and paper
co-author) is Dr.\ Michael Noseworthy, Professor of Electrical and Computer
Engineering at McMaster University, Co-Director of the McMaster School of
Biomedical Engineering, and Director of Medical Imaging Physics and Engineering
at St.\ Joseph's Healthcare, Hamilton, Ontario, Canada.  

In advance of the first meeting with the Domain Expert, they are asked to
create a list of top software packages in the domain.  This is done to help
the expert get in the right mind set in advance of the meeting.  Moreover,
by doing the exercise in advance, we avoid the potential pitfall of the expert
approving the discovered list of software without giving it adequate thought.

The Domain Experts are asked to vet the collected data and analysis.  In
particular, they are asked to vet the proposed list of software packages and the
AHP ranking.  These interactions can be done either electronically or with
in-person (or virtual) meetings.

\subsection{List of Representative Software} \label{sec_software_selection}

We have a two-step process for selecting software packages: i) identify software
candidates in the chosen domain; and, ii) filter the list to remove less
relevant members \citep{SmithEtAl2021}.

We initially identified 48 MI candidate software projects from the literature
\citep{Bjorn2017, Bruhschwein2019, Haak2015}, on-line articles \citep{Emms2019,
Hasan2020, Mu2019}, and forum discussions \citep{Samala2014}.  The full list of
48 packages is available in \citet{Dong2021}.  To reduce the length of the list
to a manageable number (29 in this case, as given in Section~\ref{ch_results}),
we filtered the original list as follows:

\begin{enumerate}

\item We removed the packages that did not have source code available, such as
\textit{MicroDicom}, \textit{Aliza}, and \textit{jivex}.

\item We focused on the MI software that provides visualization functions, as
described in Section~\ref{sec_scope}. Furthermore, we removed seven packages that were
toolkits or libraries, such as \textit{VTK}, \textit{ITK}, and \textit{dcm4che}.
We removed another three that were for PACS.

\item We removed \textit{Open Dicom Viewer}, since it has not received any
updates in a long time (since 2011).

\end{enumerate}

The Domain Expert provided a list of his top 12 software packages.  We compared
his list to our list of 29.  We found 6 packages were on both lists: \textit{3D
Slicer}, \textit{Horos}, \textit{ImageJ}, \textit{Fiji}, \textit{MRIcron} (we
actually use the update version \textit{MRIcroGL}) and \textit{Mango} (we
actually use the web version \textit{Papaya}).  Six software packages
(\textit{AFNI}, \textit{FSL}, \textit{Freesurfer}, \textit{Tarquin},
\textit{Diffusion Toolkit}, and \textit{MRItrix}) were on the Domain Expert
list, but not on our filtered list.  However, when we examined those packages,
we found they were out of scope, since their primary function was not
visualization.  The Domain Expert agreed with our final choice of 29 packages.

\subsection{Grading Software} \label{sec_grading_software}

We grade the selected software using the measurement template summarized in
\citet{SmithEtAl2021}.  The template provides measures of the qualities listed
in Section~\ref{sec_software_quality}, except for reproducibility, which is
assessed through the developer interviews (Section~\ref{sec_interview_methods}).
For each software package, we fill in the template questions. To stay within the
target of 160 person hours to measure the domain, we allocated between one and
four hours for each package. Project developers can be contacted for help
regarding installation, if necessary, but we impose a cap of about two hours on
the installation process, to keep the overall measurement time feasible.
Figure~\ref{fg_grading_template_example} shows an excerpt of the spreadsheet.
The spreadsheet includes a column for each measured software package. 

\begin{figure}[!ht]
\includegraphics[scale=0.66]{figures/template.pdf}
\caption{Grading template example}
\label{fg_grading_template_example}
\end{figure}

The full template consists of 108 questions categorized under 9 qualities.  We
designed the questions to be unambiguous, quantifiable, and measurable with
limited time and domain knowledge. We group the measures under headings for each
quality, and one for summary information. The summary information (shown in
Figure~\ref{fg_grading_template_example}) is the first section of the template.
This section summarizes general information, such as the software name, purpose,
platform, programming language, publications about the software, the first
release and the most recent change date, website, source code repository of the
product, number of developers, etc.  We follow the definitions given by
\citet{GewaltigAndCannon2012} for the software categories.  Public means
software intended for public use.  Private means software aimed only at a
specific group, while the concept category is for software written simply to
demonstrate algorithms or concepts. The three categories of development models
are (open source, free-ware and commercial) are discussed in
Section~\ref{sec_software_categories}.  Information in the summary section sets
the context for the project, but it does not directly affect the grading scores.

For measuring each quality, we ask several questions and the typical answers are
among the collection of ``yes'', ``no'', ``n/a'', ``unclear'', a number, a
string, a date, a set of strings, etc. The grader assigns each quality an
overall score, between 1 and 10, based on all the previous questions.  Several
of the qualities use the word ``surface''.  This is to highlight that, for these
qualities in particular, the best that we can do is a shallow measure.  For
instance, we are not currently doing any experiments to measure usability.
Instead, we are looking for an indication that the developers considered
usability.  We do this by looking for cues in the documentation, like a getting
started manual, a user manual and a statement of expected user characteristics.
Below is a summary of how we assess adoption of best practices by measuring each
quality.

\begin{itemize}

\item \textbf{Installability} We assess the following: 
\begin{inparaenum}[i)]
    \item existence and quality of installation instructions;
    \item the quality of the user experience via the ease of following
    instructions, number of steps, automation tools; and,
    \item whether there is a means to verify the installation.
\end{inparaenum}
If any problem interrupts the process of installation or uninstallation, we give
a lower score. We also record the Operating System (OS) used for the
installation test.

\item \textbf{Correctness \& Verifiability} We check each project to identify
any techniques used to ensure this quality, such as literate programming,
automated testing, symbolic execution, model checking, unit tests, etc. We also
examine whether the projects use Continuous Integration and Continuous Delivery
(CI/CD). For verifiability, we go through the documents of the projects to check
for the presence of requirements specifications, theory manuals, and getting
started tutorials. If a getting started tutorial exists and provides expected
results, we follow it to check the correctness of the output.

\item \textbf{Surface Reliability} We check the following: 
\begin{inparaenum}[i)]
    \item whether the software breaks during installation;
    \item the operation of the software following the getting started tutorial
    (if present);
    \item whether the error messages are descriptive; and,
    \item whether we can recover the process after an error.
\end{inparaenum}

\item \textbf{Surface Robustness} We check how the software handles
unexpected/unanticipated input. For example, we prepare broken image files for
MI software packages that load image files. We use a text file (.txt) with a
modified extension name (.dcm) as an unexpected/unanticipated input. We load a
few correct input files to ensure the function is working correctly before
testing the unexpected/unanticipated ones.

\item \textbf{Surface Usability} We examine the project's documentation,
checking for the presence of getting started tutorials and/or a user manual. We
also check whether users have channels to request support, such as an e-mail
address, or issue tracker. Our impressions of usability are based on our
interaction with the software during testing.  In general, an easy-to-use
graphical user interface will score high.

\item \textbf{Maintainability} We believe that the artifacts of a project,
including source code, documents, and building scripts, significantly influence
its maintainability. Thus, we check each project for the presence of such
artifacts as API documentation, bug tracker information, release notes, test
cases, and build scripts. We also check for the use of tools supporting issue
tracking and version control, the percentages of closed issues, and the
proportion of comment lines in the code.

\item \textbf{Reusability} We count the total number of code files for each
project. Projects with numerous components potentially provide more choices for
reuse. Furthermore, well-modularized code, which tends to have smaller parts in
separate files, is typically easier to reuse. Thus, we assume that projects with
more code files and fewer Lines of Code (LOC) per file are more reusable. We also
consider projects with API documentation as delivering better reusability.

\item \textbf{Surface Understandability} Given that time is a constraint, we
cannot look at all code files for each project; therefore, we randomly examine
10 code files for their understandability. We check the code's style within each
file, such as whether the identifiers, parameters, indentation, and formatting
are consistent, whether the constants (other than 0 and 1) are not hardcoded, and
whether the code is modularized. We also check the descriptive information for
the code, such as documents mentioning the coding standard, the comments in the
code, and the descriptions or links for details on algorithms in the code. 

\item \textbf{Visibility/Transparency} To measure this quality, we check the
existing documents to find whether the software development process and
current status of a project are visible and transparent. We examine the
development process, current status, development environment, and release notes
for each project.
\end{itemize}

As part of filling in the measurement template, we use freeware tools to collect
repository related data. \href{https://github.com/tomgi/git_stats}{GitStats}
\citep{Gieniusz2019} is used to measure the number of binary files as well as
the number of added and deleted lines in a repository. We also use this tool to
measure the number of commits over different intervals of time.
\href{https://github.com/boyter/scc}{Sloc Cloc and Code (scc)}
\citep{Boyter2021} is used to measure the number of text based files as well as
the number of total, code, comment, and blank lines in a repository.

Both tools measure the number of text-based files in a git repository and lines
of text in these files. Based on our experience, most text-based files in a
repository contain programming source code, and developers use them to compile
and build software products. A minority of these files are instructions and
other documents. So we roughly regard the lines of text in text-based files as
lines of programming code. The two tools usually generate similar but not
identical results. From our understanding, this minor difference is due to the
different techniques to detect if a file is text-based or binary.

For projects on GitHub we manually collect additional information, such as the
numbers of stars, forks, people watching this repository, open pull requests,
closed pull requests, and the number of months a repository has been on GitHub.
We need to take care with the project creation date, since a repository can have
a creation date much earlier than the first day on GitHub.  For example, the
developers created the git repository for \textit{3D Slicer} in 2002, but did
not upload a copy of it to GitHub until 2020. Some GitHub data can be found
using its GitHub Application Program Interface (API) via the following url:
\textit{https://api.github.com/repos/[owner]/[repository]} where [owner] and
[repository] are replaced by the repo specific values. The number of months a
repository has been on GitHub helps us understand the average change of metrics
over time, like the average new stars per month. 

The repository measures help us in many ways. Firstly, they help us get a fast
and accurate project overview. For example, the number of commits over the last
12 months shows how active a project has been, and the number of stars and forks
may reveal its popularity (used to assess \rqref{RQ_CompareHQ2Popular}).
Secondly, the results may affect our decisions regarding the grading scores for
some software qualities. For example, if the percentage of comment lines is low,
we double-check the understandability of the code; if the ratio of open versus
closed pull requests is high, we pay more attention to maintainability.

As in \citet{SmithEtAl2016}, Virtual machines (VMs) were used to provide an
optimal testing environment for each package. We used VMs because it is easier
to start with a fresh environment, without having to worry about existing
libraries and conflicts. Moreover, when the tests are complete the VM can be
deleted, without any impact on the host operating system. The most significant
advantage of using VMs is to level the playing field. Every software install
starts from a clean slate, which removes ``works-on-my-computer'' errors. When
filling in the measurement template, the grader notes the details for each VM,
including hypervisor and operating system version.

When grading the software, we found 27 out of the 29 packages are compatible
with two or three different OSes, such as Windows, macOS, and Linux, and 5 of
them are browser-based, making them platform-independent. However, in the
interest of time, we only performed the measurements for each project by
installing it on one of the platforms.  When it was an option, we selected
Windows as the host OS.

\subsection{Interview Methods} \label{sec_interview_methods}

The repository-based measurements summarize the information we can collect from
on-line resources. This information is incomplete because it doesn't generally
capture the development process, the developer pain points, the perceived
threats to software quality, and the developers' strategies to address these
threats.  Therefore, part of our methodology involves interviewing developers.

We based our interviews on a list of 20 questions, which can be found in
\citet{SmithEtAl2021}. Some questions are about the background of the software,
the development teams, the interviewees, and how they organize their projects.
We also ask about the developer's understanding of the users. Some questions
focus on the current and past difficulties, and the solutions the team has
found, or plan to try. We also discuss documentation, both with respect to how
it is currently done, and how it is perceived. A few questions are about
specific software qualities, such as maintainability, understandability,
usability, and reproducibility. The interviews are semi-structured based on the
question list; we ask follow-up questions when necessary.  The interview process
presented here was approved by the McMaster University Research Ethics Board
under the application number 
\href{https://github.com/smiths/AIMSS/blob/master/StateOfPractice/MACREM/Application.pdf}
{MREB\#: 5219}.

We sent interview requests to all 29 projects using contact information from
projects websites, code repository, publications, and from biographic pages at
the teams' institutions.  In the end nine developers from eight of the projects
agreed to participate: \textit{3D Slicer}, \textit{INVESALIUS 3}, \textit{dwv},
\textit{BioImage Suite Web}, \textit{ITK-SNAP}, \textit{MRIcroGL},
\textit{Weasis}, and \textit{OHIF}. We spent about 90 minutes for each
interview. One participant was too busy to have an interview, so they wrote down
their answers. In one case two developers from the same project agreed to be
interviewed. We held the meetings on-line using either Zoom or Teams, which
facilitated recording and automatic transcription. The full interview answers
can be found in \citet{Dong2021}.

\section{Measurement Results} \label{ch_results}

Table~\ref{tab_final_list} shows the 29 software packages that we measured,
along with summary data collected in the year 2020. We arrange the items in
descending order of LOC. We found the initial release dates (Rlsd) for most
projects and marked the two unknown dates with ``?''. The date of the last
update is the date of the latest update, at the time of measurement. We found
funding information (Fnd) for only eight projects.  For the Number Of
Contributors (NOC) we considered anyone who made at least one accepted commit as
a contributor. The NOC is not usually the same as the number of long-term
project members, since many projects received change requests and code from the
community.  With respect to the OS, 25 packages work on all three OSs: Windows
(W), macOS (M), and Linux (L). Although the usual approach to cross-platform
compatibility was to work natively on multiple OSes, five projects achieved
platform-independence via web applications. The full measurement data for all
packages is available in \citet{Dong2021-Data}.

\begin{table}[!ht]
\centering
\begin{tabular}{p{6cm}lllllllll}
\toprule
\multirow{2}{*}{Software} & \multirow{2}{*}{Rlsd} & \multirow{2}{*}{Updated} & \multirow{2}{*}{Fnd} & \multirow{2}{*}{NOC} & \multirow{2}{*}{LOC} & \multicolumn{3}{c}{OS} & \multirow{2}{*}{Web} \\ \cline{7-9}
 &  &  &  &  &  & W & M & L &  \\ \midrule
ParaView \citep{Ahrens2005} & 2002 & 2020-10 & \checkmark & 100 & 886326 & \checkmark & \checkmark & \checkmark & \checkmark \\
Gwyddion \citep{Nevcas2012} & 2004 & 2020-11 &  & 38 & 643427 & \checkmark & \checkmark & \checkmark &  \\
Horos \citep{horosproject2020} & ? & 2020-04 &  & 21 & 561617 &  & \checkmark &  &  \\
OsiriX Lite \citep{PixmeoSARL2019} & 2004 & 2019-11 &  & 9 & 544304 &  & \checkmark &  &  \\
3D Slicer \citep{Kikinis2014} & 1998 & 2020-08 & \checkmark & 100 & 501451 & \checkmark & \checkmark & \checkmark &  \\
Drishti \citep{Limaye2012} & 2012 & 2020-08 &  & 1 & 268168 & \checkmark & \checkmark & \checkmark &  \\
Ginkgo CADx \citep{Wollny2020} & 2010 & 2019-05 &  & 3 & 257144 & \checkmark & \checkmark & \checkmark &  \\
GATE \citep{Jan2004} & 2011 & 2020-10 &  & 45 & 207122 &  & \checkmark & \checkmark &  \\
3DimViewer \citep{TESCAN2020} & ? & 2020-03 & \checkmark & 3 & 178065 & \checkmark & \checkmark &  &  \\
medInria \citep{Fillard2012} & 2009 & 2020-11 &  & 21 & 148924 & \checkmark & \checkmark & \checkmark &  \\
BioImage Suite Web \citep{Papademetris2005} & 2018 & 2020-10 & \checkmark & 13 & 139699 &
\checkmark & \checkmark & \checkmark & \checkmark \\
Weasis \citep{Roduit2021} & 2010 & 2020-08 &  & 8 & 123272 & \checkmark & \checkmark & \checkmark &  \\
AMIDE \citep{Loening2017} & 2006 & 2017-01 &  & 4 & 102827 & \checkmark & \checkmark & \checkmark &  \\
XMedCon \citep{Nolf2003} & 2000 & 2020-08 &  & 2 & 96767 & \checkmark & \checkmark & \checkmark &  \\
ITK-SNAP \citep{Yushkevich2006} & 2006 & 2020-06 & \checkmark & 13 & 88530 & \checkmark & \checkmark & \checkmark &  \\
Papaya \citep{UTHSCSA2019} & 2012 & 2019-05 &  & 9 & 71831 & \checkmark & \checkmark & \checkmark &  \\
OHIF Viewer \citep{Ziegler2020} & 2015 & 2020-10 &  & 76 & 63951 & \checkmark & \checkmark & \checkmark & \checkmark \\
SMILI \citep{Chandra2018} & 2014 & 2020-06 &  & 9 & 62626 & \checkmark & \checkmark & \checkmark &  \\
INVESALIUS 3 \citep{Amorim2015} & 2009 & 2020-09 &  & 10 & 48605 & \checkmark & \checkmark & \checkmark &  \\
dwv \citep{Martelli2021} & 2012 & 2020-09 &  & 22 & 47815 & \checkmark & \checkmark & \checkmark & \checkmark \\
DICOM Viewer \citep{Afsar2021} & 2018 & 2020-04 & \checkmark & 5 & 30761 & \checkmark & \checkmark & \checkmark &  \\
MicroView \citep{ParallaxInnovations2020} & 2015 & 2020-08 &  & 2 & 27470 & \checkmark & \checkmark & \checkmark &  \\
MatrixUser \citep{Liu2016} & 2013 & 2018-07 &  & 1 & 23121 & \checkmark & \checkmark & \checkmark &  \\
Slice:Drop \citep{Haehn2013} & 2012 & 2020-04 &  & 3 & 19020 & \checkmark & \checkmark & \checkmark & \checkmark \\
dicompyler \citep{Panchal2010} & 2009 & 2020-01 &  & 2 & 15941 & \checkmark & \checkmark &  &  \\
Fiji \citep{Schindelin2012} & 2011 & 2020-08 & \checkmark & 55 & 10833 & \checkmark & \checkmark & \checkmark &  \\
ImageJ \citep{Rueden2017} & 1997 & 2020-08 & \checkmark & 18 & 9681 & \checkmark & \checkmark & \checkmark &  \\
MRIcroGL \citep{Rorden2021} & 2015 & 2020-08 &  & 2 & 8493 & \checkmark & \checkmark & \checkmark &  \\
DicomBrowser \citep{Archie2012} & 2012 & 2020-08 &  & 3 & 5505 & \checkmark & \checkmark & \checkmark &  \\ \bottomrule
\end{tabular}
\caption{Final software list (sorted in descending order of the number of Lines
Of Code (LOC))}
\label{tab_final_list}
\end{table}

Figure \ref{fig_language} shows the primary languages versus the number of
projects using them.  The primary language is the language used for the majority
of the project's code; in most cases projects also use other languages.  The
most popular language is \CC, with almost 40\% of projects (11 of 29).  The two
least popular choices are Pascal and Matlab, with around 3\% of projects each
(1 of 29).

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.5]{figures/PrimaryLanguages.pdf}
\centering
\caption{\label{fig_language}Primary languages versus number of projects}
\end{figure}

\subsection{Installability} \label{sec_result_installability}

Figure \ref{fg_installability_scores} lists the installability scores.  We found
installation instructions for 16 projects. Among the ones without instructions,
\textit{BioImage Suite Web} and \textit{Slice:Drop} do not need installation,
since they are web applications. Installing 10 of the projects required extra
dependencies. Five of them are web applications (as shown in
Table~\ref{tab_final_list}) and depend on a browser; \textit{dwv}, \textit{OHIF
Viewer}, and \textit{GATE} needs extra dependencies to build; \textit{ImageJ}
and	\textit{Fiji} need an unzip tool; \textit{MatrixUser} is based on Matlab;
\textit{DICOM Viewer} needs to work on a Nextcloud platform.

\begin{figure}[!ht]
\includegraphics[scale=0.47]{figures/installability_scores.pdf}
\caption{AHP installability scores}
\label{fg_installability_scores}
\end{figure}

\textit{3D Slicer} has the highest score because it had easy to follow
installation instructions, and an automated, fast, frustration-free installation
process. The installer added all dependencies automatically and no errors
occurred during the installation and uninstallation steps. Many other software
packages also had installation instructions and automated installers.  We had no
trouble installing the following packages: \textit{INVESALIUS 3},
\textit{Gwyddion}, \textit{XMedCon}, and \textit{MicroView}. We determined their
scores based on the understandability of the instructions, installation steps,
and user experience. Since \textit{BioImage Suite Web} and \textit{Slice:Drop}
needed no installation, we gave them high scores. \textit{BioImage Suite Web}
also provided an option to download cache for offline usage, which was easy to
apply.

\textit{GATE}, \textit{dwv}, and \textit{DICOM Viewer} showed severe
installation problems. We were not able to install them, even after a reasonable
amount of time (2 hours).  For \textit{dwv} and \textit{GATE} we failed to build
from the source code, but we were able to proceed with measuring other qualities
using a deployed on-line version for \textit{dwv}, and a VM version for
\textit{GATE}. For \textit{DICOM Viewer} we could not install the NextCloud
dependency, and we did not have another option for running the software.
Therefore, for \textit{DICOM Viewer} we could not measure reliability or
robustness.  The other seven qualities could be measured, since they do not
require installation.

\textit{MatrixUser} has a lower score because it depends on Matlab. We assessed
the score from the point of view of a user that would have to install Matlab and
acquire a license.  Of course, for users that already work within Matlab, the
installability score should be higher.

\subsection{Correctness \& Verifiability} \label{sec_result_correctness_verifiability}

Figure~\ref{fg_correctness_verifiability_scores} shows the scores of correctness
and verifiability. Generally speaking, the packages with higher scores adopted
more techniques to improve correctness, and had better documentation for us to
verify against.  For instance, we looked for evidence of unit testing, since it
benefits most parts of the software's life cycle, such as designing, coding,
debugging, and optimization \citep{Hamill2004}.  We only found evidence of unit
testing for about half of the projects. We identified five projects using CI/CD
tools: \textit{3D Slicer}, \textit{ImageJ}, \textit{Fiji}, \textit{dwv}, and
\textit{OHIF Viewer}.

\begin{figure}[!ht]
\includegraphics[scale=0.47]{figures/correctness_verifiability_scores.pdf}
\caption{AHP correctness \& verifiability scores}
\label{fg_correctness_verifiability_scores}
\end{figure}

Even for some projects with well-organized documentation, requirements
specifications and theory manuals were still missing.  We could not identify
theory manuals for all projects, and we did not find requirements specifications
for most projects. The only requirements-related document we found was a road
map of \textit{3D Slicer}, which contained design requirements for upcoming
changes.

\subsection{Surface Reliability} \label{sec_result_reliability}

Figure~\ref{fg_reliability_scores} shows the AHP results.  As shown in
Section~\ref{sec_result_installability}, most of the software products did not
``break'' during installation, or did not need installation; \textit{dwv} and
\textit{GATE} broke in the building stage, and the processes were not
recoverable; we could not install the dependency for \textit{DICOM Viewer}. Of
the seven software packages with a getting started tutorial and operation steps
in the tutorial, most showed no error when we followed the steps. However,
\textit{GATE} could not open macro files and became unresponsive several times,
without any descriptive error message. When assessing robustness
(Section~\ref{sec_result_robustness}), we found that \textit{Drishti} crashed
when loading damaged image files, without showing any descriptive error message.
We did not find any problems with the on-line version of
\textit{dwv}.

\begin{figure}[!ht]
\includegraphics[scale=0.47]{figures/reliability_scores.pdf}
\caption{AHP surface reliability scores}
\label{fg_reliability_scores}
\end{figure}

\subsection{Surface Robustness} \label{sec_result_robustness}

Figure \ref{fg_robustness_scores} presents the scores for surface robustness.
The packages with higher scores elegantly handled unexpected/unanticipated
inputs, typically showing a clear error message. We may have underestimated the
score of \textit{OHIF Viewer}, since we needed further customization to load
data.

\begin{figure}[!ht]
\includegraphics[scale=0.47]{figures/robustness_scores.pdf}
\caption{AHP surface robustness scores}
\label{fg_robustness_scores}
\end{figure}

Digital Imaging and Communications in Medicine (DICOM) ``defines the formats for
medical images that can be exchanged with the data and quality necessary for
clinical use'' \citep{MITA2021}. According to their documentation, all 29
software packages should support the DICOM standard. To test robustness, we
prepared two types of image files: correct and incorrect formats (with the
incorrect format created by relabelled a text file to have the ``.dcm''
extension).  All software packages loaded the correct format image, except for
\textit{GATE}, which failed for unknown reasons.  For the broken format,
\textit{MatrixUser}, \textit{dwv}, and \textit{Slice:Drop} ignored the incorrect
format of the file and loaded it regardless. They did not show any error message
and displayed a blank image. \textit{MRIcroGL} behaved similarly except that it
showed a meaningless image. \textit{Drishti} successfully detected the broken
format of the file, but the software crashed as a result.

\subsection{Surface Usability} \label{sec_result_usability}

Figure~\ref{fg_usability_scores} shows the AHP scores for surface usability. The
software with higher scores usually provided both comprehensive documented
guidance and a good user experience. \textit{INVESALIUS 3} provided an excellent
example of a detailed and precise user manual. \textit{GATE} also provided
numerous documents, but unfortunately we had difficulty understanding and using
them. We found getting started tutorials for only 11 projects, but a user manual
for 22 projects. \textit{MRIcroGL} was the only project that explicitly
documented expected user characteristics.

\begin{figure}[!ht]
\includegraphics[scale=0.47]{figures/usability_scores.pdf}
\caption{AHP surface usability scores}
\label{fg_usability_scores}
\end{figure}
 
\subsection{Maintainability} \label{sec_score_maintainability}

Figure~\ref{fg_maintainability_scores} shows the ranking results for
maintainability. We gave \textit{3D Slicer} the highest score because we found
it to have the most comprehensive artifacts. For example, as far as we could
find, only a few of the 29 projects had a product, developer's manual, or API
documentation, and only \textit{3D Slicer}, \textit{ImageJ}, \textit{Fiji}
included all three documents. Moreover, \textit{3D Slicer} has a much higher
percentage of closed issues (91.65\%) compared to \textit{ImageJ} (52.49\%) and
\textit{Fiji} (63.79\%). Table~\ref{tab_maintainability_docs} shows which
projects had these documents, in descending order of their maintainability
scores. 

\begin{figure}[!ht]
\includegraphics[scale=0.47]{figures/maintainability_scores.pdf}
\caption{AHP maintainability scores}
\label{fg_maintainability_scores}
\end{figure}

\begin{table}[!ht]
\centering
\begin{tabular}{lccc}
\toprule
\multicolumn{1}{c}{Software} & Prod.\ Roadmap & Dev.\ Manual & API Doc. \\ 
\midrule
3D Slicer & \checkmark & \checkmark & \checkmark \\
ImageJ & \checkmark & \checkmark & \checkmark \\
Weasis &  & \checkmark &  \\
OHIF Viewer &  & \checkmark & \checkmark \\
Fiji & \checkmark & \checkmark & \checkmark \\
ParaView & \checkmark &  &  \\
SMILI &  &  & \checkmark \\
medInria &  & \checkmark &  \\
INVESALIUS 3 & \checkmark &  &  \\
dwv &  &  & \checkmark \\
BioImage Suite Web &  & \checkmark &  \\
Gwyddion &  & \checkmark & \checkmark \\ 
\bottomrule
\end{tabular}
\caption{Software with the maintainability documents (listed in descending order of 
maintainability score)}
\label{tab_maintainability_docs}
\end{table}

Twenty-seven of the 29 projects used git as the version control tool, with 24 of these
using GitHub. \textit{AMIDE} used Mercurial and \textit{Gwyddion} used
Subversion. \textit{XMedCon}, \textit{AMIDE}, and \textit{Gwyddion} used
SourceForge. \textit{DicomBrowser} and \textit{3DimViewer} used BitBucket. 

\subsection{Reusability} \label{sec_result_reusability}

Figure~\ref{fg_reusability_scores} shows the AHP results for reusability. As
described in Section~\ref{sec_grading_software}, we gave higher scores to the
projects with API documentation. As shown in
Table~\ref{tab_maintainability_docs}, seven projects had API documents. We also
assumed that projects with more code files and less LOC per code file are more
reusable. Table \ref{tab_loc_per_file} shows the number of text-based files by
project, which we used to approximate the number of code files. The table also
lists the total number of lines (including comments and blanks), LOC, and
average LOC per file. We arranged the items in descending order of their
reusability scores.

\begin{figure}[!ht]
\includegraphics[scale=0.47]{figures/reusability_scores.pdf}
\caption{AHP reusability scores}
\label{fg_reusability_scores}
\end{figure}

\begin{table}[!ht]
\centering
\begin{tabular}{lllll}
\toprule
\multirow{2}{*}{Software} & \multirow{2}{*}{Text Files} & \multirow{2}{*}{Total Lines} & \multirow{2}{*}{LOC} & \multirow{2}{*}{LOC/file} \\
 &  &  &  &  \\ 
\midrule
OHIF Viewer & 1162 & 86306 & 63951 & 55 \\
3D Slicer & 3386 & 709143 & 501451 & 148 \\
Gwyddion & 2060 & 787966 & 643427 & 312 \\
ParaView & 5556 & 1276863 & 886326 & 160 \\
OsiriX Lite & 2270 & 873025 & 544304 & 240 \\
Horos & 2346 & 912496 & 561617 & 239 \\
medInria & 1678 & 214607 & 148924 & 89 \\
Weasis & 1027 & 156551 & 123272 & 120 \\
BioImage Suite Web & 931 & 203810 & 139699 & 150 \\
GATE & 1720 & 311703 & 207122 & 120 \\
Ginkgo CADx & 974 & 361207 & 257144 & 264 \\
SMILI & 275 & 90146 & 62626 & 228 \\
Fiji & 136 & 13764 & 10833 & 80 \\
Drishti & 757 & 345225 & 268168 & 354 \\
ITK-SNAP & 677 & 139880 & 88530 & 131 \\
3DimViewer & 730 & 240627 & 178065 & 244 \\
DICOM Viewer & 302 & 34701 & 30761 & 102 \\
ImageJ & 40 & 10740 & 9681 & 242 \\
dwv & 188 & 71099 & 47815 & 254 \\
MatrixUser & 216 & 31336 & 23121 & 107 \\
INVESALIUS 3 & 156 & 59328 & 48605 & 312 \\
AMIDE & 183 & 139658 & 102827 & 562 \\
Papaya & 110 & 95594 & 71831 & 653 \\
MicroView & 137 & 36173 & 27470 & 201 \\
XMedCon & 202 & 129991 & 96767 & 479 \\
MRIcroGL & 97 & 50445 & 8493 & 88 \\
Slice:Drop & 77 & 25720 & 19020 & 247 \\
DicomBrowser & 54 & 7375 & 5505 & 102 \\
dicompyler & 48 & 19201 & 15941 & 332 \\ 
\bottomrule
\end{tabular}
\caption{Number of files and lines (sorted in descending order of reusability
scores)}
\label{tab_loc_per_file}
\end{table}

\subsection{Surface Understandability} \label{sec_result_understandability}

Figure~\ref{fg_surface_understandability_scores} shows the scores for surface
understandability. All projects had a consistent coding style with parameters in
the same order for all functions; modularized code; and, clear comments, indicating
what is done, not how. However, we only found explicit identification of a
coding standard for 3 out of the 29: \textit{3D Slicer}, \textit{Weasis}, and
\textit{ImageJ}. We also found hard-coded constants (rather than symbolic
constants) in \textit{medInria}, \textit{dicompyler}, \textit{MicroView}, and
\textit{Papaya}. We did not find any reference to the algorithms used in
projects \textit{XMedCon}, \textit{DicomBrowser}, \textit{3DimViewer},
\textit{BioImage Suite Web}, \textit{Slice:Drop}, \textit{MatrixUser},
\textit{DICOM Viewer}, \textit{dicompyler}, and \textit{Papaya}. 

\begin{figure}[!ht]
\includegraphics[scale=0.47]{figures/understandability_scores.pdf}
\caption{AHP surface understandability scores}
\label{fg_surface_understandability_scores}
\end{figure}

\subsection{Visibility/Transparency} \label{sec_result_visibility_transparency}

Figure~\ref{fg_visibility_transparency_scores} shows the AHP scores for
visibility/transparency. Generally speaking, the teams that actively documented
their development process and plans scored higher.
Table~\ref{tab_Visibility/Transparency_docs} shows the projects that had
documents for the development process, project status, development environment,
and release notes, in descending order of their visibility/transparency
scores.

\begin{figure}[!ht]
\includegraphics[scale=0.47]{figures/visibility_transparency_scores.pdf}
\caption{AHP visibility/transparency scores}
\label{fg_visibility_transparency_scores}
\end{figure}

\begin{table}[!ht]
\centering
\begin{tabular}{lllll}
\toprule
Software & Dev.\ Process & Proj.\ Status & Dev.\ Env. & Rls.\ Notes \\ 
\midrule
3D Slicer & \checkmark & \checkmark & \checkmark & \checkmark \\
ImageJ & \checkmark & \checkmark & \checkmark & \checkmark \\
Fiji & \checkmark & \checkmark & \checkmark &  \\
MRIcroGL &  &  &  & \checkmark \\
Weasis &  &  & \checkmark & \checkmark \\
ParaView &  & \checkmark &  &  \\
OHIF Viewer &  &  & \checkmark & \checkmark \\
DICOM Viewer &  &  & \checkmark & \checkmark \\
medInria &  &  & \checkmark & \checkmark \\
SMILI &  &  &  & \checkmark \\
Drishti &  &  &  & \checkmark \\
INVESALIUS 3 &  &  &  & \checkmark \\
OsiriX Lite &  &  &  & \checkmark \\
GATE &  &  &  & \checkmark \\
MicroView &  &  &  & \checkmark \\
MatrixUser &  &  &  & \checkmark \\
BioImage Suite Web &  &  & \checkmark &  \\
ITK-SNAP &  &  &  & \checkmark \\
Horos &  &  &  & \checkmark \\
dwv &  &  &  & \checkmark \\
Gwyddion &  &  &  & \checkmark \\ 
\bottomrule
\end{tabular}
\caption{Software with visibility/transparency related documents (listed in
descending order of visibility/transparency score)}
\label{tab_Visibility/Transparency_docs}
\end{table}

\subsection{Overall Scores} \label{Sec_OverallQ}

As described in Section~\ref{sec_AHP}, for our AHP measurements, we have nine
criteria (qualities) and 29 alternatives (software packages). In the absence of
a specific real world context, we assumed all nine qualities are equally
important. Figure~\ref{fg_overall_scores} shows the overall scores in descending
order. Since we produced the scores from the AHP process, the total sum of the
29 scores is precisely 1.0.

\begin{figure}[!ht]
\includegraphics[scale=0.47]{figures/overall_scores.pdf}
\caption{Overall AHP scores with an equal weighting for all 9 software qualities}

\label{fg_overall_scores}
\end{figure}

The top four software products \textit{3D Slicer}, \textit{ImageJ},
\textit{Fiji}, and \textit{OHIF Viewer} have higher scores in most criteria.
\textit{3D Slicer} has a score in the top two for all qualities; \textit{ImageJ}
ranks near the top for all qualities, except for correctness \& verifiability.
\textit{OHIF Viewer} and \textit{Fiji} have similar overall scores, with
\textit{Fiji} doing better in installability and \textit{OHIF Viewer} doing
better in correctness \& verifiability.  Given the installation problems, we may
have underestimated the scores on reliability and robustness for \textit{DICOM
Viewer}, but we compared it equally for the other seven qualities.

\section{Comparison to Community Ranking} \label{Sec_VsCommunityRanking}

To address~\rqref{RQ_CompareHQ2Popular} about how our ranking compares to the
popularity of projects as judged by the scientific community, we make two
comparisons:
\begin{itemize}
\item A comparison of our ranking (from Section~\ref{ch_results}) with the
community ratings on GitHub, as shown by GitHub stars, number of forks, and
number of people watching the projects; and,
\item A comparison of top-rated software from our methodology with the top
recommendations from our domain experts (as mentioned in
Section~\ref{sec_software_selection}).
\end{itemize}

Table~\ref{tab_ranking_vs_GitHub} shows our ranking of the 29 MI projects, and
their GitHub metrics, if applicable. As mentioned in
Section~\ref{sec_score_maintainability}, 24 projects used GitHub. Since GitHub
repositories have different creation dates, we collect the number of months each
stayed on GitHub, and calculate the average number of new stars, people
watching, and forks per 12 months. Section~\ref{sec_grading_software} describes
the method of getting the creation date.  The items in
Table~\ref{tab_ranking_vs_GitHub} are listed in descending order of the average
number of new stars per year.  The non-GitHub items are listed in the order of
our ranking.  We collected all GitHub statistics in July 2021.  

Generally speaking, most of the top-ranking MI software projects also received
greater attention and popularity on GitHub. Between our ranking and the GitHub
stars-per-year ranking, four of the top five software projects appear in both
lists. Our top five packages are scattered among the first eight positions on the
GitHub list. However, as discussed below there are discrepancies between the two
lists.

In some cases projects are popular in the community, but were assigned a low
rank by our methodology.  This is the case for \textit{dwv}. The reason for the
low ranking is that, as mentioned in Section~\ref{sec_result_installability}, we
failed to build it locally, and used the test version on its websites for the
measurements. We followed the instructions and tried to run the command ``yarn
run test'' locally, which did not work. In addition, the test version did not
detect a broken DICOM file and displayed a blank image as described in
Section~\ref{sec_result_robustness}. We might underestimate the scores for
\textit{dwv} due to uncommon technical issues. 

We also ranked \textit{DICOM Viewer} much lower than its popularity. As
mentioned in Section~\ref{sec_result_installability}, it depended on the
NextCloud platform that we could not successfully install. Thus, we might
underestimate the scores of its surface reliability and surface robustness. 

Further reason for discrepancies between our ranking and the community's ranking
is that we weighted all qualities equally. This is not likely how users
implicitly rank the different qualities. As a result, some projects with high
community popularity may have scored lower with our method because of a
relatively higher (compared to the scientific community's implicit ranking)
weighting of the poor scores for some qualities. A further explanation for
discrepancies between our measures and the star measures may also be due to
inaccuracy with using stars to approximate popularity.  Stars are not an ideal
measure because stars represent the community's feeling in the past more than
they measure current preferences \citep{Szulik2017}.  The issue with stars is
that they tend only to be added, not removed.  A final reason for
inconsistencies between our ranking and the community's ranking is that, as for
consumer products, more factors influence popularity than just quality.

\begingroup
\renewcommand{\arraystretch}{0.85}
\begin{table}[!ht]
\centering
\begin{tabular}{llllll}
\toprule
Software & Comm.\ Rank & Our Rank & Stars/yr & Watches/yr & Forks/yr \\ 
\midrule
3D Slicer & 1 & 1 & 284 & 19 & 128 \\
OHIF Viewer & 2 & 4 & 277 & 19 & 224 \\
dwv & 3 & 19 & 124 & 12 & 51 \\
ImageJ & 4 & 2 & 84 & 9 & 30 \\
ParaView & 5 & 5 & 67 & 7 & 28 \\
Horos & 6 & 12 & 49 & 9 & 18 \\
Papaya & 7 & 17 & 45 & 5 & 20 \\
Fiji & 8 & 3 & 44 & 5 & 21 \\
DICOM Viewer & 9 & 29 & 43 & 6 & 9 \\
INVESALIUS 3 & 10 & 8 & 40 & 4 & 17 \\
Weasis & 11 & 7 & 36 & 5 & 19 \\
dicompyler & 12 & 26 & 35 & 5 & 14 \\
OsiriX Lite & 13 & 11 & 34 & 9 & 24 \\
MRIcroGL & 14 & 18 & 24 & 3 & 3 \\
GATE & 15 & 24 & 19 & 6 & 26 \\
Ginkgo CADx & 16 & 14 & 19 & 4 & 6 \\
BioImage Suite Web & 17 & 6 & 18 & 5 & 7 \\
Drishti & 18 & 27 & 16 & 4 & 4 \\
Slice:Drop & 19 & 21 & 10 & 2 & 5 \\
ITK-SNAP & 20 & 13 & 9 & 1 & 4 \\
medInria & 21 & 9 & 7 & 3 & 6 \\
SMILI & 22 & 10 & 3 & 1 & 2 \\
MatrixUser & 23 & 28 & 2 & 0 & 0 \\
MicroView & 24 & 15 & 1 & 1 & 1 \\
Gwyddion & 25 & 16 & n/a & n/a & n/a \\
XMedCon & 26 & 20 & n/a & n/a & n/a \\
DicomBrowser & 27 & 22 & n/a & n/a & n/a \\
AMIDE & 28 & 23 & n/a & n/a & n/a \\
3DimViewer & 29 & 25 & n/a & n/a & n/a \\ 
\bottomrule
\end{tabular}
\caption{Software ranking by our methodology versus the community (Comm.)\
ranking using GitHub metrics (Sorted in descending order of community
popularity, as estimated by the number of new stars per year)}
\label{tab_ranking_vs_GitHub}
\end{table}
\endgroup

As shown in Section~\ref{sec_software_selection}, our domain experts recommended
a list of top software with 12 software products.  All the top 4 entries from
the Domain Expert's list are among the top 12 ranked by our methodology. Three
of the top four on both lists are the same: \textit{3D Slicer}, \textit{ImageJ},
and \textit{Fiji}. \textit{3D Slicer} is top project by both rankings (and by
the GitHub stars measure as well).  The Domain Expert ranked \textit{Horos} as
their second choice, while we ranked it twelfth.  Our third ranked project,
\textit{OHIF Viewer} was not listed by the Domain Expert.  Neither were the
software packages that we ranked from fifth to eleventh (\textit{ParaView},
\textit{Weasis}, \textit{medInria}, \textit{BioImage Suite Web}, \textit{OsiriX
Lite}, \textit{INVESALIUS}, and \textit{Gwyddion}).  The software mentioned by
the Domain Expert that we did not rank were the six recommended packages that
did not have visualization as the primary function (as discussed in
Section~\ref{sec_software_selection}).  The differences between the list
recommended by our methodology and the Domain Expert are not surprising.  As
mentioned above, our methodology weights all qualities equally, but that may not
be the case for the Domain Expert's impressions.  Moreover, although the Domain
Expert has significant experience with MI software, they have not used all 29
packages that were measured.

Although our ranking and the estimate of the community's ranking are not perfect
measures, they do suggest a correlation between best practices and popularity.
We do not know which comes first, the use of best practices or popularity, but
we do know that the top ranked packages tend to incorporate best practices. The
next sections will explore how the practices of the MI community compare to the
broader research software community. We will also investigate the practices from
the top projects that others within the MI community, and within the broader
research software community, can potentially adopt.

\section{Comparison Between MI and Research Software for Artifacts}
\label{Sec_CompareArtifacts}

As part of filling in the measurement template (from
Section~\ref{sec_grading_software}), we summarized the artifacts observed in
each MI package. Table~\ref{artifactspresent} groups the artifacts by frequency
into categories of common (20 to 29 ($>$67\%) packages), uncommon (10 to 19
(33-67\%) packages), and rare (1 to 9 ($<$33\%) packages). \citet{Dong2021-Data}
summarizes the full measurements.  Tables~\ref{tab_maintainability_docs}
and~\ref{tab_Visibility/Transparency_docs} show the details on which projects
use which types of artifacts for documents related to maintainability and
visibility, respectively.

\begin{table}[ht!]
    \begin{center}
    \begin{tabular}{ p{4.6 cm} p{5.6 cm} p{5 cm}}
    \toprule
    Common & Uncommon & Rare \\
    \midrule
    README (29) & Build scripts (18) & Getting Started (9)\\
    Version control (29) & Tutorials (18) & Developer's manual (8)\\
    License (28) & Installation guide (16) & Contributing (8)\\
    Issue tracker (28) & Test cases (15) & API documentation (7)\\
    User manual (22) & Authors (14) & Dependency list (7)\\
    Release info. (22) & Frequently Asked Questions (FAQ) (14) & Troubleshooting guide (6)\\
     & Acknowledgements (12) & Product roadmap (5)\\
     & Changelog (12) & Design documentation (5)\\
     & Citation (11) & Code style guide (3)\\
     & & Code of conduct (1)\\
     & & Requirements (1)\\
    \bottomrule
    \end{tabular}
    \caption{Artifacts Present in MI Packages, Classified by Frequency (The number 
    in brackets is the number of occurrences)}
    \label{artifactspresent}
    \end{center}
\end{table}

We answer~\rqref{RQ_CompareArtifacts} by comparing the artifacts that we
observed in MI repositories to those observed and recommended for research
software in general. Our comparison may point out areas where some MI software
packages fall short of current best practices. This is not intended to be a
criticism of any existing packages, especially since in practice not every
project needs to achieve the highest possible quality. However, rather than
delve into the nuances of which software can justify compromising which
practices we will write our comparison under the ideal assumption that every
project has sufficient resources to match best practices.
    
Table~\ref{Tbl_Guidelines} (based on data from \citep{SmithAndMichalski2022})
shows that MI artifacts generally match the recommendations found in nine
current research software development guidelines:
\begin{itemize}
\item United States Geological Survey Software Planning Checklist
\citep{USGS2019},
\item DLR (German Aerospace Centre) Software Engineering Guidelines
\citep{TobiasEtAl2018}, 
\item Scottish Covid-19 Response Consortium Software Checklist
\citep{BrettEtAl2021},
\item Good Enough Practices in Scientific Computing \citep{WilsonEtAl2016},
\item xSDK (Extreme-scale Scientific Software Development Kit) Community Package
Policies \citep{SmithAndRoscoe2018},
\item Trilinos Developers Guide \citep{HerouxEtAl2008},
\item EURISE (European Research Infrastructure Software Engineers') Network
Technical Reference \citep{ThielEtAl2020},
\item CLARIAH (Common Lab Research Infrastructure for the Arts and Humanities)
Guidelines for Software Quality \citep{vanGompelEtAl2016}, and
\item A Set of Common Software Quality Assurance Baseline Criteria for Research
Projects \citep{OrvizEtAl2017}.
\end{itemize}

In Table~\ref{Tbl_Guidelines} each row corresponds to an artifact.  For a given
row, a checkmark in one of the columns means that the corresponding guideline
recommends this artifact.  The last column shows whether the artifact appears in
the measured set of MI software, either not at all (blank), commonly (C),
uncommonly (U) or rarely (R).  We did our best to interpret the meaning of each
artifact consistently between guidelines and specific MI software, but the
terminology and the contents of artifacts are not standardized.  The challenge
even exists for the ubiquitous README file.  As illustrated by
\citet{PranaEtAl2018}, the content of README files shows significant variation
between projects.  Although some content is reasonably consistent, with 97\% of
README files contain at least one section describing the `What' of the
repository and 89\% offering some `How' content, other categories are more
variable.  For instance, information on `Contribution', `Why', and `Who',
appear in 28\%, 26\% and 53\% of the analyzed files, respectively
\citep{PranaEtAl2018}.  

The frequency of checkmarks in Table~\ref{Tbl_Guidelines} indicates the
popularity of recommending a given artifact, but it does not imply that the most
commonly recommended artifacts are the most important artifacts. Just because a
guideline does not explicitly recommend an artifact, does not mean the guideline
authors do not value that artifact.  They may have excluded it because it is out
of the scope of their recommendations, or outside their experience.  For
instance, an artifact related to uninstall is only explicitly mentioned by
\citet{vanGompelEtAl2016}, but other guideline authors would likely see its
value.  They may simply feel that uninstall is implied by install, or they may
have never asked themselves whether they need separate uninstall instructions.

\begin{table}[!ht]
\begin{center}
\begin{tabular}{ p{2.5cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1.2cm}p{1cm}p{0.8cm} }
\toprule
~ \ & \cite{USGS2019} & \cite{TobiasEtAl2018} & \cite{BrettEtAl2021} &
\cite{WilsonEtAl2016} & \cite{SmithAndRoscoe2018} & \cite{HerouxEtAl2008} &
\cite{ThielEtAl2020} & \cite{vanGompelEtAl2016} & \cite{OrvizEtAl2017} & MI\\
\midrule
LICENSE & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & &
\checkmark & \checkmark & \checkmark & C\\
README &  & \checkmark & \checkmark & \checkmark & \checkmark & & \checkmark &
\checkmark & \checkmark & C\\
CONTRIBUTING &  & \checkmark & \checkmark & \checkmark & \checkmark & &
\checkmark & \checkmark & \checkmark & R\\
CITATION &  &  &  & \checkmark & & & & \checkmark & \checkmark & U\\
CHANGELOG &  & \checkmark &  & \checkmark & \checkmark & & \checkmark &  &  & U\\
INSTALL &  &  &  &  & \checkmark & & \checkmark & \checkmark & \checkmark & U\\
\midrule
Uninstall &  &  &  &  & & & & \checkmark & &  \\
Dependency List &  &  & \checkmark & & \checkmark & & & \checkmark &  & R\\
Authors &  &  &  &  &  &  & \checkmark & \checkmark & \checkmark & U\\
Code of Conduct &  &  &  &  & & & \checkmark & & & R\\
Acknowledgements &  &  &  &  &  &  & \checkmark & \checkmark & \checkmark & U\\
Code Style Guide &  & \checkmark &  &  & & & \checkmark & \checkmark & \checkmark & R\\
Release Info. &  & \checkmark &  &  & & \checkmark & \checkmark & & & C\\
Prod.\ Roadmap &  &  &  &  & & \checkmark & \checkmark & \checkmark & & R\\
\midrule
Getting started &  &  &  &  & \checkmark & & \checkmark & \checkmark & \checkmark & R\\
User manual &  &  & \checkmark &  & & & \checkmark & & & C\\
Tutorials &  &  &  &  & & & \checkmark & & & U\\
FAQ &  &  &  &  & & & \checkmark & \checkmark & \checkmark & U\\
\midrule
Issue Track &  & \checkmark & \checkmark & & \checkmark & \checkmark &
\checkmark & & \checkmark & C\\
Version Control &  & \checkmark & \checkmark & \checkmark & \checkmark &
\checkmark & \checkmark & \checkmark & \checkmark & C\\ 
Build Scripts &  & \checkmark &  & \checkmark & \checkmark & \checkmark &
\checkmark & & \checkmark & U\\
\midrule
Requirements &  & \checkmark &  &  & & \checkmark &  &  & \checkmark & R\\
Design Doc.\ &  & \checkmark  & \checkmark &  & \checkmark & & \checkmark &
\checkmark& \checkmark & R\\
API Doc. &  &  &  &  & \checkmark & & \checkmark & \checkmark & \checkmark & R\\
Test Plan &  & \checkmark &  &  & & \checkmark & & & &  \\
Test Cases & \checkmark & \checkmark & \checkmark &  & \checkmark & \checkmark &
\checkmark & \checkmark & \checkmark & U\\
\bottomrule
\end{tabular}
\caption{Comparison of Recommended Artifacts in Software Development Guidelines
to Artifacts in MI Projects (C for Common, U for Uncommon and R for Rare)}
\label{Tbl_Guidelines}
\end{center}
\end{table}

Two of the items that appear in Table~\ref{artifactspresent} do not appear in
the software development guidelines shown in Table~\ref{Tbl_Guidelines}:
Troubleshooting guide and Developer's manual.  Although the guidelines do not
name these two artifacts, the information contained within them overlaps with
the recommended artifacts.  A Troubleshooting guideline contains information
that would typically be found in a User manual.  A Developer's guide overlaps
with information from the README, INSTALL, Uninstall, Dependency List, Release
Information, API documentation and Design documentation.  In our current
analysis, we have identified artifacts by the names given by the software
guidelines and MI examples.  In the future, a more in-depth analysis would look
at the knowledge fragments that are captured in the artifacts, rather than
focusing on the names of the files that collect these fragments together.

Although the MI community shows examples of 88\% (23 of 26) of the practices we
found in research software guidelines (Table~\ref{Tbl_Guidelines}), we did not
observe three recommended artifacts: i) Uninstall, ii) Test plans, and iii)
Requirements.  Uninstall is likely an omission caused by the focus on installing
software. Given the storage capacity of current hardware, developers and users
are not generally concerned with uninstall.  Moreover, as mentioned above,
uninstall is not particularly emphasized in existing recommendations.  We did
not observe test plans for MI software, but that doesn't mean plans weren't
created; it means that the plans are not under version control.  Test plans
would have to at least be implicitly created, since we observed test cases with
reasonable frequency for MI software (test cases are categorized as uncommon).

MI software is like other research software in its neglect of requirements
documentation.  Although requirements documentation is recommended by some
\citep{TobiasEtAl2018, HerouxEtAl2008, SmithAndKoothoor2016}, in practice
research software developers often do not produce a proper requirements
specification \citep{HeatonAndCarver2015}. \citet{SandersAndKelly2008}
interviewed 16 scientists from 10 disciplines and found that none of the
scientists created requirements specifications, unless regulations in their
field mandated such a document. \citet{Nguyen-HoanEtAl2010} showed requirements
are the least commonly produced type of documentation for research software in
general. When looking at the pain points for research software developers,
\citet{WieseEtAl2019} found that software requirements and management is the
software engineering discipline that most hurts scientific developers,
accounting for 23\% of the technical problems reported by study participants.
The lack of support for requirements is likely due to the perception that
up-front requirements are impossible for research software
\citep{CarverEtAl2007, SegalAndMorris2008}, but if we drop the insistence on
``up-front'' requirements, allowing instead for the requirements to be written
iteratively and incrementally, requirements are feasible \citep{Smith2016}.  

Table~\ref{Tbl_Guidelines} shows several artifacts that are rarely observed in
practice.  A theme among these rare artifacts is that many of them are related
to developers more than users.  For instance, the developer-focused, rare, and
in some cases non-observed, artifacts include the following:

\begin{itemize}

\item \textbf{A Contributing file} provides new contributors with the information
that they need to start adding/modifying the repository's files.
\citet{Abdalla2016} provides a simple template for creating an open-source
contributor guideline. 

\item \textbf{A Developer Code of Conduct} explicitly states the expectations
for developers on how they should treat one another \citep{TouraniEtAl2017}. The
code outlines rules for communication and establishes enforcement mechanisms for
violations.  As \citet{TouraniEtAl2017} states, the code of conduct documents
the spirit of a community so that anyone can comfortably contribute regardless
of ethnicity, gender, or sexual orientation. Three popular code's of conduct are
\citep{TouraniEtAl2017}:
\href{https://www.contributor-covenant.org/version/2/1/code_of_conduct/}
{Contributor Covenant}, \href{https://ubuntu.com/community/code-of-conduct}
{Ubuntu Code of Conduct}, and \href{https://www.djangoproject.com/conduct/}
{Django Code of Conduct}. A code of conduct can improve inclusivity, which
brings the benefit of a wider pool of contributors.  For example, a code of
conduct can improve the participation of women \citep{SinghEtAl2021}. A standard
of ethical behaviour can be captured in the code, for projects that are looking
to abide by a code of ethics, such as the IEEE Code of Ethics \citep{IEEE1999},
or the Professional Engineers of Ontario code of ethics \citep[p.\
23--24]{PEO2021}.

\item \textbf{Code Style Guidelines} present standards for writing code. Style
guides codify such elements as formatting, commenting, naming identifiers, best
practices and dangers to avoid \citep{Carty2020}. For instance, most coding
style guides will specify using ALLCAPS when naming symbolic constants.
Understandability improves under standardization, since developers spend
more time on the content of the code, and less time distracted by its style.
Three sample style guides are:
\href{https://google.github.io/styleguide/javaguide.html} {Google Java Style
Guide},
\href{http://cnl.sogang.ac.kr/cnlab/lectures/programming/python/PEP8_Style_Guide.pdf}
{PEP8 Style Guide for Python}, and
\href{https://google.github.io/styleguide/cppguide.html} {Google \CC Style
Guide}.  Linting tools, like \href{https://pypi.org/project/flake8/}{flake8} for
Python, can be used to enforced coding styles, like the PEP8 standard.

\item \textbf{A Product Roadmap} explains the vision and direction for a product
offering \citep{MunchEtAl2019}.  Although they have different forms, all
roadmaps cover the following: \begin{inparaenum}[i)]
	\item Where are we now?,
	\item Where are we going?, and
	\item How can we get there? \end{inparaenum} \citep{PhaalEtAl2005}. As
stated by \citet{Pichler2012}, a product roadmap provides the following
benefits: continuity of purpose, facilitation of collaboration, and assistance
with prioritization. Creating a roadmap involves the following steps:
\begin{inparaenum}[i)]
	\item define and outline a strategic mission and product vision, 
	\item scan the environment, 
	\item revise and distill the product vision to write the product roadmap, and 
	\item estimate the product life cycle and evaluate the mix of planned
development efforts \end{inparaenum} \citep{VahaniittyEtAl2002}.  

\item \textbf{Requirements Documentation} records the functionalities, expected
performance, goals, context, design constraints, external interfaces and other
quality attributes of the software \citep{IEEE1998}.  Templates, which provide
structure, guidelines and rules, are often used for writing requirements
documents. Example templates include \citet{ESA1991}, \citet{IEEE1998},
\citet{NASA1989}, and \citet{RobertsonAndRobertson1999Vol}. There is no
universally accepted template. \citet{SmithEtAl2007} provides a template
tailored to research software.

\item \textbf{Design Documentation} explicitly states the design goals and
priorities, records the likely changes, decomposes the complex system into
separate modules, and specifies the relationship between the modules. Design
documentation shows the syntax of the interface for the modules, and in some
cases also documents the semantics.  Some potential elements of design
documentation include the following:

\begin{itemize}
	\item Represent the design using Unified Modelling Language class diagrams.
	This approach is suited to object-oriented design and designs that use
	patterns \citep{Gamma1995}.
	\item Rigorously document the design following the template for a Module
	Guide (MG) \citep{ParnasEtAl1984}.  An MG organizes the modules in a
	hierarchy by their secrets.
	\item Explain the design using data flow diagrams to show typical use cases
	for input transformation.
	\item Traceability
	\item Using a textual description, list for each module (or class), the
	state variables (if any), exported constants and all exported access
	programs.  This shows the interface that can used to access each module's
	services.
	\item Document the Module Interface Specification (MIS)
	\citep{HoffmanAndStrooper1995}. An MIS is an abstract model that formally
	shows each module's access programs and the associated transitions and
	outputs based on their state, environment, and input variables
	\citep{ElSheikhEtAl2004, SmithAndYu2009}.
\end{itemize}

\item \textbf{API Documentation} shows developers the services or data provided
by the software application (or library) through such resources as its methods
or objects \citep{MengEtAl2018}.  Understandability is improved by API
documentation \citep{MengEtAl2018}. API documentation can be generated using
tools like Doxygen, pydoc, and javadoc.

\item \textbf{A test plan} Verification involves checking that the governing
equations for the model, together with other definitions, including boundary
and/or initial conditions, are solved correctly.  Validation, on the other hand,
involves checking that the model is close enough to reality for whatever
scientific or engineering problem is being addressed by the software.
Verification is an exercise in mathematics, while validation is an exercise in
engineering and science~\cite[p.~24]{Roache1998}.  Verification is not just
important for the code.  As shown by the IEEE Standard for Software Verification
and Validation Plans~\cite[p.\ 412]{VanVliet2000}, V\&V activities are
recommended for each phase of the software development lifecycle.

\end{itemize}

The rare artifacts for MI software are similar to the rare artifacts for Lattice
Boltzmann solvers \citep{Michalski2021}, except LBM software is more likely to
have developer related artifacts, like Contributing, Dependency list, and Design
documentation.

To improve MI software in the future, an increased use of checklists could help.
Developers can use checklists to ensure they follow best practices.  Some
examples include checklists merging branches into master \citep{Brown2015},
checklists for saving and sharing changes to the project \citep{WilsonEtAl2016},
checklists for new and departing team members \citep{HerouxAndBernholdt2018},
checklists for processes related to commits and releases \citep{HerouxEtAl2008}
and checklists for overall software quality \citep{ThielEtAl2020, SSI2022}.  For
instance, for Lattice Boltzmann solver software, ESPResSo has a checklist for
managing releases \citep{Michalski2021}. 

The above discussion shows that, taken together, MI projects fall somewhat short
of recommended best practices for research software.  However, MI software is
not alone in this.  Many, if not most, research projects fall short of best
practices.  A gap exists in scientific computing development practices and
software engineering recommendations \citep{Storer2017, Kelly2007,
OwojaiyeEtAl2021_CSE}. \citet{JohansonAndHasselbring2018} observe that the
state-of-the-practice for research software in industry and academia does not incorporate
state-of-the-art SE tools and methods.  This causes sustainability and
reliability problems \citep{FaulkEtAl2009}. Rather than benefit from capturing
and reusing previous knowledge, projects waste time and energy ``reinventing the
wheel'' \citep{deSouzaEtAl2019}.

\section{Comparison of Tool Usage Between MI and Other Research Software}
\label{Sec_CompareTools}

Developers use software tools to support the development, verification,
maintenance, and evolution of software, software processes, and artifacts
\citep[p.\ 501]{GhezziEtAl2003}. MI software uses tools for CI/CD, user support,
version control, documentation, and project management.  To answer
\rqref{RQ_CompareToolsProjMngmnt} we summarize the tool usage in these
categories, and compare this to the usage by the research software community.

Table~\ref{tab_user_support_model} summarizes the user support models by the
number of projects using each model (projects may use more than one support
model). We do not know whether the prevalent use of GitHub issues for user
support is by design, or whether this just naturally happens as users seek
help. The common use of GitHub by MI developers is not surprising, given that
GitHub is the largest code host in the world, with over 128 million public
repositories and over 23 million users (as of roughly February 26, 2020)
\citep{Kashyap2020}.

\begin{table}[!ht]
\centering
\begin{tabular}{lc}
\toprule
\multicolumn{1}{c}{User Support Model} & Num.\ Projects \\
\midrule
GitHub issue & 24 \\
Frequently Asked Questions (FAQ) & 12 \\
Forum & 10 \\
E-mail address & 9 \\
GitLab issue, SourceForge discussions & 2 \\
Troubleshooting & 2 \\
Contact form & 1 \\ 
\bottomrule
\end{tabular}
\caption{\label{tab_user_support_model}User support models by number of projects}
\end{table}

From Section~\ref{sec_score_maintainability}, 27 of the 29 projects used git as
the version control tool, one used Mercurial and one used Subversion.  The
hosting is on GitHub for 24 packages, SourceForge for three and BitBucket for
two.  Although teams may have a process for accepting new contributions, no one
discussed this during their interviews. However, most teams (eight of nine)
mentioned using GitHub and pull requests to manage contributions from the
community. The interviewees generally gave very positive feedback on using
GitHub. Some teams previously used a different approach to version control and
eventually transferred to git and GitHub.  The past approaches included
contributions from e-mail (three teams), contributions from forums (one team)
and e-mailing the git repository back and forth between developers (one team).

The common use of version control for MI software illustrates considerable
improvement from the poor adoption of version control tools that Wilson lamented
in 2006 \citep{Wilson2006}.  The proliferation of version control tools for MI
matches the increase in the broader research software community.  A little over
10 years ago \citet{Nguyen-HoanEtAl2010} estimated that only 50\% of research
software projects use version control, but even at that time
\citet{Nguyen-HoanEtAl2010} noted an increase from previous usage levels. A
survey in 2018 shows 81\% of developers use a version control system
\citep{AlNoamanyAndBorghi2018}. \citet{Smith2018} has similar results, showing
version control usage for alive projects in mesh generation, geographic
information systems and statistical software for psychiatry increasing from
75\%, 89\% and 17\% (respectively) to 100\%, 95\% and 100\% (respectively) over
a four-year period ending in 2018. (For completeness the same study showed a
decrease in version control usage for seismology software over the same time
period, from 41\% down to 36\%).  A recent survey by \citet{CarverEtAl2022}
shows version control use among practitioners at over 95\%, with 83/87 survey
respondents indicating that they use it. All but one of the software guides
cited in Section~\ref{Sec_CompareArtifacts} includes the advice to use version
control. (The USGS guide \citep{USGS2019} was the only set of recommendations to
not mention version control.) The high usage of version control tools in MI
software matches the trend in research software in general.

As mentioned in Section~\ref{sec_result_correctness_verifiability}, we
identified five projects using CI/CD tools (about 17\% of the assessed
projects). We found CI/CD using projects by examining the documentation and
source code of all projects. The count of CI/CD usage may actually be higher,
since traces of CI/CD usage may not always appear in a repository.  This was the
case for a study of LBM software, where interviews with developers showed that
more projects used CI/CD than was evident from repository artifacts alone
\citep{Michalski2021}.  The 17\% utilization for MI software contrasts with the
high frequency with which research software development guidelines recommend
continuous integration \citep{BrettEtAl2021, Brown2015, ThielEtAl2020,
Zadka2018, vanGompelEtAl2016}. Although there is currently little data available
on CI/CD utilization for research software, our impression is that CI/CD is not
yet common practice, despite its recommendation.  For LBM software at least the
situation is similar to MI software, with only 12.5\% of 24 LBM packages showing
evidence of CI/CD in their repositories \citep{Michalski2021}.  The survey of
\citet{CarverEtAl2022} suggests higher use of CI/CD with 54\% (54/100)
respondents indicating that they use it.

For documentation tools and methods mentioned by the interviewees, the most
popular (mentioned by about 30\% of developers) were forum discussions and
videos.  The second most popular options (mentioned by about 20\% of developers)
were GitHub, wiki pages, workshops, and social media. The least frequently
mentioned options (about 10\% of developers) included writing books, google
forms and state management.  In contrasting MI software with LBM software, the
most significant documentation tool difference is that LBM software often uses
document generation tools, like doxygen and sphinx \citep{Michalski2021}, while
MI does not appear to use these tools. 

Some interviewees mentioned the project management tools they used. Generally
speaking, the interviewees talked about two types of tools:
\begin{inparaenum}[i)]
\item trackers, including GitHub, issue trackers, bug trackers and Jira; and,
\item documentation tools, including GitHub, Wiki page, Google Doc, and
Confluence.
\end{inparaenum}
Of the specifically named tools in the above lists, interviewees mentioned
GitHub 3 times, and each of the other tools once each.

Based on information provided by \citet{JungEtAl2022}, tool utilization for MI
software has much in common with tool utilization for ocean modelling software.
Both use tools for editing, compiling, code management, testing, building, and
project management.  From the data available, ocean modelling differs from MI
software in its use of Kanban boards for project management.

\section{Comparison of Principles, Process, and Methodologies to Research Software in General} \label{Sec_CompareMethodologies}

We answer research question \rqref{RQ_CompareMethodologies} by comparing the
principles, processes, and methodologies used for MI software to what can be
gleaned from the literature on research software in general. In our interviews
with developers the responses about development model were vague, with only two
interviewees following a definite development model. In some cases the
interviewees felt their process was similar to an existing development model.
Three teams (about 38\%) either followed agile, or something similar to agile.
Two teams (25\%) either followed a waterfall process, or something similar.
Three teams (about 38\%) explicitly stated that their process was undefined or
self-directed.

Our observations of an informally defined process, with elements of agile
methods, matches what has been observed for research software in general.
Scientific developers naturally use an agile philosophy \citep{AckroydEtAl2008,
CarverEtAl2007, EasterbrookAndJohns2009, Segal2005, HeatonAndCarver2015}, or an
amethododical process \citep{Kelly2013}, or a knowledge acquisition driven
process \citep{Kelly2015}.  A waterfall like process can work for research
software \citep{Smith2016}, especially if the developers work iteratively and
incrementally, but externally document their work as if they followed a
rationale design process \citep{parnas1986rational}.

No interviewee introduced any strictly defined project management process. The
most common approach was following the issues, such as bugs and feature
requests. Additionally, the \textit{3D Slicer} team had weekly meetings to
discuss the goals for the project; the \textit{INVESALIUS 3} team relied on the
GitHub process for their project management; the \textit{ITK-SNAP} team had a
fixed six-month release pace; only the interviewee from the \textit{OHIF} team
mentioned that the team has a project manager; the \textit{3D Slicer} team and
\textit{BioImage Suite Web} team do nightly builds and tests. The \textit{OHIF}
developer believes that a better project management process can improve junior
developer efficiency while also improving internal and external communication.

We identified the use of unit testing in less than half of the 29 projects. On
the other hand, the interviewees believed that testing (including usability
tests with users) was the top solution to improve correctness, usability, and
reproducibility.  This level of testing matches what was observed for LBM
software \citep{Michalski2021} and is apparently greater than the level of
testing for ocean modelling software.  \citet{JungEtAl2022} reports that ocean
modellers underemphasize testing.

As the observed artifacts in Table~\ref{artifactspresent} show, none of the 29
projects emphasize documentation. None of them had theory manuals, although we
did identify a road map in the \textit{3D Slicer} project.  We did not find
requirements specifications. Table~\ref{tab_opinion_doc} summarizes
interviewees' opinions on documentation. Interviewees from each of the eight
projects thought that documentation was essential to their projects, and most of
them said that it could save their time to answer questions from users and
developers. Most of them saw the need to improve their documentation, and only
three of them thought that their documentations conveyed information clearly
enough. Nearly half of developers also believed that the lack of time prevented
them from improving documentation.

\begin{table}[!ht]
\centering
\begin{tabular}{ll}
\toprule
Opinion on Documentation & Num.\ Ans. \\ 
\midrule
Documentation is vital to the project & 8 \\
Documentation of the project needs improvements & 7 \\
Referring to documentation saves time to answer questions & 6 \\
Lack of time to maintain good documentation & 4 \\
Documentation of the project conveys information clearly & 3 \\
Coding is more fun than documentation & 2 \\
Users help each other by referring to documentation & 1 \\ 
\bottomrule
\end{tabular}
\caption{Opinions on documentation by the numbers of interviewees with the
answers}
\label{tab_opinion_doc}
\end{table}

As Table~\ref{Tbl_Guidelines} suggests, an emphasis on documentation, especially
for new developers, is echoed in research software guidelines. Multiple
guidelines recommend a document explaining how to contribute to a project, often
named CONTRIBUTING. Guidelines also recommend tutorials, user guides and quick
start examples. \citet{SmithAndRoscoe2018} suggests including instructions
specifically for on-boarding new developers. For open-source software in general
(not just research software), \citet{Fogel2005} recommends providing tutorial
style examples, developer guidelines, demos, and screenshots.

\section{Developer Pain Points} \label{painpoints}

Based on interviews with nine developers (described in
Section~\ref{sec_interview_methods}), we answer three research questions (first
mentioned in Section~\ref{sec_motivation}): \rqref{RQ_PainPoints}) What are the
pain points for developers working on research software projects?;
\rqref{RQ_ComparePainPoints}) How do the pain points of developers from MI
compare to the pain points for research software in general?; and
\rqref{RQ_Concerns}) For MI developers what specific best practices are taken to
address the pain points and software quality concerns? 

Our interviews identified pain points related to a lack of time and funding,
technology hurdles, improving correctness, and improving usability.  In this
section, we go through each pain point and contrast the MI experience with
observations from other domains.  We also cover potential ways to address the
pain points, as promoted by the community.  (Later, in
Section~\ref{ch_recommendations}, we propose additional pain mitigation
strategies based on our experience.)  In addition to pain points, we summarize
MI developer strategies for improving maintainability and reproducibility.
Although the interviewees did not explicitly identify these two qualities as
pain points, they were discussed as part of our interview process
\citep{SmithEtAl2021}.  The interviewee's practices for addressing pain points
and improving quality can potentially be emulated by other MI developers.
Moreover, these practices may provide examples that can be followed by other
research software domains.

\citet{PintoEtAl2018} lists some pain points that did not come up in our
conversations with MI developers: interruptions while coding, scope bloat, lack
of user feedback, hard to collaborate on software projects, and aloneness.
\citet{WieseEtAl2019} also mention two research software pain points that did
not explicitly arise in our interviews: reproducibility, and software scope
determination.  To the list of pain points not discussed for MI, our study of
LBM software \citep{SmithEtAl2022} adds lack of software experience for the
developers, technical debt, and documentation. We did not observe any pain
points for MI that were not also observed for LBM. From the pain points
mentioned above, although the topics of reproducibility and technical debt did
not come up in our MI interviews, we covered these two topics as part of the
discussion of software qualities, as summarized at the end of this section.
Although previous studies show pain points that were not mentioned by MI
developers, we cannot conclude that these pain points are not relevant for MI
software development, since we only interviewed nine developers for about an
hour each.

\begin{enumerate}

\item[P\refstepcounter{pnum}\thepnum \label{P_LackDevTime}:] \textbf{Lack of
Development Time:} Many interviewees thought lack of time, along with lack of
funding (discussed next), were their most significant obstacles. Other domains
of research software also experience the lack of time pain point
\citep{PintoEtAl2018, PintoEtAl2016, WieseEtAl2019}. Our study of LBM software
\citep{SmithEtAl2022} also highlighted lack of time as a significant pain point.

Potential and proven solutions suggested by the interviewees include:

\begin{itemize}
\item Shifting from development to maintenance when the team does not have
enough developers for building new features and fixing bugs at the same time;
\item Improving documentation to save time answering users' and developers'
questions;
\item Supporting third-party plugins and extensions; and,
\item Using GitHub Actions for continuous integration and continuous
development.
\end{itemize}

\item[P\refstepcounter{pnum}\thepnum \label{P_LackFunding}:] \textbf{Lack of
Funding:} Developers felt the pain of having to attract funding to develop and
maintain their software. For instance, the interviewees from \textit{3D Slicer}
and \textit{OHIF} said getting funding for software maintenance is more
challenging than finding funding for research. The interviewee from the
\textit{ITK-SNAP} team thought more funding was a way to solve the lack of time
problem, because they could hire more dedicated developers. On the other hand,
the interviewee from the \textit{Weasis} team did not feel that funding could
solve the same problem, since they would still need time to supervise the project. 

Funding challenges have also been noted by others \citep{GewaltigAndCannon2012,
Goble2014, KaterbowAndFeulner2018, SmithEtAl2022}. Researchers that devote time
to software have the additional challenge that funding agencies do not always
count software when they are judging the academic excellence of the applicant.
\citet{WieseEtAl2019} reported developer pains related to publicity, since
publishing norms have historically made it difficult to get credit for creating
software.  As studied by \citet{HowisonAndBullard2016}, research software
(specifically biology software, but the trend likely applies to other research
software domains) is infrequently cited. \citet{PintoEtAl2018} also mentions the
lack of formal reward system for research software.

An interviewee proposed an idea for increasing funding: Licensing the software
to commercial companies to integrate it into their products.
    
\item[P\refstepcounter{pnum}\thepnum \label{P_TechnologyHurdles}:]
\textbf{Technology Hurdles:} The technology hurdles mentioned by MI developers
include: hard to keep up with changes in OS and libraries, difficult to transfer
to new technologies, hard to support multiple OSes, and hard to support lower-end
computers. Developers expressed difficulty balancing between four factors:
cross-platform compatibility, convenience to development and maintenance,
performance, and security.

The pain point survey of \citet{WieseEtAl2019} highlights that technology
hurdles are an issue for research software in general.  Some of the
technical-related problems mentioned by \citet{WieseEtAl2019} include dependency
management, cross-platform compatibility (also mentioned by
\citet{PintoEtAl2018}), CI, hardware issues and operating system issues. From
\citep{SmithEtAl2022} technology pain points for LBM developers include setting
up parallelization and CI. 

The solutions proposed by the MI developers include the following:

\begin{itemize}
\item Adopting a web-based approach with backend servers, to better support
lower-end computers;
\item Using memory-mapped files to consume less computer memory, to better
support lower-end computers; 
\item Using computing power from the computers GPU for web applications;
\item Maintaining better documentations to ease the development and maintenance
processes;
\item Improving performance via more powerful computers, which one interviewee
pointed out has already happened to reduce the balance problem.
\end{itemize}

As the above list shows, developers perceive that web-based applications will
address the technology hurdle.  Table~\ref{tab_native_vs_web} shows the teams'
choices between native application and web application. Most of the 29 teams (24
of 29, or 83\%) chose to develop native applications. For the eight teams we
interviewed, three of them were building web applications, and the
\textit{MRIcroGL} team was considering a web-based solution.

\begin{table}[!ht]
\centering
\begin{tabular}{lll}
\toprule
Software Team & Native Application & Web Application \\ 
\midrule
3D Slicer & \checkmark & \\
INVESALIUS 3 & \checkmark & \\
dwv & & \checkmark \\
BioImage Suite Web & & \checkmark \\
ITK-SNAP & \checkmark & \\
MRIcroGL & \checkmark & \\
Weasis & \checkmark & \\
OHIF & & \checkmark \\ 
\midrule
Total number among the eight teams & 5 & 3 \\
Total number among the 29 teams & 24 & 5 \\ 
\bottomrule
\end{tabular}
\caption{Teams' choices between native application and web application}
\label{tab_native_vs_web}
\end{table}

The advantage for native applications is higher performance, while web
applications have the advantage of cross-platform compatibility and a simpler
build process.  These web advantages mirror the native disadvantages of
difficulty with cross-platform compatibility and a complex build process.  The
lower performance disadvantage of web applications can be improved with a server
backend, but in this case there are disadvantages for privacy protection and
server costs.

\item[P\refstepcounter{pnum}\thepnum \label{P_Correctness}:]
\textbf{Ensuring Correctness:} Interviewees identified multiple threats to
correctness.  The most frequently mentioned threat was complexity.  Complexity
enters the software by various means, including the large variety of data formats,
complicated data standards, differing outputs between medical imaging machines,
and the addition of (non-viewing related) functionality.  Other threats to
correctness identified include the following:

\begin{itemize}
\item Lack of real world image data for testing, in part because of patient
privacy concerns (\citet{WieseEtAl2019} mentions that the pain point of privacy
concerns also arises for research software in general);
\item Tests are expensive and time-consuming because of the need for huge datasets;
\item Software releases are difficult to manage;
\item No systematic unit testing; and,
\item No dedicated quality assurance team.
\end{itemize}

As implied by the above threats to correctness, testing was the most often
mentioned strategy for MI developers for ensuring correctness.  Seven teams
mentioned test related activities, including test-driven development, component
tests, integration tests, smoke tests, regression tests, self tests and
automated tests.  With the common emphasis on testing to improve correctness, MI
software is ahead of some other scientific domains.  For scientific software in
general \citet{PintoEtAl2018} mention the problem of insufficient testing and
\citet{HannayEtAl2009} show that more developers think testing is important than
the number that believe they have a sufficient understanding of testing
concepts.  Our study of LBM software suggests that this domain shares the
challenges of insufficient testing and insufficient understanding of testing
concepts \citep{SmithEtAl2022}. Automated testing is a specific challenge for
LBM software since free testing services do not offer adequate facilities for
large amounts of data \citep{SmithEtAl2022}. Although not specifically mentioned
during our interviews, the large data sets for MI likely cause a challenge for
using free testing services, like GitHub Actions.

Research software in general often struggles with the oracle problem for testing
because for many potential test cases the developer doesn't have a means to
judge the correctness of their calculated solutions \citep{HannayEtAl2009,
KanewalaAndBieman2013, KellyEtAl2011, WieseEtAl2019}.  The MI developers did not
allude to this challenge, likely because for a give image (test case) it is
possible to determine, possibly using other software, the expected analysis
results.

A frequently cited strategy for building confidence in correctness (mentioned by
3 interviewees) is a two state development process with stable releases and
nightly builds.  Other strategies for ensuring correctness that came up during
the interviews include CI/CD, using de-identified copies of medical images for
debugging, sending beta versions to medical workers who can access the data to
do the tests, and collecting/maintaining a dataset of problematic images.  Some
additional strategies used by MI developers include:

\begin{itemize}
\item Using open datasets.
\item If (part of) the team belongs to a medical school or a hospital, using the
datasets they can access;
\item If the team has access to MRI scanners, self-building sample images for
testing;
\item If the team has connections with MI equipment manufacturers, asking for
their help on data format problems;
\end{itemize}

The feedback from the interviewees makes it clear that increased connections
between the development team and medical professionals/institutions could ease
the pain of ensuring correctness via testing.

\item[P\refstepcounter{pnum}\thepnum \label{P_Usability}:]
\textbf{Usability:}  

The discussion with the developers focused on usability issues for two classes
of users: the end users and other developers.  The threats to usability for end
users include an unintuitive user interface, inadequate feedback from the
interface (such as lack of a progress bar), users being unable to determine the purpose of
the software, not all users knowing if the software includes certain features, not
all users understanding how to use the command line tool, and not all users
understanding that the software is a web application. For developers the threats to
usability include not being able to find clear instructions on how to deploy the
software, and the architecture being difficult for new developers to understand.

At least to some extent the problems for MI software users are due to holes in
their background knowledge.  The survey of \citet{WieseEtAl2019} for research
software in general also mentioned that users do not always have the expertise
required to install or use the software. \citet{SmithEtAl2022} observes a
similar pattern for LBM software, with several LBM developers noting that users
sometimes try to use incorrect method combinations. Furthermore, some LBM users
think that the packages will work out of the box to solve their cases, while in
reality computational fluid dynamics knowledge needs to be applied to correctly
modify the packages for a new endeavour.

To improve the usability of MI software, the most common strategies mentioned by
developers are as follows:

\begin{itemize}
    \item Use documentation (user manuals, mailing lists, forums) (mentioned by
    4 developers)
    \item Usability tests and interviews with end users; and, (mentioned by 3
    developers)
    \item Adjusting the software according to user feedback. (mentioned by 3
    developers)
\end{itemize}

Other suggested and practiced strategies include a graphical user interface,
testing every release with active users, making simple things simple and
complicated things possible, focusing on limited number of functions, icons with
clear visual expressions, designing the software to be intuitive, having a UX
designer, dialog windows for important notifications, providing an example for
users to follow, downsampling images to consume less memory, and an option to
load only part of the data to boost performance.  The last two points recognize
that an important component of usability is performance, since poor performance
frustrates users.

\end{enumerate}

Up to this point, we have covered the pain points that came up in interviews
with MI developers, along with a summary of the techniques that are currently
used to address these pain points.  Although the developers did not explicitly
identify the qualities of maintainability and reproducibility as pain points in
our interviews, as part of our interview questions
(Section~\ref{sec_interview_methods}) they did share their approaches for
improving these qualities, as discussed below.

\begin{enumerate}
\item[Q\refstepcounter{qnum}\theqnum \label{Q_Maintainability}:]
\textbf{Maintainability:} Although not explicitly highlighted as a pain point
during our interviews, \citet{Nguyen-HoanEtAl2010} rate maintainability as the
third most important software quality for research software in general. The push
for sustainable software \citep{deSouzaEtAl2019} is motivated by the pain that
past developers have had with accumulating too much technical debt
\citep{KruchtenEtAl2012}.  For LBM software, \citet{SmithEtAl2022} identifies
technical debt as one of the developer pain points.

To improve maintainability, the most popular (with five out of nine interviewees
mentioning it) strategy is to use a modular approach, with often repeated
functions in a library.  Other strategies that were mentioned for improving
maintainability include supporting third-party extensions, an easy-to-understand
architecture, a dedicated architect, starting from simple solutions, and
documentation.  The \textit{3D Slicer} team used a well-defined structure for
the software, which they named as an ``event-driven MVC pattern''. Moreover,
\textit{3D Slicer} discovers and loads necessary modules at runtime, according
to the configuration and installed extensions. The \textit{BioImage Suite Web}
team had designed and re-designed their software multiple times in the last 10+
years. They found that their modular approach effectively supports
maintainability \citep{Joshi2011}. 

\item[Q\refstepcounter{qnum}\theqnum \label{Q_Reproducibility}:]
\textbf{Reproducibility:}  Although the MI developers did not mention
reproducibility explicitly as a pain point, they did mention the need to improve
documentation.  Good documentation does not just address the pain points of lack
of developer time (\ppref{P_LackDevTime}), technology hurdles
(\ppref{P_TechnologyHurdles}), usability \ppref{P_Usability}, and
Maintainability.  Documentation is also necessary for reproducibility. The
challenges of inadequate documentation are a known problem for research software
\citep{PintoEtAl2018, WieseEtAl2019} and for non-research software
\citep{LethbridgeEtAl2003}. 

In our interviews, we discussed threats to reproducibility and strategies for
improving it.  The threats that were mentioned include closed-source software,
no user interaction tests, no unit tests, the need to change versions of some
common libraries, variability between CPUs, and misinterpretation of how
manufacturers create medical images. 

The most commonly cited (by 6 teams) strategy to improve reproducibility was
testing (regression tests, unit tests, having good tests). The second most
common strategy (mentioned by 5 teams) is making code, data, and documentation
available, possibly by creating open-source libraries.  Other ideas that were
mentioned include running the same tests on all platforms, a dockerized version
of the software to insulate it from the OS environment, using standard
libraries, monitoring the upgrades of the library dependencies, clearly
documenting the version information, bringing along the exact versions of all
the dependencies with the software, providing checksums of the data, and
benchmarking the software against other software that overlaps in functionality.
Specifically one interviewee suggested using \textit{3D Slicer} as the benchmark
to test their reproducibility.

\end{enumerate}

\section{Recommendations} \label{ch_recommendations}

In this section we provide recommendations to address the pain points from
Section~\ref{painpoints} to answer~\rqref{RQ_Recommend}.  Our recommendations
are not lists of criticisms for what should have been done in the past, or what
should be done now; they are suggestions for consideration in the future. We
expand on some of the ideas that came out of our interviews with developers
(Section~\ref{painpoints}), including continuous integration, moving to web
applications, and enriching the test data sets. We also bring in new ideas from
our experience like employing linters, peer review, design for change and
assurance cases.  Our aim is to mention ideas that are at least somewhat beyond
conventional best practices. The ideas listed here have the potential to become
best practices in the medium to long-term. We list the ideas roughly in the
order of increasing implementation effort.

\subsection{Use Continuous Integration} \label{Sec_ContinuousIntegration}

Continuous integration involves frequent pushes to a code repository.  With
every push the software is built and tested \citep[p.\ 13]
{HumbleAndFarley2010}, \citep{ShahinEtAl2017, Fowler2006}.  CI can take
significant time and effort to set up and integrate into a team's workflow, but
the benefits are significant, as follows:

\begin{itemize}
	\item Elimination of headaches associated with a separate integration phase
	\citep{Fowler2006}, \citep[p.\ 20]{HumbleAndFarley2010}. If developers
	postpone integration, integration problems are inevitable.  Continuous
	integration means that problems are immediately obvious and the source of
	the problem can be isolated to the small increment that was just committed.
	\item Detection and removal of bugs \citep{Fowler2006} via
	automated testing.  To improve productivity, defects are best discovered and
	fixed at the point where they are introduced \citep[p.\
	23]{HumbleAndFarley2010}.  Code is not the only source of errors; they are
	also found in the files and scripts related to configuration management
	\citep[p.\ 18]{HumbleAndFarley2010}.
	\item Everyone is always working on a stable base, since the rejection of
	inadequate commits means that the main branch will always be working.  A
	stable base will always pass all tests.  If the CI system uses generators
	and linters, it will also have current documentation and coding standard
	compliant code.  A stable base improves developer productivity, allowing
	them to focus on coding, testing, and documentation.
\end{itemize}

CI consists of the following elements:

\begin{itemize}
	\item A version control system \citep{Fowler2006}. To be effective, all
	files should be under version control, not just code files.  Anything that
	is needed to build, install and run the software should be under version
	control, including configuration files, build scripts, test harnesses, and
	operating system configurations \citep[p. 19]{HumbleAndFarley2010}.
	Fortunately for the MI, as shown in Section~\ref{sec_score_maintainability}
	all our measured projects use version control.
	\item A fully automated build system \citep{Fowler2006}.  As \citet[p.\
	5]{HumbleAndFarley2010} point out, deploying software manually is an
	anti-pattern.  For MI software, Table~\ref{artifactspresent} shows 18 of 29
	packages (62\%) were observed to include build scripts.  Projects without a
	build system will need to add one to pursue using CI.
	\item An automated test system \citep{Fowler2006}. Building quality software
	involves creating automated tests at the unit, component, and acceptance
	test level, and executing these tests whenever someone makes a change to the
	code, its configuration, the environment, or the software stack that it runs
	on \citep[p.\ 83]{HumbleAndFarley2010}. As Table~\ref{artifactspresent}
	shows, test cases are in the uncommon category for MI software artifacts,
	which means that some MI projects will need to increase their testing
	automation if they wish to pursue CI.
	\item An automated system for other tasks, such as code checking,
	documentation building and web-site updating.  These other tasks are not
	essential to CI, but they can be incorporated to improve the quality
	of the code and the communication between developers and users. For
	instance, a static analysis (possibly via linters) of the code may find poor
	programming practice or lack of adherence to adopted coding standards.
	\item An integration build system to pull everything together.  Every time
	there is a check-in (for instance a pull request), the integration server
	automatically checks out the sources onto the integration machine, starts a
	build, runs tests, and informs the committer of the results. 
\end{itemize}

To enable incorporation into a team's workflow, \citet[p.\
60]{HumbleAndFarley2010} explain that the usual approach for CI is to keep the
build and test process short. Since MI files are large, the tests run with every
check-in may need to focus on simple code interface tests, saving large tests
for less frequent execution.  A more sophisticated option to address the
bottleneck for merges is CIVET (Continuous Integration, Verification,
Enhancement, and Testing), which solves this problem by intelligently pinning,
cancelling, and if necessary, restarting jobs as merges occur
\citep{SlaughterEtAl2021}. A more sophisticated process management system can
also enforce rules for pull requests, like checking that a test specification
includes the test's motivation, a test description, and a design description for
all changes \citet{SlaughterEtAl2021}. 

Setting up a CI system has never been easier than it is today.  A dedicated CI
server (either physically or virtually) can be installed with tools such as
\href{https://www.jenkins.io/} {Jenkins}, \href{http://buildbot.net/}
{Buildbot}, \href{https://www.gocd.org/} {Go}, and
\href{http://integrity.github.io/} {Integrity}. However, installation on your
own server is often unnecessary since there are many hosted CI solutions, such
as: \href{https://travis-ci.org/} {Travis CI},
\href{https://github.com/features/actions} {GitHub Actions} and
\href{https://circleci.com/} {CircleCI}.  All that is required to begin using a
hosted CI is to select the service and then edit a few lines of a YAML
configuration file in the project's root directory.

\citet{ShahinEtAl2017} highlights the following challenges for adopting CI: lack
of awareness and transparency, lack of expertise and skills, coordination and
collaboration challenges, more pressure and workload for team members, general
resistance to change, scepticism and distrust on continuous practices. The most
common reason given for not adopting CI is that developers are not familiar
enough with CI \citep{HiltonEtAl2016}.  \citet{ShahinEtAl2017} observes that
these problems can be mitigated via improving testing activities, planning and
documentation, promoting a team mindset, adopting new rules and policies, and
decomposing development into smaller units.

\subsection{Move To Web Applications} \label{sec_webapps}

Section~\ref{painpoints} describes the pain point of technology hurdles
(\ppref{P_TechnologyHurdles}), which motivates considering the use of web
applications. Here we give further advice to help with deciding whether to adopt
a web application. The decision will be based on whether, on balance, the web
application improves the four factors identified by developers: compatibility,
maintainability, performance, and security. To enable decision-making, a team
will need to prioritize between these factors, based on their objectives and
experience. The suggestions are intended to provide ideas and avenues for
exploration; a web application will not be the right fit for all projects and
all teams.

\begin{itemize}

\item \textbf{Modern technologies may improve frontend performance.} Web
applications with only a frontend usually perform worse than native
applications. However, new technologies may ease this difference. For example,
some JavaScript libraries can help the frontend harness the power of the
computer's GPU and accelerate graphical computing. In addition, there are new
frameworks helping developers with cross-platform compatibility. For example,
the \href{https://flutter.dev/}{Flutter} project enables support for web,
mobile, and desktop OS with one codebase.  Other options include
\href{https://vuejs.org/} {Vue}, \href{https://angular.io/} {Angular} and
\href{https://reactjs.org/} {React} and \href{https://elm-lang.org/}{Elm}.  

\item \textbf{Backend servers can potentially deliver high performance.} Web
applications with backend servers may perform even better than native
applications. If a team needs to support lower-end computers, it is good to use
back-end servers for heavy computing tasks.  For backend servers where traffic
and latency is not an issue, options include
\href{https://www.django-rest-framework.org/} {Django}, \href{https://laravel.com/} {Laravel} and
\href{https://nodejs.org/en/} {Node.js}.  The advantage of Django is that it provides access to Python
libraries.  For backend servers where traffic and latency is an issue,
\href{https://github.com/gin-gonic/gin} {Gin} is an option.

\item \textbf{Backend servers can have low costs.} Serverless solutions from
major cloud service providers (like Amazon Web Services (AWS) and Google Cloud
Platform) may be worth exploring. Serverless solutions still use a server, but
the server provider only charges the team when they use the server. The solution
is event-driven, and costs the team by the number of requests processed. Thus,
serverless can be very cost-effective for less intensively used functions.

\item \textbf{Web transmission may diminish security.} Transferring sensitive
data on-line can be a problem for projects requiring high security. Regulations
for some MI applications may forbid doing web transmissions. In this case, a web
application with a backend may not be an option.

\end{itemize}

\subsection{Enrich the Testing Datasets} 
\label{sec_recommendations_testing_dataset}

As described in Section~\ref{painpoints}, ensuring correctness
(\ppref{P_Correctness}) via testing can be problematic because of limited access
to real-world medical imaging datasets.  We build on the suggestions we heard
from our interviewees as follows:

\begin{itemize}
\item \textbf{Build and maintain good connections to datasets.} A team can build
connections with professionals working in the medical domain, who may have
access to private datasets and can perform tests for the team. If a team has
such professionals as internal members, the process can be simplified.

\item \textbf{Collect and maintain datasets over time.} A team may face problems
caused by various unique inputs over the years of software development. This
data should be collected and maintained over time to form a good, comprehensive,
dataset for testing.

\item \textbf{Search for open data sources.} In general, there are many open MI
datasets.  For instance, there are
\href{https://nihcc.app.box.com/v/ChestXray-NIHCC}{Chest X-ray Datasets} by
National Institute of Health \citep{WangEtAl2017},
\href{https://www.cancerimagingarchive.net/}{Cancer Imaging Archive}
\citep{PriorEtAl2017}, \href{https://medpix.nlm.nih.gov/home}{MedPix} by
National Library of Medicine \citep{Smirniotopoulos2014}, and datasets for liver
\citep{BilicEtAl2019} and brain \citep{MenzeEtAl2015} tumor segmentation
benchmarks.  A team developing MI software should be able to find more open
datasets according to their needs.

\item \textbf{Create sample data for testing.} If a team can access tools
creating sample data, they may also self-build datasets for testing. For
example, an MI software development team can use an MRI scanner to create images
of objects, animals, and volunteers. The team can build the images based on
specific testing requirements.

\item \textbf{Remove privacy from sensitive data.} For data with sensitive
information, a team can ask the data owner to remove such information or add
noise to protect privacy. One example is using de-identified copies of medical
images for testing.

\item \textbf{Establish community collaboration in the domain.} During our
interviews with developers in the MI domain, we heard many stories of asking for
supports from other professionals or equipment manufacturers. However, we
believe that broader collaboration between development teams can address this
problem better. Some datasets are too sensitive to share, but if the community
has some kind of ``group discussion'', teams can better express their needs, and
professionals can better offer voluntary support for testing. Ultimately, the
community can establish a nonprofit organization as a third party, which
maintains large datasets, tests Open Source Software (OSS) in the domain, and
protects privacy. 

\end{itemize}

\subsection{Employ Linters} \label{Sec_Linters}

A linter is a tool that statically analyzes code to find programming errors,
suspicious constructs, and stylistic inconsistencies \citep{Wikipedia2022_Lint}.
Linters can be used as an ad hoc check for code files, but they really come into
their own when used as part of a continuous integration system, as discussed in
Section~\ref{Sec_ContinuousIntegration}. Almost none of the research software
guidelines that we consulted, summarized in Section~\ref{Sec_CompareArtifacts},
mention linters.  The one exception is \citet{ThielEtAl2020}.  Despite the lack
of mention in the guidelines, we believe that linters have the potential to
improve code quality at a relatively low cost.  

Linters have the following benefits: finding potential bugs, finding memory
leaks, improving performance, standardizing code with respect to formatting,
removing silly errors before code reviews, and catching potential security
issues \citep{SourceLevel2022_Lint}. Most popular programming languages have an
accompanying linter.  For example, Python has the options of PyLint, flake8 and
Black \citep{Zadka2018}.

We recommend the use of linters because they are relatively easy to incorporate
into a developer's workflow, and they address several MI pain points
(Section~\ref{painpoints}).  For instance, linters address the lack of
development time (\ppref{P_LackDevTime}) by increasing the developer's
productive time via guarding against making frustrating, time-consuming, mundane
mistakes.  Moreover, since a linter can include rules that capture the wisdom of
senior programmers, it can help newer developers avoid common mistakes. With
respect to the technology hurdle pain point (\ppref{P_TechnologyHurdles}),
linters can assist with the move toward web applications
(Section~\ref{sec_webapps}).  For instance, ESLint in React is a pluggable
linter that lets the developer know if they have imported something and not used
it, if a function could be short-handed, if there are indentation
inconsistencies, etc. \citep{Whitehouse2018}. By insisting on code
standardization linters can reduce technical debt and thus improve
maintainability (\qref{Q_Maintainability}). Although linters are tools for code
analysis, the idea of statically checking for adherence to basic rules can be
extended to check documentation. \citet{SmithEtAl2018_StatSoft} shows how the
use of tools to enforce documentation standards partially explains the
relatively higher quality of statistical tools that are part of the
Comprehensive R Archive Network (CRAN).

\subsection{Conduct a Mix of Rigorous and Informal Peer Reviews} \label{Sec_PeerReview}

We advocate incorporating peer review into the development process, as
frequently recommended for research software \citep{HerouxEtAl2008, Givler2020,
OrvizEtAl2017, USGS2019}. In most cases a modern, lightweight review, should be
adequate.  Modern code review is informal, tool-based, asynchronous, and focused
on reviewing code changes \citep{SadowskiEtAl2018}. Managing a project via
GitHub pull requests is an example of a modern approach to reviewing code.
Software development organizations have moved to this lightweight style of code
review because of the inefficiencies of rigorous inspections
\citep{RigbyAndBird2013}.  However, for important parts of the code, developers
may benefit from mixing in a more rigorous approach. 

\citet{Fagan1976} began work on rigorous review via code inspection.  Elements
of a typical inspection include reviewing the code against a checklist (checking
the consistency of variable names, look for terminating loops, etc.), performing
specific review tasks (such as summarizing the code's purpose, cross-referencing
the code to the technical manual, creating a data dictionary for a given module,
etc.) Rigorous inspection finds 60-65\% of latent defects on average, and often
tops 85\% in defect removal efficiency \citep{Jones2008}. The success rate of
code inspection is generally higher than most forms of testing, which average
between 30 and 35\% for defect removal efficiency \citep{EbertAndJones2009,
Jones2008}. For research software, \citep{KellyAndShepard2000} show a task based
inspection approach can be effective. Task based inspection is an ideal fit with
an issue tracking system, like GitHub.  The review tasks can be issues, so that
they can be easily assigned, monitored and recorded. Potential issues include
assigning junior developers to test getting-started tutorial and installation
instructions.

As indicated in Section~\ref{Sec_CompareMethodologies} some MI projects use
modern code review, via issue tracking and the use of GitHub.  Those MI projects
not incorporating modern code review would likely benefit by adopting it.
Although a rigorous code inspection is likely not worth the required resources,
for critical parts of the code, developers may want to adopt a more rigorous
approach. For instance, developers may drop the modern requirement for
asynchronous review and instead occasionally use synchronous review to help
uncover errors and disseminate best practices throughout the team.  For
instance, teams could periodically meet, either in-person or virtually, and have
junior members walk through their code.  In-person reviews will likely help
realize the benefits of modern code review noticed by
\citet{BirdAndBacchelli2013}: defect detection, knowledge transfer, increased
team awareness, and creation of alternative solutions to problems.

Due to improving code quality and increasing knowledge transfer, peer review
addresses the same pain points and qualities as linters
(Section~\ref{Sec_Linters}): \ppref{P_LackDevTime}, \ppref{P_TechnologyHurdles},
and \qref{Q_Maintainability}. Peer review can potentially find misunderstandings
in of how the code implements the required theory, which will improve the
software's correctness (\ppref{P_Correctness}). The benefits of peer review for
addressing pain points can be increased by extending the review from just code,
to also reviewing all software artifacts, including documentation, build
scripts, test cases and the development process itself.

\subsection{Design For Change} \label{Sec_DesForChange}

In our ``state of the practice'' assessment exercise for LBM software
\citep{SmithEtAl2022}, we noticed that LBM developers implicitly used
modularization based on the principle of design for change to improve
maintainability (\qref{Q_Maintainability}).  We recommend that MI developers use
the same principle for their modularizations.  Although the advice to modularize
research software to handle complexity is common \citep{WilsonEtAl2014,
StewartEtAl2017, Storer2017}, specific guidelines on how to divide the software
into modules is less prevalent.  Not every decomposition is a good design for
supporting change, as shown by \citet{Parnas1972a}.  A design with low cohesion
and high coupling \citep[p.\ 48]{GhezziEtAl2003} will make change difficult.
Especially in research software, where change is inevitable, designers need to
produce a modularization that supports change. \citep{JungEtAl2022} points out
that ocean modelling software is currently feeling the pain of not emphasizing
modularization in legacy code.

Specific examples of design for change for LBM software \citep{SmithEtAl2022}
include the following:

\begin{itemize}
\item \href{https://github.com/pylbm/pylbm}{pyLBM} has decoupled geometries and
models of their system using abstraction and modularization of the source code,
to make it easy to add new features.  The pyLBM design allows for independent
changes to the geometry and the model.  pyLBM also redeveloped data structures
to ease future change. 
\item \href{https://github.com/CFD-GO/TCLB}{TCLB} \citep{rokicki2016adjoint} is
designed to allow for the addition of some LBM features, but changes to major
aspects of the system would be difficult. For example, ``implementing a new
model will be an easy contribution'', but changes to the ``Cartesian mesh  will
be a nightmare'' \citep{SmithEtAl2022}. The design of TCLB highlights that not
every conceivable change needs to be supported, only the likely changes.  
\end{itemize}

As the LBM examples above illustrate, developers can accomplish design for
change by first identifying likely changes, either implicitly or explicitly, and
second by hiding each likely change behind a well-defined module interface.
This approach mirrors the recommendations from \citet{Parnas1972a}.
\citet{SmithEtAl2022} (under Section ``Documentation as Part of the Development
Process'') lists ideas for how to document the design, including the likely
changes, so that they are more visible to others.

\subsection{Assurance Case} \label{AssuranceCases}

To ensure correctness (\ppref{P_Correctness} and to achieve the quality of
reproducibility (\qref{Q_Reproducibility}), we recommend considering the use of
assurance cases.  \citet{RinehartEtAl2015} defines an assurance case as ``[a]
reasoned and compelling argument, supported by a body of evidence, that a
system, service, or organization will operate as intended for a defined
application in a defined environment.''  An assurance cases provide an organized
and explicit argument that the software and its documentation achieves desired
qualities, such as correctness and reproducibility.  Although assurance cases
have been successfully employed for safety critical systems
\citep{RinehartEtAl2015}, this technique is relatively new for research software
\citep{SmithEtAl2020_AC, Smith2018}.

One way to present an assurance case is through the Goal Structuring Notation
(GSN) \cite{Spriggs2012}, which make arguments clear, easy to read and, hence,
easy to challenge. GSN starts with a Top Goal (Claim), like ``Program X delivers
correct outputs when used for its intended use/purpose in its intended
environment.''  We then decompose this top goal into Sub-Goals, which themselves
may be further decomposed.  The purpose of the decomposition is to take the
abstract higher level goals and bring them down to something concrete that can
be proven.  The decomposition ends with the terminal Sub-Goals that are
supported by Solutions (Evidence). Typical evidence will consist of documents,
expert reviews, test case results, peer review, etc.  Within the GSN framework,
there are also strategy blocks, which describe the rationale for decomposing a
Goal or Sub-Goal into more detailed Sub-Goals. A common tool for creating,
editing, and presenting, a GSN argument is \href{https://astah.net/} {Astah}.  

\citet{SmithEtAl2020_AC} shows the example of arguing for the correctness of the
Analysis of Functional NeuroImages (AFNI) package 3dfim+ \citep{Ward2000}.
3dfim+ analyzes the activity of the brain by computing the correlation between
an ideal signal and the measured brain signal for each voxel. The assurance case
for the correctness of 3dfim+ has the top level decomposed into four sub-goals,
as shown in Figure~\ref{TopGoal}.  This example follows the same pattern as used
for medical devices~\cite{Wassyng2015}.  The first sub-goal (GR) argues for the
quality of the documentation of the requirements.  The second sub-goal (GD) says
that the design complies with the requirements and the third proposes that the
implementation also complies with the requirements.  The fourth sub-goal (GA)
claims that the inputs to 3dfim+ will satisfy the operational assumptions, since
we need valid input to make an argument for the correctness of the output.

\begin{figure}[!ht]
\centering
\includegraphics[width=1.0\textwidth]{./figures/TopGoal.pdf}
\caption{Top Goal of the assurance case and its sub-goals}
\label{TopGoal}
\end{figure}

Preparing an assurance case for the pre-existing 3dfim+ software shows the value
of an assurance cases for research software. Although \citet{SmithEtAl2020}
found no errors in the output of the existing software, the rigour of the
proposed approach did lead to finding ambiguities and omissions in the existing
documentation, such as missing information on the coordinate system convention.
In addition, a potential concern for the software itself was identified from the
GA argument: running the software does not produce any warning about the
obligation of the user to provide data that matches the parametric statistical
model employed for the correlation calculations.

\subsection{Generate All Things} \label{Sec_GenAllThings}

To address developer pain points, we propose automatically generating MI code
and its documentation via a Domain Specific Modelling (DSM) approach. A DSM
approach uses models to capture knowledge in domains such as physics, computing,
mathematics, documentation, and certification.  Developers combine and transform
knowledge via explicit ``recipes'', which weave it together to generate the
desired code, documentation, test cases, inspection reports and build scripts. A
recipe can even potentially be written to generate an assurance case
(Section~\ref{AssuranceCases}). Our definition here of DSM is more general than
usual; in this recommendation DSM implies generation of all software artifacts,
not just the code. DSM moves development to a higher level of abstraction so
that domain experts can work without concern for low-level implementation
details. DSM allows developers to optimally generate code and documentation,
reduce the likelihood of errors, eliminate redundancy, and automate maintenance.
With a DSM approach MI developers can experiment with different algorithm
choices, different input formats, different outputs formats, etc.

DSMs are not typically used for research software.  In the future, some believe
that they will transform coding, documentation, design, and verification
\citep{JohansonAndHasselbring2018, Smith2018}. DSMs remove the distraction of
writing software, allowing developers to focus on their science.  A DSM approach
removes the maintenance headaches of documentation duplicates and near
duplicates \citep{LucivEtAl2018}, since developers capture knowledge once and
transform it as needed.  Code generation has previously been applied to improve
research software, such as linear algebra software packages like Blitz++
\citep{Veldhuizen1998}, and ATLAS (Automatically Tuned Linear Algebra Software)
\citep{WhaleyEtAl2001}.  Software/hardware generation has been applied for
digital signal processing in Spiral \citep{Pueschel2001}. A generative approach
has also been used for a family of efficient, type-safe Gaussian elimination
algorithms \citet{Carette2008}. \citep{LoggEtAl2012} use code generation when
solving partial differential equations in FEniCS (Finite Element and
Computational Software). \citet{MatkerimEtAl2013} and \citet{OberEtAl2018} use
DSMs for High Performance Computing (HPC), using UML (Unified Modelling
Language) for their domain models. Unlike previous DSM work, the current
recommendation focuses on generating all software artifacts (requirements,
design, etc.), not just code. \citet{SzymczakEtAl2016} presents initial work on
this approach, \citet{SmithAndCarette2021-BRIC} presents a motivating example,
and \citep{CaretteEtAl2021-Drasil} provides a prototype.

A ``generate all things'' approach addresses multiple pain points.  For example,
a generative approach can decrease development time (\ppref{P_LackDevTime}) by
automation, once the necessary infrastructure is in place.  The DSM approach
addresses the technology related pain point (\ppref{P_TechnologyHurdles})
because technology information can be captured in the models and transformed as
needed. To ensure correctness (\ppref{P_Correctness}), a DSM approach should be
correct by construction.  If there are mistakes, DSM has the advantage that they
are propagated throughout the generated artifacts, which greatly increases the
chance that someone will notice the mistake.  Maintainability
(\qref{Q_Maintainability}) is addressed because developers write the recipes
used for generation at a high level making them relatively easy to change.
Usability (\ppref{P_Usability}) is addressed because of the emphasis on
generating up-to-date documentation.  Reproducibility.

\section{Threats to Validity} \label{sec_threats_to_validity}

Below we categorize and list the threats to validity that we have identified.
Our categories come from an analysis of software engineering secondary studies
by \citet{AmpatzoglouEtAl2019}, where a secondary study analyzes the data from a
set of primary studies.  \citet{AmpatzoglouEtAl2019} is appropriate because a
common example of a secondary study is a systematic literature review. Our
methodology is a systematic software review --- the primary studies are the
software packages, and our work collects and analyzes these primary studies.  We
identified similar threats to validity in our assessment of the state of the
practice of Lattice Boltzmann Solvers \citep{SmithEtAl2022}.

\subsection{Reliability}

A study is reliable if repetition of the study by different researchers using
the original study's methodology would lead to the same results
\citep{RunesonAndHost2009}. Reliability means that data and analysis are
independent of the specific researcher(s) doing the study.  For the current
study the identified reliability related threats are as follows:

\begin{itemize}
\item One individual does the manual measures for all packages. A different
evaluator might find different results, due to differences in abilities,
experiences, and biases.
\item The manual measurements for the full set of packages took several months.
Over this time the software repositories may have changed and the reviewer's
judgement may have drifted.
\end{itemize}

In \citet{SmithEtAl2016} we reduced concern over the reliability risk associated
with the reviewer's judgement  % BLIND REVIEW
by demonstrating that the measurement process is reasonably reproducible.  In
\citet{SmithEtAl2016} we graded five software products by two reviewers. Their
rankings were almost identical. As long as each grader uses consistent
definitions, the relative comparisons in the AHP results will be consistent
between graders.

\subsection{Construct Validity}

\citet{RunesonAndHost2009} defines construct validity as when the adopted
metrics represent what they are intended to measure. Our construct threats are
often related to how we assume our measurements influences the various software
qualities, as summarized in Section~\ref{sec_grading_software}. Specifically,
our construct validity related threats include the following:

\begin{itemize}
\item We make indirect measurement of software qualities since meaningful direct
measures for qualities like maintainability, reusability and verifiability, are
unavailable.  We follow the usual assumption that developers achieve higher
quality by following procedures and adhering to standards \citep[p.\
112]{VanVliet2000}.
\item As mentioned in Section~\ref{sec_result_installability}, we could not
install or build \textit{dwv}, \textit{GATE}, and \textit{DICOM Viewer}. We used
a deployed on-line version for \textit{dwv}, a VM version for \textit{GATE}, but
no alternative for \textit{DICOM Viewer}. We might underestimate their rank due
to this uncommon technical issue.
\item Measuring software robustness only involved two pieces of data. This is
likely part of the reason for limited variation in the robustness scores
(Figure~\ref{fg_robustness_scores}). We could add more robustness data by
pushing the software to deal with more unexpected situations, like a broken
Internet connection, but this would require a larger investment of measurement
time. 
\item We may have inaccurately estimated maintainability by assuming a higher
ratio of comments to source code improves maintainability. Moreover, we assumed
that maintainability is improved if a high percentage of issues are closed, but
a project may have a wealth of open issues, and still be maintainable.
\item We assess reusability by the number of code files and LOC per file. This
measure is indicative of modularity, but it does not necessarily mean a good
modularization. The modules may not be general enough to be easily reused, or
the formatting may be poor, or the understandability of the code may be low.
\item The understandability measure relies on 10 random source code files, but
the 10 files will not necessarily be representative. 
\item As discussed in Section~\ref{Sec_OverallQ}, our overall AHP ranking makes
the unrealistic assumption of equal weighting.
\item We approximated popularity by stars and watches
(Section~\ref{Sec_VsCommunityRanking}), but this assumption may not be valid. 
\item As mentioned in Section~\ref{sec_interview_methods}, one interviewee was
too busy to participate in a full interview, so they provided written answers
instead. Since we did not have the chance to explain our questions or ask them
follow-up questions, there is a possibility of misinterpretation of the
questions or answers.
\item In building Table~\ref{Tbl_Guidelines} some judgement was necessary on our
part, since not all guidelines use the same names for artifacts that contain
essentially the same information.
\end{itemize}

\subsection{Internal Validity} \label{Sec_InternalValidity}

Internal validity means that discovered causal relations are trustworthy and
cannot be explained by other factors \citep{RunesonAndHost2009}. In our
methodology the internal validity threats include the following:

\begin{itemize}
\item In our search for software packages
(Section~\ref{sec_software_selection}), we may have missed a relevant package.
\item Our methodology assumes that all relevant software development activities
will leave a trace in the repositories, but this is not necessarily true. For
instance, the possibility exists that CI usage was higher than what we observed
through the artifacts (Section~\ref{Sec_CompareTools}). As another example,
although we saw little evidence of requirements
(Section~\ref{Sec_CompareTools}), maybe teams keep this kind of information
outside their repos, possibly in journal papers or technical reports.
\item We interviewed a relatively small sample of 8 teams.  Their pain points
(Section~\ref{painpoints}) may not be representative of the rest of their
community.
\end{itemize}

\subsection{External Validity}

If the results of a study can be generalized (applied) to other
situations/cases, then the study is externally valid \citep{RunesonAndHost2009}.
We are confident that our search was exhaustive.  We do not believe that we
missed any highly popular examples.  Therefore, the bulk of our validity
concerns are internal (Section~\ref{Sec_InternalValidity}).
However, our hope is that the trends observed, and the lessons learned for MI
software can be applied to other research software.  With that in mind we
identified the following threat to external validity:

\begin{itemize}
\item We cannot generalize our results if the development of MI software is
fundamentally different from other research software.
\end{itemize}

Although there are differences, like the importance of data privacy for MI data,
we found the approach to developing LBM software \citep{SmithEtAl2022} and MI
software to be similar.  Except for the domain specific aspects, we believe that
the trends observed in the current study are externally valid for other research
software.

\section{Conclusions} \label{ch_conclusions}

\wss{Go through the RQs?}

We analyzed the state of the practice for the MI domain with the goal of
understanding current practice, answering our six research questions
(Section~\ref{sec_motivation}) and providing recommendations for current and
future projects.  Our methods in Section~\ref{ch_methods} form a general process
to evaluate domain-specific software, that we apply to the specific domain of MI
software. We identified 48 MI software candidates, then, with the help of the
Domain Expert selected 29 of them to our final list. Section~\ref{ch_results}
lists our measurements to nine software qualities for the 29 projects, and
Section~\ref{painpoints} contains our interviews with eight of the 29 teams,
discussing their development process and five software qualities.  We answered
our research questions. In addition, Section~\ref{ch_recommendations} presents
our recommendations on research software development.

Compare to LBM.

\subsection{Key Findings}

With the measurement results in Section~\ref{ch_results}, we summarized the
current status of MI software development. We ranked the 29 software projects in
nine qualities.  Based on the grading scores \textit{3D Slicer},
\textit{ImageJ}, and \textit{OHIF Viewer} are the top three software packages.

The interview results in Section~\ref{painpoints} show some merits, drawbacks,
and pain points within the development process. The three primary categories of
pain points are:
\begin{itemize}
\item the lack of funding and time;
\item the difficulty to balance between four factors: cross-platform
compatibility, convenience to development \& maintenance, performance, and
security;
\item the lack of access to real-world datasets for testing.
\end{itemize}
We summarized the solutions from the developers to address these problems,
including developing a web-based approach with backend servers and maintaining
better documentation. We also collected the status of documentation.  In our
interviews, we found that for all 8 interviewed teams felt documentation is
vital to a project, with the most popular form of documentation being forum
discussions and videos.  With respect to project management almost all teams
used GitHub and pull requests to manage contributions.  Very few teams used a
specific development model.  It appears that the development process is more ad
hoc than planned for the majority of projects.

The above findings are the basis for our answers to the research questions. We
identified the existing artifacts, tools, principles, processes, and
methodologies in the 29 projects. By comparisons with the implied popularity of
existing projects we found: 1) four of the top five software projects in our
ranking were also among the top five ones receiving the most GitHub stars per
year (Table~\ref{tab_ranking_vs_GitHub}); 2) three of the top four in our
ranking were among the top four provided by the domain experts.

Section~\ref{ch_recommendations} presents our recommendations on improving
software qualities and easing pain points during development. Some highlighted
recommendations are as follows:
\begin{itemize}
\item adopting test-driven development with unit tests, integration tests, and
nightly tests;
\item maintaining good documentation (e.g., installation instructions,
requirements specifications, theory manuals, getting started tutorials, user
manuals, project plan, developer's manual, API documentation, requirements on
coding standards, development process, project status, development environment,
and release notes);
\item using CI/CD;
\item using git and GitHub;
\item modular approach with the design principle proposed by
\citet{ParnasEtAl2000};
\item considering newer technologies (e.g.,\ web application and serverless
solution);
\item various ways of enriching the testing datasets, such as using existing
open data sources and establishing greater community collaboration in the MI
domain (Section~\ref{sec_recommendations_testing_dataset}).
\end{itemize}

\subsection{Future Works}

With learnings from this project, we summarized recommendations for the future
state of the practice assessments:
\begin{itemize}
    \item we can make the surface measurements less shallow. For example:
    \begin{itemize}
        \item Surface reliability: our current measurement relies on
        the processes of installation and getting started tutorials. However,
        not all software needs installation or has a getting started tutorial.
        We can design a list of operation steps, perform the same operations
        with each software, and record any errors.
        \item Surface robustness: we used damaged images as inputs for
        this measuring MI software. This process is similar to fuzz testing
        \citep{enwiki:1039424308}, which is one type of fault injection
        \citep{enwiki:1039005082}. We may adopt more fault injection methods, and
        identify tools and libraries to automate this process.
        \item Surface usability: we can design usability tests and test
        all software projects with end-users. The end-users can be volunteers
        and domain experts.
        \item Surface understandability: our current method does not require
        understanding the source code. As software engineers, perhaps we can
        select a small module of each project, read the source code and
        documentation, try to understand the logic, and score the ease of the
        process.  Ideas for getting started are available in
        \citet{SmithEtAl2021}.
        \item Measure modifiability as part of the measurement of
        maintainability.  An experiment could be conducted asking participants
        to make modifications, observing the study subjects during the
        modifications, testing the resulting software and surveying the
        participants \citep{SmithEtAl2021}.
    \end{itemize}
	\item We can further automate the measurements on the grading template. For
	example, with automation scripts and the GitHub API, we may save significant
	time on retrieving the GitHub metrics through a GitHub Metric Collector.
	This Collector can take GitHub repository links as input, automatically
	collect metrics from the GitHub API, and record the results.
	\item The rubric for the grading standard can be made more explicit.
	\item We can improve some interview questions. Some examples are:
	\begin{itemize}
	    \item in {Q14}, ``Do you think improving this process can tackle the
	    current problem?'' is a yes-or-no question, which is not informative
	    enough. As mentioned in Section~\ref{Sec_CompareMethodologies}, most
	    interviewees ignored it. We can change it to ``By improving this
	    process, what current problems can be tackled?''; 
	    \item in {Q16}, we can ask for more details about the modular
	    approach, such as ``What principles did you use to divide code into
	    modules? Can you describe an example of using your principles?'';
	    \item {Q17} and {Q18} should respectively ask understandability to
	    developers and usability to end-users, since there was confusion during
	    the interviews on the meaning of the terminology.
	\end{itemize}
	\item We can better organize the interview questions. Since we use audio
	conversion tools to transcribe the answers, we should make the transcription
	easier to read. For example, we can order them together for questions about
	the five software qualities and compose a similar structure for each.
	\item We can mark the follow-up interview questions with keywords. For
	example, say ``this is a follow-up question'' every time asking one. Thus, we
	record this sentence in the transcription, and it will be much easier to
	distinguish the follow-up questions from the 20 designed questions.
\end{itemize}

\section*{Acknowledgements}

We would like to thank Peter Michalski and Oluwaseun Owojaiye for fruitful
discussions on topics relevant to this paper.  We would also like to thank Jason
Balaci for advice on web applications.

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
%%  \bibliographystyle{elsarticle-harv} 
%%  \bibliography{<your bibdatabase>}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{MedImageSoft_SOP}

% \begin{thebibliography}{00}

% %% \bibitem[Author(year)]{label}
% %% Text of bibliographic item

% \bibitem[ ()]{}

% \end{thebibliography}

\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.

%%%%%%%%%%%