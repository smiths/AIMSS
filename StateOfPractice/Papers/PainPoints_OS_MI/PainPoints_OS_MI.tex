\documentclass[11pt]{article}

\usepackage{authblk}
\usepackage{fancyhdr}
\usepackage{float}

\usepackage[round, sort]{natbib}

\usepackage{amssymb}
\usepackage{amsmath,amsthm}

\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\usepackage{multirow}
\usepackage{pgfplots}
\usepackage{arydshln}
\usepackage{paralist}
\usepackage{booktabs}
\usepackage{pbox}

\oddsidemargin 0mm
\evensidemargin 0mm
\textwidth 165mm
\textheight 205mm

\pagestyle{fancy}
\lhead{\leftmark}
\chead{}
\rhead{\rightmark}
\cfoot{\thepage}

\newcounter{rqnum} %research question number
\newcommand{\rqtherqnum}{RQ`'\therqnum}
\newcommand{\rqref}[1]{RQ\ref{#1}}

\newcounter{pnum} %pain point number
\newcommand{\ppthepnum}{P`'\thepnum}
\newcommand{\ppref}[1]{P\ref{#1}}

\newcounter{qnum} %quality number
\newcommand{\qthepnum}{Q`'\theqnum}
\newcommand{\qref}[1]{Q\ref{#1}}

\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\small\bf
+}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\small\bf +}}

\pgfplotsset{compat=1.13}

\begin{document}

\title{State of the Practice for Medical Imaging Software Based on Open Source Repositories}

\author[1,*]{W.\ Spencer Smith}
\author[1]{Ao Dong}
\author[1]{Jacques Carette}
\author[2]{Michael D.\ Noseworthy}

\affil[1]{McMaster University, Computing and Software Department, {Canada}}
\affil[2]{McMaster University, Electrical \& Computer Engineering Department,
{Canada}}
\affil[*]{Corresponding Author}

\maketitle

% RQ5 to RQ10
% Some of Section 1 (Introduction)
% Sections 7 to 10 (includes use of tools, some of which was based on interviews with developers)
% Some of Sections 11 to 13 (Threats to Validity, Future Work, Conclusions)

\begin{abstract}

We present the state of the practice for Medical Imaging (MI) software. We
selected 29 medical imaging projects from 48 candidates, assessed 10 software
qualities (installability, correctness/ verifiability, reliability, robustness,
usability, maintainability, reusability, understandability,
visibility/transparency and reproducibility) by answering 108 questions for each
software project, and interviewed 8 of the 29 development teams. Based on the
quantitative data for the first 9 qualities, we ranked the MI software with the
Analytic Hierarchy Process (AHP). The four top ranked software products are:
\textit{3D Slicer}, \textit{ImageJ}, \textit{Fiji}, and \textit{OHIF Viewer}.
Our ranking is mostly consistent with the community's ranking, with four of our
top five projects also appearing in the top five of a list ordered by
stars-per-year. Generally, MI software is in a healthy state as shown by the
following: in the repositories we observed 88\% of the documentation artifacts
recommended by research software development guidelines, 100\% of MI projects
use version control tools, and developers appear to use the common quasi-agile
research software development process. However, the current state of the
practice deviates from the existing guidelines because of the rarity of some
recommended artifacts (like test plans, requirements specification, code of
conduct, code style guidelines, product roadmaps, and Application Program
Interface (API) documentation), low usage of continuous integration (17\% of the
projects), low use of unit testing (about 50\% of projects), and room for
improvement with documentation (six of nine developers felt their documentation
wasn't clear enough). From interviewing the developers, we identified five pain
points and two qualities of potential concern: lack of development time, lack of
funding, technology hurdles, ensuring correctness, usability, maintainability,
and reproducibility. The interviewees proposed strategies to improve the state
of the practice, to address the identified pain points, and to improve software
quality. Combining their ideas with ours, we have the following list of
recommendations: increase documentation, increase testing by enriching datasets,
increase continuous integration usage, move to web applications, employ linters,
use peer reviews, design for change, add assurance cases, and incorporate a
``Generate All Things'' approach.

\end{abstract}

\noindent \emph{Keywords:}
	medical imaging, research software, software engineering, software
	quality, analytic hierarchy process, developer interviews

\section{Introduction} \label{ch_intro}

We aim to study the state of software development practice for Medical Imaging
(MI) software.  MI tools use images of the interior of the body (from sources
such as Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron
Emission Tomography (PET) and Ultrasound) to provide information for diagnostic,
analytic, and medical applications \citep{FDA2021, enwiki:1034887445,
Zhang2008}.  Figure~\ref{Fig_Example}, which shows an image of the brain,
highlights the importance and value of MI. Through MI medical practitioners and
researchers can noninvasively gain insights into the human body, including
information on injuries and illnesses. Given the importance of MI software and
the high number of competing software projects, we wish to understand the merits
and drawbacks of the current development processes, tools, and methodologies. We
aim to assess through a software engineering lens the quality of the existing
software with the hope of highlighting standout examples, understanding current
pain points and providing guidelines and recommendations for future development.

\begin{figure}[!ht]
    \begin{center}
        \includegraphics[scale=0.25]{MPR.png}        
    \end{center}
    \caption{Example brain image showing a multi-planar reformat using Horos
	(free open-source medical imaging/DICOM viewer for OSX, based on OsiriX)}
    \label{Fig_Example}
\end{figure}
    
\subsection{Research Questions} \label{sec_motivation}

Not only do we wish to gain insight into the state of the practice for MI
software, we also wish to understand the development of research software in
general. We wish to understand the impact of the often cited gap, or chasm,
between software engineering and research software \citep{Kelly2007,
Storer2017}. Although scientists spend a substantial proportion of their working
hours on software development \citep{Hannay2009, Prabhu2011}, many developers
learn software engineering skills by themselves or from their peers, instead of
from proper training \citep{Hannay2009}. \citet{Hannay2009} observe that many
scientists showed ignorance and indifference to standard software engineering
concepts. For instance, according to a survey by \citet{Prabhu2011}, more than
half of their 114 subjects did not use a proper debugger when coding.

To gain insights, we devised 10 research questions, which can be applied to MI,
as well as to other domains, of research software \citep{SmithEtAl2021,
SmithAndMichalski2022}.  We designed the questions to learn about the
community's interest in, and experience with, software artifacts, tools,
principles, processes, methodologies, and qualities.  When we mention artifacts
we mean the documents, scripts and code that constitutes a software development
project. Example artifacts include requirements, specifications, user manuals,
unit tests, system tests, usability tests, build scripts, API (Application
Programming Interface) documentation, READMEs, license documents, process
documents, and code.  Once we have learned what MI developers do, we then put
this information in context by contrasting MI software against the trends shown
by developers in other research software communities.  Our aim is to collect
enough information to understand the current pain points experienced by the MI
software development community so that we can make some preliminary
recommendations for future improvements. 

We based the structure of the paper on the research questions, so for each
research question below we point to the section that contains our answer.  We
start with identifying the relevant examples of MI software for the assessment
exercise:

\begin{enumerate}
	\item[RQ\refstepcounter{rqnum}\therqnum \label{RQ_WhatProjects}:] What MI
	software projects exist, with the constraint that the source code must be
	available for all identified projects? (Section~\ref{ch_results})
	\item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_HighestQuality}:] Which
	of the projects identified in \rqref{RQ_WhatProjects} follow current best
	practices, based on evidence found by experimenting with the software and
	searching the artifacts available in each project's repository?
	(Section~\ref{ch_results})
	\item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_CompareHQ2Popular}:] How
	similar is the list of top projects identified in \rqref{RQ_HighestQuality}
	to the most popular projects, as viewed by the scientific community?
	(Section~\ref{Sec_VsCommunityRanking})
    \item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_CompareArtifacts}:] How
	do MI projects compare to research software in general with respect to the
	artifacts present in their repositories?
	(Section~\ref{Sec_CompareArtifacts})
	\item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_CompareToolsProjMngmnt}:]
	How do MI projects compare to research software in general with respect to
	the use of tools (Section~\ref{Sec_CompareTools}) for:
	\begin{enumerate} 
		\item [\rqref{RQ_CompareToolsProjMngmnt}.a] development; and,
		\item [\rqref{RQ_CompareToolsProjMngmnt}.b] project management?
	\end{enumerate}
	\item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_CompareMethodologies}:]
	How do MI projects compare to research software in general with respect to
	principles, processes, and methodologies used?
	(Section~\ref{Sec_CompareMethodologies})
	\item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_PainPoints}:] What are
	the pain points for developers working on MI software projects?
	(Section~\ref{painpoints})
	\item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_ComparePainPoints}:] How
	do the pain points of developers from MI compare to the pain points
	for research software in general? (Section~\ref{painpoints})
	\item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_Concerns}:] For MI
	developers what specific best practices are taken to address the pain points
	and software quality concerns? (Section~\ref{painpoints})
	\item [RQ\refstepcounter{rqnum}\therqnum \label{RQ_Recommend}:]
	What research software development practice could potentially address the
	pain point concerns identified in \rqref{RQ_PainPoints}?
	(Section~\ref{ch_recommendations})

\end{enumerate}

\subsection{Scope} \label{sec_scope}

To make the project feasible, we only cover MI visualization software.  As a
consequence we are excluding many other categories of MI software, including
Segmentation, Registration, Visualization, Enhancement, Quantification,
Simulation, plus MI archiving and telemedicine systems (Compression, Storage,
and Communication) (as summarized by \citet{Bankman2000} and
\citet{Angenent2006}).  We also exclude Statistical Analysis and Image-based
Physiological Modelling \citep{enwiki:1034877594} and Feature Extraction,
Classification, and Interpretation \citep{Kim2011}. Software that provides MI
support functions is also out of scope; therefore, we have not assessed the
toolkit libraries VTK \citep{SchroederEtAl2006} and ITK \citep{McCormick2014}.
Finally, Picture Archiving and Communication System (PACS), which helps users to
economically store and conveniently access images \citep{Choplin1992}, are
considered out of scope. 

\subsection{Methodology Overview}

We designed a general methodology to assess the state of the practice for
research software \citep{SmithEtAl2021, SmithAndMichalski2022}. Details can be
found in Section~\ref{ch_methods}.  Our methodology has been applied to MI
software \citep{Dong2021} and Lattice Boltzmann Solvers \citep{Michalski2021,
SmithEtAl2024}.  This methodology builds off prior work to assess the state of
the practice for such domains as Geographic Information Systems
\citep{smith2018state}, Mesh Generators \citep{smith2016state}, Seismology
software \citep{Smith2018Seismology}, and Statistical software for psychology
\citep{smith2018statistical}.  In keeping with the previous methodology, we have
maintained the constraint that the work load for measuring a given domain should
be feasible for a team as small as one person, and for a short time, ideally
around a person month of effort. We consider a person month as $20$ working days
($4$ weeks in a month, with $5$ days of work per week) at $8$ person hours per
day, or $20 \times 8 = 160$ person hours.

With our methodology, we first choose a research software domain (in the current
case MI) and identify a list of about 30 software packages. (For measuring MI we
used 29 software packages.)  We approximately measure the qualities of each
package by filling in a grading template. Compared with our previous
methodology, the new methodology also includes repository based metrics, such as
the number of files, number of lines of code, percentage of issues that are
closed, etc.  With the quantitative data in the grading template, we rank the
software with the Analytic Hierarchy Process (AHP) (Section~\ref{ch_background}
provides details). After this, as another addition to our previous methodology,
we interview some development teams to further understand the status of their 
development process.

\section{Background} \label{ch_background}

To measure the existing MI software we need two sets of definitions: i) the
definitions of relevant software license models (Section
\ref{sec_software_categories}); and, ii) the definitions of the software
qualities that we will be assessing (Section \ref{sec_software_quality}). In our
assessment we rank the software packages for each quality; therefore, this
section also provides the background on our ranking process --- the Analytic
Hierarchy Process (Section \ref{sec_AHP}).

\subsection{Software Categories} \label{sec_software_categories}

When assessing software packages, we need to know the software's license.  In
particular, we need to know whether the source code will be available to us or
not.  We define three common software categories.  We will only assess software
that fits under the Open Source Software license.

\begin{itemize}

\item \textbf{Open Source Software (OSS)} For OSS, the source code is openly
accessible. Users have the right to study, change and distribute it under a
license granted by the copyright holder. For many OSS projects, the development
process relies on the collaboration of different contributors worldwide
\citep{Corbly2014}. Accessible source code usually exposes more ``secrets'' of a
software project, such as the underlying logic of software functions, how
developers achieve their works, and the flaws and potential risks in the final
product. Thus, OSS is suitable for researchers analyzing the qualities of a
project.

\item \textbf{Freeware} Freeware is software that can be used free of charge.
Unlike OSS, the authors of do not allow access or modify the source code
\citep{LINFO2006}. To many end-users, the differences between freeware and OSS
may not be relevant. However, software developers who wish to modify the source
code, and researchers looking for insight into software development process may
find the inaccessible source code a problem. 

\item \textbf{Commercial Software} ``Commercial software is software developed
by a business as part of its business'' \citep{GNU2019}. Typically speaking,
commercial software requires users to pay to access all of its features,
excluding access to the source code. However, some commercial software is also
free of charge \citep{GNU2019}. Based on our experience, most commercial
software products are not OSS.

\end{itemize}

\subsection{Software Quality Definitions} \label{sec_software_quality}

Quality is defined as a measure of the excellence or worth of an entity.  As is
common practice, we do not think of quality as a single measure, but rather as a
set of measures.  That is, quality is a collection of different qualities, often
called ``ilities.''  Below we list the 10 qualities of interest for this study.
The order of the qualities follows the order used in \citet{GhezziEtAl2003},
which puts related qualities (like correctness and reliability) together.
Moreover, the order is roughly the same as the order developers consider
qualities in practice.

\begin{itemize}
	\item \textbf{Installability} The effort required for the installation
    and/or uninstallation of software in a specified environment
    \citep{ISO/IEC25010, lenhard2013measuring}.

	\item \textbf{Correctness \& Verifiability} A program is correct if it
    matches its specification \citep[p.\ 17]{GhezziEtAl2003}.  The specification
    can either be explicitly or implicitly stated.  The related quality of
    verifiability is the ease with which the software components or the
    integrated product can be checked to demonstrate its correctness. 

	\item \textbf{Reliability} The probability of failure-free operation of a
	computer program in a specified environment for a specified time
	\citep{musa1987software}, \citep[p.\ 357]{GhezziEtAl2003}.

	\item \textbf{Robustness} Software possesses the characteristic of
	robustness if it behaves ``reasonably'' in two situations: i) when it
	encounters circumstances not anticipated in the requirements specification,
	and ii) when users violate the assumptions in its requirements specification 
	\citep[p.\ 19]{GhezziEtAl2003}, \citep{boehm2007software}.

	\item \textbf{Usability} ``The extent to which a product can be used by
	specified users to achieve specified goals with effectiveness, efficiency,
	and satisfaction in a specified context of use'' \citep{ISO/TR16982:2002,
	ISO9241-11:2018}.

	\item \textbf{Maintainability} The effort with which a software system or
	component can be modified to i) correct faults; ii) improve performance or
	other attributes; iii) satisfy new requirements
	\citep{IEEEStdGlossarySET1990, boehm2007software}.

	\item \textbf{Reusability} ``The extent to which a software component can be
	used with or without adaptation in a problem solution other than the one for
	which it was originally developed'' \citep{kalagiakos2003non}.

	\item \textbf{Understandability} ``The capability of the software product to
	enable the user to understand whether the software is suitable, and how it
	can be used for particular tasks and conditions of use'' \citep{iso2001iec}.

	\item \textbf{Visibility/Transparency} The extent to which all the steps
	of a software development process and the current status of it are conveyed
	clearly \citep[p.\ 32]{GhezziEtAl2003}.

	\item \textbf{Reproducibility} ``A result is said to be reproducible if
	another researcher can take the original code and input data, execute it,
	and re-obtain the same result'' \citep{BenureauAndRougier2017}.
\end{itemize}

\subsection{Analytic Hierarchy Process (AHP)} \label{sec_AHP}

Saaty developed AHP in the 1970s, and people have widely used it since to make
and analyze multiple criteria decisions \citep{VaidyaEtAl2006}. AHP organizes
multiple criteria in a hierarchical structure and uses pairwise comparisons
between alternatives to calculate relative ratios \citep{Saaty1990}. AHP works
with sets of $n$ \textit{options} and $m$ \textit{criteria}.  In our project
$n=29$ and $m=9$ since there are 29 options (software products) and 9 criteria
(qualities). We rank the software for each of the qualities, and then we combine
the quality rankings into an overall ranking based on the relative priorities
between qualities.

The first step for ranking the software choices for a given quality involves a
pairwise comparison between each of the $n$ software options for that quality.
AHP expresses the comparison through an $n \times n$ matrix $A$. When comparing
option $i$ and option $j$, the value of $A_{ij}$ is decided as follows, with the
value of $A_{ji}$ generally equal to $1/A_{ij}$ \citep{Saaty1990}: $A_{ij} = 1$
if criterion $i$ and criterion $j$ are equally important, while $A_{ij} = 9$ if
criterion $i$ is extremely more important than criterion $j$.  The natural
numbers between 1 and 9 are used to show the different levels of relative
importance between these two extremes. The above assumes that option $i$ is of
equal, or more, importance compared to option $j$ ($i \geq j$).  If that is not
the case, we reverse $i$ and $j$ and determine $A_{ji}$ first, then $A_{ij} =
1/A_{ji}$.

Section~\ref{sec_grading_software} shows how we measure the software via a
grading template.  For the AHP process, the relevant measure is the subjective
score from $1$ to $10$ for each quality for each package. To turn these
subjective measures $x_{\text{sub}}$ and $y_{\text{sub}}$ into Saaty's
pair-wise scores for option $x$ versus option $y$, respectively, we use the
following calculation:
\[
\begin{cases}
\min\{9, x_{\text{sub}} - y_{\text{sub}} + 1\} & x_{\text{sub}} \geq y_{\text{sub}} \\
1 / \min\{9, y_{\text{sub}} - x_{\text{sub}} + 1\} & x_{\text{sub}} < y_{\text{sub}}
\end{cases}
\]

\noindent For example, we measured the usability for 3D Slicer and Ginkgo CADx
as $8$ and $7$, respectively; therefore, on the 9-point scale, 3D Slicer compared
to Ginkgo CADx is 2 and Ginkgo CADx versus 3D Slicer is 1/2, as shown in the
sample AHP calculations (Table~\ref{Tbl_SampleAHP}).

The second step is to calculate the priority vector $w$ from $A$.  The
vector $w$ ranks the software options by how well they achieve the given
quality.  The priority vector can be calculated by solving the equation
\citep{Saaty1990}:
\begin{equation} 
    A w = \lambda_{\text{max}} w,
\end{equation}
where $\lambda_{\text{max}}$ is the maximal eigenvalue of $A$.  In this project,
$w$ is approximated with the classic \textit{mean of normalized values} approach
\citep{AlessioEtAl2006}:

\begin{equation}
w_i = \frac{1}{n}\sum_{j=1}^{n}\frac{A_{ij}}{\sum_{k=1}^{n}A_{kj}}
\end{equation}

Table~\ref{Tbl_SampleAHP} summarizes the above two steps for the quality of
installability.  The matrix $A$ is shown in the first set of columns, then the
normalized version of $A$ and finally the average of the normalized values to
form the vector $w$ in the last column.

\begin{table}[h!]
\begin{center}
\begin{tabular}{ l c c c c c | c c c c c | c }
 \toprule
 ~ & \multicolumn{5}{c|}{$A_{ij}$} & \multicolumn{5}{c|}{${A_{ij}}/{\sum_{k=1}^{n}A_{kj}}$} & ~\\
 \midrule
 ~ & \rotatebox{90}{3D Slicer} & \rotatebox{90}{Ginkgo} & \rotatebox{90}{XMedCon} & $\cdots$ & \rotatebox{90}{Gwyddion} & \rotatebox{90}{3D Slicer} & \rotatebox{90}{Ginkgo} & \rotatebox{90}{XMedCon} & $\cdots$ & \rotatebox{90}{Gwyddion} & AVG \\
 \midrule
 3D Slicer & 1 & 2 & 4 & $\cdots$ & 2 & 0.071 & 0.078 & 0.060 & $\cdots$ & 0.078 & 0.068\\
 Ginkgo & 1/2 & 1 & 3 & $\cdots$ & 1 & 0.036 & 0.039 & 0.045 & $\cdots$ & 0.039 & 0.041\\
 XMedCon & 1/4 & 1/3 & 1 & $\cdots$ & 1/3 & 0.018 & 0.013 & 0.015 & $\cdots$ & 0.013 & 0.015\\
 $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$\\
 Gwyddion & 1/2 & 1 & 3 & $\cdots$ & 1 & 0.036 & 0.039 & 0.045 & $\cdots$ & 0.039 & 0.041\\  
 \midrule
 SUM = & 14.01 & 25.58 & 66.75 & $\cdots$ & 25.58 & 1.000 & 1.000 & 1.000 & $\cdots$ & 1.000 & 1.000\\
 \bottomrule
\end{tabular}
\end{center}
\caption{Sample AHP Calculations for the Quality of Usability} \label{Tbl_SampleAHP}
\end{table}

We repeat the first and second steps for each of the qualities.  The third step
combines the quality rankings into an overall ranking.  Following AHP, we need
to first prioritize the qualities.  The AHP method finds the priority of quality
$i$ ($p_i$) in the same way that the score ($w_j$) was found for software
package $j$ evaluated for a given quality (as shown above).  That is, we
conduct a pairwise comparison between the priority of different qualities to
construct the $m \times m$ matrix $A$, and then we take the mean of normalized
values for row $i$ to find the priority value $p_i$ for quality $i$.  If we
introduce the notation that $w^i_j$ is the score for quality $i$ for package
$j$, then the overall score $S_j$ for package $j$ is found via:

$$S_j = \sum_{i=1}^m w^i_j p_i$$ 

\section{Methodology} \label{ch_methods}

We developed a methodology for evaluating the state of the practice of research
software \citep{SmithEtAl2021, SmithAndMichalski2022}.  The methodology can be
instantiated for a specific domain of scientific software, which in the current
case is medical imaging software for visualization.  Our methodology involves
and engages a domain expert partner throughout, as discussed in
Section~\ref{sec_vet_software_list}.  The four main steps of the methodology
are:

\begin{enumerate}
\item Identify list of representative software packages
(Section~\ref{sec_software_selection});
\item Measure (or grade) the selected software
(Section~\ref{sec_grading_software});
\item Interview developers (Section~\ref{sec_interview_methods});
\item Answer the research questions (as given in Section~\ref{sec_motivation}).
\end{enumerate}

In the sections below we provide additional detail on the above steps, while
concurrently giving examples of how we applied the methodology to the MI domain.

\subsection{Interaction With Domain Expert} \label{sec_vet_software_list}

The Domain Expert is an important member of the state of the practice assessment
team. Pitfalls exist if non-experts attempt to acquire an authoritative list of
software, or try to definitively rank the software. Non-experts have the problem
that they can only rely on information available on-line, which has the
following drawbacks:
\begin{inparaenum}[i)]
  \item the on-line resources could have false or inaccurate information; and,
  \item the on-line resources could leave out relevant information that is so
in-grained with experts that nobody thinks to explicitly record it.
\end{inparaenum}

Domain experts may be recruited from academia or industry.  The only
requirements are knowledge of the domain and a willingness to be engaged in the
assessment process.  The Domain Expert does not have to be a software developer,
but they should be a user of domain software.  Given that the domain experts are
likely to be busy people, the measurement process cannot put too much of a burden
on their time.  For the current assessment, our Domain Expert (and paper
co-author) is Dr.\ Michael Noseworthy, Professor of Electrical and Computer
Engineering at McMaster University, Co-Director of the McMaster School of
Biomedical Engineering, and Director of Medical Imaging Physics and Engineering
at St.\ Joseph's Healthcare, Hamilton, Ontario, Canada.  

In advance of the first meeting with the Domain Expert, they are asked to
create a list of top software packages in the domain.  This is done to help
the expert get in the right mind set in advance of the meeting.  Moreover,
by doing the exercise in advance, we avoid the potential pitfall of the expert
approving the discovered list of software without giving it adequate thought.

The Domain Experts are asked to vet the collected data and analysis.  In
particular, they are asked to vet the proposed list of software packages and the
AHP ranking.  These interactions can be done either electronically or with
in-person (or virtual) meetings.

\subsection{List of Representative Software} \label{sec_software_selection}

We have a two-step process for selecting software packages: i) identify software
candidates in the chosen domain; and, ii) filter the list to remove less
relevant members \citep{SmithEtAl2021}.

We initially identified 48 MI candidate software projects from the literature
\citep{Bjorn2017, Bruhschwein2019, Haak2015}, on-line articles \citep{Emms2019,
Hasan2020, Mu2019}, and forum discussions \citep{Samala2014}.  The full list of
48 packages is available in \citet{Dong2021}.  To reduce the length of the list
to a manageable number (29 in this case, as given in Section~\ref{ch_results}),
we filtered the original list as follows:

\begin{enumerate}

\item We removed the packages that did not have source code available, such as
\textit{MicroDicom}, \textit{Aliza}, and \textit{jivex}.

\item We focused on the MI software that provides visualization functions, as
described in Section~\ref{sec_scope}. Furthermore, we removed seven packages that were
toolkits or libraries, such as \textit{VTK}, \textit{ITK}, and \textit{dcm4che}.
We removed another three that were for PACS.

\item We removed \textit{Open Dicom Viewer}, since it has not received any
updates in a long time (since 2011).

\end{enumerate}

The Domain Expert provided a list of his top 12 software packages.  We compared
his list to our list of 29.  We found 6 packages were on both lists: \textit{3D
Slicer}, \textit{Horos}, \textit{ImageJ}, \textit{Fiji}, \textit{MRIcron} (we
actually use the update version \textit{MRIcroGL}) and \textit{Mango} (we
actually use the web version \textit{Papaya}).  Six software packages
(\textit{AFNI}, \textit{FSL}, \textit{Freesurfer}, \textit{Tarquin},
\textit{Diffusion Toolkit}, and \textit{MRItrix}) were on the Domain Expert
list, but not on our filtered list.  However, when we examined those packages,
we found they were out of scope, since their primary function was not
visualization.  The Domain Expert agreed with our final choice of 29 packages.

\subsection{Grading Software} \label{sec_grading_software}

We grade the selected software using the measurement template summarized in
\citet{SmithEtAl2021}.  The template provides measures of the qualities listed
in Section~\ref{sec_software_quality}, except for reproducibility, which is
assessed through the developer interviews (Section~\ref{sec_interview_methods}).
For each software package, we fill in the template questions. To stay within the
target of 160 person hours to measure the domain, we allocated between one and
four hours for each package. Project developers can be contacted for help
regarding installation, if necessary, but we impose a cap of about two hours on
the installation process, to keep the overall measurement time feasible.
Figure~\ref{fg_grading_template_example} shows an excerpt of the spreadsheet.
The spreadsheet includes a column for each measured software package. 

\begin{figure}[!ht]
\includegraphics[scale=0.66]{template.pdf}
\caption{Grading template example}
\label{fg_grading_template_example}
\end{figure}

The full template consists of 108 questions categorized under 9 qualities.  We
designed the questions to be unambiguous, quantifiable, and measurable with
limited time and domain knowledge. We group the measures under headings for each
quality, and one for summary information. The summary information (shown in
Figure~\ref{fg_grading_template_example}) is the first section of the template.
This section summarizes general information, such as the software name, purpose,
platform, programming language, publications about the software, the first
release and the most recent change date, website, source code repository of the
product, number of developers, etc.  We follow the definitions given by
\citet{GewaltigAndCannon2012} for the software categories.  Public means
software intended for public use.  Private means software aimed only at a
specific group, while the concept category is for software written simply to
demonstrate algorithms or concepts. The three categories of development models
are (open source, free-ware and commercial) are discussed in
Section~\ref{sec_software_categories}.  Information in the summary section sets
the context for the project, but it does not directly affect the grading scores.

For measuring each quality, we ask several questions and the typical answers are
among the collection of ``yes'', ``no'', ``n/a'', ``unclear'', a number, a
string, a date, a set of strings, etc. The grader assigns each quality an
overall score, between 1 and 10, based on all the previous questions.  Several
of the qualities use the word ``surface''.  This is to highlight that, for these
qualities in particular, the best that we can do is a shallow measure.  For
instance, we are not currently doing any experiments to measure usability.
Instead, we are looking for an indication that the developers considered
usability.  We do this by looking for cues in the documentation, like a getting
started manual, a user manual and a statement of expected user characteristics.
Below is a summary of how we assess adoption of best practices by measuring each
quality.

\begin{itemize}

\item \textbf{Installability} We assess the following: 
\begin{inparaenum}[i)]
    \item existence and quality of installation instructions;
    \item the quality of the user experience via the ease of following
    instructions, number of steps, automation tools; and,
    \item whether there is a means to verify the installation.
\end{inparaenum}
If any problem interrupts the process of installation or uninstallation, we give
a lower score. We also record the Operating System (OS) used for the
installation test.

\item \textbf{Correctness \& Verifiability} We check each project to identify
any techniques used to ensure this quality, such as literate programming,
automated testing, symbolic execution, model checking, unit tests, etc. We also
examine whether the projects use Continuous Integration and Continuous Delivery
(CI/CD). For verifiability, we go through the documents of the projects to check
for the presence of requirements specifications, theory manuals, and getting
started tutorials. If a getting started tutorial exists and provides expected
results, we follow it to check the correctness of the output.

\item \textbf{Surface Reliability} We check the following: 
\begin{inparaenum}[i)]
    \item whether the software breaks during installation;
    \item the operation of the software following the getting started tutorial
    (if present);
    \item whether the error messages are descriptive; and,
    \item whether we can recover the process after an error.
\end{inparaenum}

\item \textbf{Surface Robustness} We check how the software handles
unexpected/unanticipated input. For example, we prepare broken image files for
MI software packages that load image files. We use a text file (.txt) with a
modified extension name (.dcm) as an unexpected/unanticipated input. We load a
few correct input files to ensure the function is working correctly before
testing the unexpected/unanticipated ones.

\item \textbf{Surface Usability} We examine the project's documentation,
checking for the presence of getting started tutorials and/or a user manual. We
also check whether users have channels to request support, such as an e-mail
address, or issue tracker. Our impressions of usability are based on our
interaction with the software during testing.  In general, an easy-to-use
graphical user interface will score high.

\item \textbf{Maintainability} We believe that the artifacts of a project,
including source code, documents, and building scripts, significantly influence
its maintainability. Thus, we check each project for the presence of such
artifacts as API documentation, bug tracker information, release notes, test
cases, and build scripts. We also check for the use of tools supporting issue
tracking and version control, the percentages of closed issues, and the
proportion of comment lines in the code.

\item \textbf{Reusability} We count the total number of code files for each
project. Projects with numerous components potentially provide more choices for
reuse. Furthermore, well-modularized code, which tends to have smaller parts in
separate files, is typically easier to reuse. Thus, we assume that projects with
more code files and fewer Lines of Code (LOC) per file are more reusable. We also
consider projects with API documentation as delivering better reusability.

\item \textbf{Surface Understandability} Given that time is a constraint, we
cannot look at all code files for each project; therefore, we randomly examine
10 code files for their understandability. We check the code's style within each
file, such as whether the identifiers, parameters, indentation, and formatting
are consistent, whether the constants (other than 0 and 1) are not hardcoded, and
whether the code is modularized. We also check the descriptive information for
the code, such as documents mentioning the coding standard, the comments in the
code, and the descriptions or links for details on algorithms in the code. 

\item \textbf{Visibility/Transparency} To measure this quality, we check the
existing documents to find whether the software development process and
current status of a project are visible and transparent. We examine the
development process, current status, development environment, and release notes
for each project.
\end{itemize}

As part of filling in the measurement template, we use freeware tools to collect
repository related data. \href{https://github.com/tomgi/git_stats}{GitStats}
\citep{Gieniusz2019} is used to measure the number of binary files as well as
the number of added and deleted lines in a repository. We also use this tool to
measure the number of commits over different intervals of time.
\href{https://github.com/boyter/scc}{Sloc Cloc and Code (scc)}
\citep{Boyter2021} is used to measure the number of text based files as well as
the number of total, code, comment, and blank lines in a repository.

Both tools measure the number of text-based files in a git repository and lines
of text in these files. Based on our experience, most text-based files in a
repository contain programming source code, and developers use them to compile
and build software products. A minority of these files are instructions and
other documents. So we roughly regard the lines of text in text-based files as
lines of programming code. The two tools usually generate similar but not
identical results. From our understanding, this minor difference is due to the
different techniques to detect if a file is text-based or binary.

For projects on GitHub we manually collect additional information, such as the
numbers of stars, forks, people watching this repository, open pull requests,
closed pull requests, and the number of months a repository has been on GitHub.
We need to take care with the project creation date, since a repository can have
a creation date much earlier than the first day on GitHub.  For example, the
developers created the git repository for \textit{3D Slicer} in 2002, but did
not upload a copy of it to GitHub until 2020. Some GitHub data can be found
using its GitHub Application Program Interface (API) via the following url:
\textit{https://api.github.com/repos/[owner]/[repository]} where [owner] and
[repository] are replaced by the repo specific values. The number of months a
repository has been on GitHub helps us understand the average change of metrics
over time, like the average new stars per month. 

The repository measures help us in many ways. Firstly, they help us get a fast
and accurate project overview. For example, the number of commits over the last
12 months shows how active a project has been, and the number of stars and forks
may reveal its popularity (used to assess \rqref{RQ_CompareHQ2Popular}).
Secondly, the results may affect our decisions regarding the grading scores for
some software qualities. For example, if the percentage of comment lines is low,
we double-check the understandability of the code; if the ratio of open versus
closed pull requests is high, we pay more attention to maintainability.

As in \citet{SmithEtAl2016}, Virtual machines (VMs) were used to provide an
optimal testing environment for each package. We used VMs because it is easier
to start with a fresh environment, without having to worry about existing
libraries and conflicts. Moreover, when the tests are complete the VM can be
deleted, without any impact on the host operating system. The most significant
advantage of using VMs is to level the playing field. Every software install
starts from a clean slate, which removes ``works-on-my-computer'' errors. When
filling in the measurement template, the grader notes the details for each VM,
including hypervisor and operating system version.

When grading the software, we found 27 out of the 29 packages are compatible
with two or three different OSes, such as Windows, macOS, and Linux, and 5 of
them are browser-based, making them platform-independent. However, in the
interest of time, we only performed the measurements for each project by
installing it on one of the platforms.  When it was an option, we selected
Windows as the host OS.

\subsection{Interview Methods} \label{sec_interview_methods}

The repository-based measurements summarize the information we can collect from
on-line resources. This information is incomplete because it doesn't generally
capture the development process, the developer pain points, the perceived
threats to software quality, and the developers' strategies to address these
threats.  Therefore, part of our methodology involves interviewing developers.

We based our interviews on a list of 20 questions, which can be found in
\citet{SmithEtAl2021}. Some questions are about the background of the software,
the development teams, the interviewees, and how they organize their projects.
We also ask about the developer's understanding of the users. Some questions
focus on the current and past difficulties, and the solutions the team has
found, or plan to try. We also discuss documentation, both with respect to how
it is currently done, and how it is perceived. A few questions are about
specific software qualities, such as maintainability, understandability,
usability, and reproducibility. The interviews are semi-structured based on the
question list; we ask follow-up questions when necessary.  The interview process
presented here was approved by the McMaster University Research Ethics Board
under the application number
\href{https://github.com/smiths/AIMSS/blob/master/StateOfPractice/MACREM/Application.pdf}
{MREB\#: 5219}.

We sent interview requests to all 29 projects using contact information from
projects websites, code repository, publications, and from biographic pages at
the teams' institutions.  In the end nine developers from eight of the projects
agreed to participate: \textit{3D Slicer}, \textit{INVESALIUS 3}, \textit{dwv},
\textit{BioImage Suite Web}, \textit{ITK-SNAP}, \textit{MRIcroGL},
\textit{Weasis}, and \textit{OHIF}. We spent about 90 minutes for each
interview. One participant was too busy to have an interview, so they wrote down
their answers. In one case two developers from the same project agreed to be
interviewed. We held the meetings on-line using either Zoom or Teams, which
facilitated recording and automatic transcription. The full interview answers
can be found in \citet{Dong2021}.

\section{Comparison of Tool Usage Between MI and Other Research Software}
\label{Sec_CompareTools}

Developers use software tools to support the development, verification,
maintenance, and evolution of software, software processes, and artifacts
\citep[p.\ 501]{GhezziEtAl2003}. MI software uses tools for CI/CD, user support,
version control, documentation, and project management.  To answer
\rqref{RQ_CompareToolsProjMngmnt} we summarize the tool usage in these
categories, and compare this to the usage by the research software community.

Table~\ref{tab_user_support_model} summarizes the user support models by the
number of projects using each model (projects may use more than one support
model). We do not know whether the prevalent use of GitHub issues for user
support is by design, or whether this just naturally happens as users seek
help. The common use of GitHub by MI developers is not surprising, given that
GitHub is the largest code host in the world, with over 128 million public
repositories and over 23 million users (as of roughly February 26, 2020)
\citep{Kashyap2020}.

\begin{table}[!ht]
\centering
\begin{tabular}{lc}
\toprule
\multicolumn{1}{c}{User Support Model} & Num.\ Projects \\
\midrule
GitHub issue & 24 \\
Frequently Asked Questions (FAQ) & 12 \\
Forum & 10 \\
E-mail address & 9 \\
GitLab issue, SourceForge discussions & 2 \\
Troubleshooting & 2 \\
Contact form & 1 \\ 
\bottomrule
\end{tabular}
\caption{\label{tab_user_support_model}User support models by number of projects}
\end{table}

From Section~\ref{sec_score_maintainability}, 27 of the 29 projects used git as
the version control tool, one used Mercurial and one used Subversion.  The
hosting is on GitHub for 24 packages, SourceForge for three and BitBucket for
two.  Although teams may have a process for accepting new contributions, no one
discussed this during their interviews. However, most teams (eight of nine)
mentioned using GitHub and pull requests to manage contributions from the
community. The interviewees generally gave very positive feedback on using
GitHub. Some teams previously used a different approach to version control and
eventually transferred to git and GitHub.  The past approaches included
contributions from e-mail (three teams), contributions from forums (one team)
and e-mailing the git repository back and forth between developers (one team).

The common use of version control for MI software illustrates considerable
improvement from the poor adoption of version control tools that Wilson lamented
in 2006 \citep{Wilson2006}.  The proliferation of version control tools for MI
matches the increase in the broader research software community.  A little over
10 years ago \citet{Nguyen-HoanEtAl2010} estimated that only 50\% of research
software projects use version control, but even at that time
\citet{Nguyen-HoanEtAl2010} noted an increase from previous usage levels. A
survey in 2018 shows 81\% of developers use a version control system
\citep{AlNoamanyAndBorghi2018}. \citet{Smith2018} has similar results, showing
version control usage for alive projects in mesh generation, geographic
information systems and statistical software for psychiatry increasing from
75\%, 89\% and 17\% (respectively) to 100\%, 95\% and 100\% (respectively) over
a four-year period ending in 2018. (For completeness the same study showed a
small decrease in version control usage for seismology software over the same
time period, from 41\% down to 36\%).  A recent survey by \citet{CarverEtAl2022}
shows version control use among practitioners at over 95\%, with 83/87 survey
respondents indicating that they use it. All but one of the software guides
cited in Section~\ref{Sec_CompareArtifacts} includes the advice to use version
control. (The USGS guide \citep{USGS2019} was the only set of recommendations to
not mention version control.) The high usage of version control tools in MI
software matches the trend in research software in general.

As mentioned in Section~\ref{sec_result_correctness_verifiability}, we
identified five projects using CI/CD tools (about 17\% of the assessed
projects). We found which projects used CI/CD by examining the documentation and
source code of all projects. The count of CI/CD usage may actually be higher,
since traces of CI/CD usage may not always appear in a repository.  This was the
case for a study of LBM software, where interviews with developers showed that
more projects used CI/CD than was evident from repository artifacts alone
\citep{Michalski2021}.  The 17\% utilization for MI software contrasts with the
high frequency with which research software development guidelines recommend
continuous integration \citep{BrettEtAl2021, Brown2015, ThielEtAl2020,
Zadka2018, vanGompelEtAl2016}. Although there is currently little data available
on CI/CD utilization for research software, our impression is that CI/CD is not
yet common practice, despite its recommendation.  This is certainly the case for
LBM software, where usage numbers are similar to MI software, with only 12.5\%
of 24 LBM packages showing evidence of CI/CD in their repositories
\citep{Michalski2021}.  The survey of \citet{CarverEtAl2022} suggests higher use
of CI/CD with 54\% (54/100) of respondents indicating that they use it. However,
that survey measures something different from the current one by surveying
practitioners, rather than assessing projects.  Additional information on CI/CD
is given in the recommendations (Section~\ref{Sec_ContinuousIntegration}).

For documentation tools and methods mentioned by the interviewees, the most
popular (mentioned by about 30\% of developers) were forum discussions and
videos.  The second most popular options (mentioned by about 20\% of developers)
were GitHub, wiki pages, workshops, and social media. The least frequently
mentioned options (about 10\% of developers) included writing books, and google
forms.  In contrasting MI software with LBM software, the most significant
documentation tool difference is that LBM software often uses document
generation tools, like doxygen and sphinx \citep{Michalski2021}, while MI does
not appear to use these tools. 

Some interviewees mentioned the project management tools they used. Generally
speaking, the interviewees talked about two types of tools:
\begin{inparaenum}[i)]
\item trackers, including GitHub, issue trackers, bug trackers and Jira; and,
\item documentation tools, including GitHub, Wiki page, Google Doc, and
Confluence.
\end{inparaenum}
Of the specifically named tools in the above lists, interviewees mentioned
GitHub 3 times, and each of the other tools once each.

Based on information provided by \citet{JungEtAl2022}, tool utilization for MI
software has much in common with tool utilization for ocean modelling software.
Both use tools for editing, compiling, code management, testing, building, and
project management.  From the data available, ocean modelling differs from MI
software in its use of Kanban boards for project management.

\section[Comparison to Other Research Software]{Comparison of Principles, Process, and
Methodologies to Research Software in General} \label{Sec_CompareMethodologies}

We answer research question \rqref{RQ_CompareMethodologies} by comparing the
principles, processes, and methodologies used for MI software to what can be
gleaned from the literature on research software in general. In our interviews
with developers the responses about development model were vague, with only two
interviewees following a definite development model. In some cases the
interviewees felt their process was similar to an existing development model.
Three teams (about 38\%) either followed agile, or something similar to agile.
Two teams (25\%) either followed a waterfall process, or something similar.
Three teams (about 38\%) explicitly stated that their process was undefined or
self-directed.

Our observations of an informally defined process, with elements of agile
methods, matches what has been observed for research software in general.
Scientific developers naturally use an agile philosophy \citep{AckroydEtAl2008,
CarverEtAl2007, EasterbrookAndJohns2009, Segal2005, HeatonAndCarver2015}, or an
amethododical process \citep{Kelly2013}, or a knowledge acquisition driven
process \citep{Kelly2015}.  A waterfall-like process can work for research
software \citep{Smith2016}, especially if the developers work iteratively and
incrementally, but externally document their work as if they followed a
rationale design process \citep{parnas1986rational}.

No interviewee introduced any strictly defined project management process. The
most common approach was following the issues, such as bugs and feature
requests. Additionally, the \textit{3D Slicer} team had weekly meetings to
discuss the goals for the project; the \textit{INVESALIUS 3} team relied on the
GitHub process for their project management; the \textit{ITK-SNAP} team had a
fixed six-month release pace; only the interviewee from the \textit{OHIF} team
mentioned that the team has a project manager; the \textit{3D Slicer} team and
\textit{BioImage Suite Web} team do nightly builds and tests. The \textit{OHIF}
developer believes that a better project management process can improve junior
developer efficiency while also improving internal and external communication.

We identified the use of unit testing in less than half of the 29 projects. On
the other hand, the interviewees believed that testing (including usability
tests with users) was the top solution to improve correctness, usability, and
reproducibility.  This level of testing matches what was observed for LBM
software \citep{Michalski2021} and is apparently greater than the level of
testing for ocean modelling software.  \citet{JungEtAl2022} reports that ocean
modellers underemphasize testing.

As the observed artifacts in Table~\ref{artifactspresent} show, none of the 29
projects emphasize documentation. None of them had theory manuals, although we
did identify a road map in the \textit{3D Slicer} project.  We did not find
requirements specifications. Table~\ref{tab_opinion_doc} summarizes
interviewees' opinions on documentation. Interviewees from each of the eight
projects thought that documentation was essential to their projects, and most of
them said that it could save their time to answer questions from users and
developers. Most of them saw the need to improve their documentation, and only
three of them thought that their documentations conveyed information clearly
enough. Nearly half of developers also believed that the lack of time prevented
them from improving documentation.

\begin{table}[!ht]
\centering
\begin{tabular}{ll}
\toprule
Opinion on Documentation & Num.\ Ans. \\ 
\midrule
Documentation is vital to the project & 8 \\
Documentation of the project needs improvements & 7 \\
Referring to documentation saves time to answer questions & 6 \\
Lack of time to maintain good documentation & 4 \\
Documentation of the project conveys information clearly & 3 \\
Coding is more fun than documentation & 2 \\
Users help each other by referring to documentation & 1 \\ 
\bottomrule
\end{tabular}
\caption{Opinions on documentation by the numbers of interviewees with the
answers}
\label{tab_opinion_doc}
\end{table}

As Table~\ref{Tbl_Guidelines} suggests, an emphasis on documentation, especially
for new developers, is echoed in research software guidelines. Multiple
guidelines recommend a document explaining how to contribute to a project, often
named CONTRIBUTING. Guidelines also recommend tutorials, user guides and quick
start examples. \citet{SmithAndRoscoe2018} suggests including instructions
specifically for on-boarding new developers. For open-source software in general
(not just research software), \citet{Fogel2005} recommends providing tutorial
style examples, developer guidelines, demos, and screenshots.

\section{Developer Pain Points} \label{painpoints}

Based on interviews with nine developers (described in
Section~\ref{sec_interview_methods}), we answer three research questions (first
mentioned in Section~\ref{sec_motivation}): \rqref{RQ_PainPoints}) What are the
pain points for developers working on research software projects?;
\rqref{RQ_ComparePainPoints}) How do the pain points of developers from MI
compare to the pain points for research software in general?; and
\rqref{RQ_Concerns}) For MI developers what specific best practices are taken to
address the pain points and software quality concerns? 

Our interviews identified pain points related to a lack of time and funding,
technology hurdles, improving correctness, and improving usability.  In this
section, we go through each pain point and contrast the MI experience with
observations from other domains.  We also cover potential ways to address the
pain points, as promoted by the community.  (Later, in
Section~\ref{ch_recommendations}, we propose additional pain mitigation
strategies based on our experience.)  In addition to pain points, we summarize
MI developer strategies for improving maintainability and reproducibility.
Although the interviewees did not explicitly identify these two qualities as
pain points, we did discuss threats to these qualities and ways to improve them
as part of our interview process \citep{SmithEtAl2021}.  The interviewee's
practices for addressing pain points and improving quality can potentially be
emulated by other MI developers. Moreover, these practices may provide examples
that can be followed by other research software domains.

\citet{PintoEtAl2018} lists some pain points that did not come up in our
conversations with MI developers: interruptions while coding, scope bloat, lack
of user feedback, hard to collaborate on software projects, and aloneness.
\citet{WieseEtAl2019} also mention two research software pain points that did
not explicitly arise in our interviews: reproducibility, and software scope
determination.  To the list of pain points not discussed for MI, our study of
LBM software \citep{SmithEtAl2024} adds lack of software experience for the
developers, technical debt, and documentation. We did not observe any pain
points for MI that were not also observed for LBM. From the pain points
mentioned above, although the topics of reproducibility and technical debt did
not come up in our MI interviews, we covered these two topics as part of the
discussion of software qualities, as summarized at the end of this section.
Although previous studies show pain points that were not mentioned by MI
developers, we cannot conclude that these pain points are not relevant for MI
software development, since we only interviewed nine developers for about an
hour each.

\begin{enumerate}

\item[P\refstepcounter{pnum}\thepnum \label{P_LackDevTime}:] \textbf{Lack of
Development Time:} Many interviewees thought lack of time, along with lack of
funding (discussed next), were their most significant obstacles. Other domains
of research software also experience the lack of time pain point
\citep{PintoEtAl2018, PintoEtAl2016, WieseEtAl2019}. Our study of LBM software
\citep{SmithEtAl2024} also highlighted lack of time as a significant pain point.

Potential and proven solutions suggested by the interviewees include:

\begin{itemize}
\item Shifting from development to maintenance when the team does not have
enough developers for building new features and fixing bugs at the same time;
\item Improving documentation to save time answering users' and developers'
questions;
\item Supporting third-party plugins and extensions; and,
\item Using GitHub Actions for CI/CD (Continuous Integration and Continuous
Delivery.)
\end{itemize}

\item[P\refstepcounter{pnum}\thepnum \label{P_LackFunding}:] \textbf{Lack of
Funding:} Developers felt the pain of having to attract funding to develop and
maintain their software. For instance, the interviewees from \textit{3D Slicer}
and \textit{OHIF} said getting funding for software maintenance is more
challenging than finding funding for research. The interviewee from 
\textit{ITK-SNAP} thought more funding was a way to solve the lack of time
problem, because they could hire more dedicated developers. On the other hand,
the interviewee from \textit{Weasis} did not feel that funding could
solve the same problem, since they would still need time to supervise the project. 

Funding challenges have also been noted by others \citep{GewaltigAndCannon2012,
Goble2014, KaterbowAndFeulner2018, SmithEtAl2024}. Researchers that devote time
to software have the additional challenge that funding agencies do not always
count software when they are judging the academic excellence of the applicant.
\citet{WieseEtAl2019} reported developer pains related to publicity, since
publishing norms have historically made it difficult to get credit for creating
software.  As studied by \citet{HowisonAndBullard2016}, research software
(specifically biology software, but the trend likely applies to other research
software domains) is infrequently cited. \citet{PintoEtAl2018} also mentions the
lack of formal reward system for research software.

An interviewee proposed an idea for increasing funding: Licensing the software
to commercial companies to integrate it into their products.
    
\item[P\refstepcounter{pnum}\thepnum \label{P_TechnologyHurdles}:]
\textbf{Technology Hurdles:} The technology hurdles mentioned by MI developers
include: hard to keep up with changes in OS and libraries, difficult to transfer
to new technologies, hard to support multiple OSes, and hard to support lower-end
computers. Developers expressed difficulty balancing between four factors:
cross-platform compatibility, convenience to development and maintenance,
performance, and security.

The pain point survey of \citet{WieseEtAl2019} highlights that technology
hurdles are an issue for research software in general.  Some technical-related
problems mentioned by \citet{WieseEtAl2019} include dependency management,
cross-platform compatibility (also mentioned by \citet{PintoEtAl2018}), CI,
hardware issues and operating system issues. From \citep{SmithEtAl2024}
technology pain points for LBM developers include setting up parallelization and
CI. 

The solutions proposed by the MI developers include the following:

\begin{itemize}
\item Adopting a web-based approach with backend servers, to better support
lower-end computers;
\item Using memory-mapped files to consume less computer memory, to better
support lower-end computers; 
\item Using computing power from the computers GPU for web applications;
\item Maintaining better documentations to ease the development and maintenance
processes;
\item Improving performance via more powerful computers, which one interviewee
pointed out has already happened.
\end{itemize}

As the above list shows, developers perceive that web-based applications will
address the technology hurdle.  Table~\ref{tab_native_vs_web} shows the teams'
choices between native application and web application. Most of the 29 teams (24
of 29, or 83\%) chose to develop native applications. For the eight teams we
interviewed, three of them were building web applications, and the
\textit{MRIcroGL} team was considering a web-based solution.

\begin{table}[!ht]
\centering
\begin{tabular}{lll}
\toprule
Software Team & Native Application & Web Application \\ 
\midrule
3D Slicer & \checkmark & \\
INVESALIUS 3 & \checkmark & \\
dwv & & \checkmark \\
BioImage Suite Web & & \checkmark \\
ITK-SNAP & \checkmark & \\
MRIcroGL & \checkmark & \\
Weasis & \checkmark & \\
OHIF & & \checkmark \\ 
\midrule
Total number among the eight teams & 5 & 3 \\
Total number among the 29 teams & 24 & 5 \\ 
\bottomrule
\end{tabular}
\caption{Teams' choices between native application and web application}
\label{tab_native_vs_web}
\end{table}

The advantage for native applications is higher performance, while web
applications have the advantage of cross-platform compatibility and a simpler
build process.  These web advantages mirror the native disadvantages of
difficulty with cross-platform compatibility and a complex build process.  The
lower performance disadvantage of web applications can be improved with a server
backend, but in this case there are disadvantages for privacy protection and
server costs.  These issues are discussed further in the recommendations
(Section~\ref{sec_webapps}).

\item[P\refstepcounter{pnum}\thepnum \label{P_Correctness}:]
\textbf{Ensuring Correctness:} Interviewees identified multiple threats to
correctness.  The most frequently mentioned threat was complexity.  Complexity
enters the software by various means, including the large variety of data formats,
complicated data standards, differing outputs between medical imaging machines,
and the addition of (non-viewing related) functionality.  Other threats to
correctness identified include the following:

\begin{itemize}
\item Lack of real world image data for testing, in part because of patient
privacy concerns (\citet{WieseEtAl2019} mentions that the pain point of privacy
concerns also arises for research software in general);
\item Tests are expensive and time-consuming because of the need for huge datasets;
\item Software releases are difficult to manage;
\item No systematic unit testing; and,
\item No dedicated quality assurance team.
\end{itemize}

As implied by the above threats to correctness, testing was the most often
mentioned strategy for MI developers for ensuring correctness.  Seven teams
mentioned test related activities, including test-driven development, component
tests, integration tests, smoke tests, regression tests, self tests and
automated tests.  With the common emphasis on testing to improve correctness, MI
software is ahead of some other scientific domains.  For scientific software in
general \citet{PintoEtAl2018} mention the problem of insufficient testing and
\citet{HannayEtAl2009} show that more developers think testing is important than
the number that believe they have a sufficient understanding of testing
concepts.  Our study of LBM software suggests that this domain shares the
challenges of insufficient testing and insufficient understanding of testing
concepts \citep{SmithEtAl2024}. Automated testing is a specific challenge for
LBM software since free testing services do not offer adequate facilities for
large amounts of data \citep{SmithEtAl2024}. Although not specifically mentioned
during our interviews, the large data sets for MI likely also cause a challenge
for using free testing services, like GitHub Actions.

Research software in general often struggles with the oracle problem for testing
because for many potential test cases the developer doesn't have a means to
judge the correctness of their calculated solutions \citep{HannayEtAl2009,
KanewalaAndBieman2013, KellyEtAl2011, WieseEtAl2019}.  The MI developers did not
allude to this challenge, likely because for a give image (test case) it is
possible to determine, potentially by using other software, the expected
analysis results.

A frequently cited strategy for building confidence in correctness (mentioned by
3 interviewees) is a two state development process with stable releases and
nightly builds.  Other strategies for ensuring correctness that came up during
the interviews include CI/CD, using de-identified copies of medical images for
debugging, sending beta versions to medical workers who can access the data to
do the tests, and collecting/maintaining a dataset of problematic images.  Some
additional strategies used by MI developers include:

\begin{itemize}
\item Using open datasets.
\item If (part of) the team belongs to a medical school or a hospital, using the
datasets they can access;
\item If the team has access to MRI scanners, self-building sample images for
testing;
\item If the team has connections with MI equipment manufacturers, asking for
their help on data format problems;
\end{itemize}

The feedback from the interviewees makes it clear that increased connections
between the development team and medical professionals/institutions could ease
the pain of ensuring correctness via testing.

\item[P\refstepcounter{pnum}\thepnum \label{P_Usability}:]
\textbf{Usability:}  

The discussion with the developers focused on usability issues for two classes
of users: the end users and other developers.  The threats to usability for end
users include an unintuitive user interface, inadequate feedback from the
interface (such as lack of a progress bar), users being unable to determine the purpose of
the software, not all users knowing if the software includes certain features, not
all users understanding how to use the command line tool, and not all users
understanding that the software is a web application. For developers the threats to
usability include not being able to find clear instructions on how to deploy the
software, and the architecture being difficult for new developers to understand.

At least to some extent the problems for MI software users are due to holes in
their background knowledge.  The survey of \citet{WieseEtAl2019} for research
software in general also mentioned that users do not always have the expertise
required to install or use the software. \citet{SmithEtAl2024} observes a
similar pattern for LBM software, with several LBM developers noting that users
sometimes try to use incorrect method combinations. Furthermore, some LBM users
think that the packages will work out of the box to solve their cases, while in
reality computational fluid dynamics knowledge needs to be applied to correctly
modify the packages for a new endeavour.

To improve the usability of MI software, the most common strategies mentioned by
developers are as follows:

\begin{itemize}
    \item Use documentation (user manuals, mailing lists, forums) (mentioned by
    4 developers)
    \item Usability tests and interviews with end users; and, (mentioned by 3
    developers)
    \item Adjusting the software according to user feedback. (mentioned by 3
    developers)
\end{itemize}

Other suggested and practiced strategies include a graphical user interface,
testing every release with active users, making simple things simple and
complicated things possible, focusing on limited number of functions, icons with
clear visual expressions, designing the software to be intuitive, having a UX
(User eXperience) designer, dialog windows for important notifications,
providing an example for users to follow, downsampling images to consume less
memory, and providing an option to load only part of the data to boost
performance.  The last two points recognize that an important component of
usability is performance, since poor performance frustrates users.

\end{enumerate}

Up to this point, we have covered the pain points that came up in interviews
with MI developers, along with a summary of the techniques that are currently
used to address these pain points.  Although the developers did not explicitly
identify the qualities of maintainability and reproducibility as pain points in
our interviews, as part of our interview questions
(Section~\ref{sec_interview_methods}) they did share their approaches for
improving these qualities, as discussed below.

\begin{enumerate}
\item[Q\refstepcounter{qnum}\theqnum \label{Q_Maintainability}:]
\textbf{Maintainability:} \citet{Nguyen-HoanEtAl2010} rate maintainability as the
third most important software quality for research software in general. The push
for sustainable software \citep{deSouzaEtAl2019} is motivated by the pain that
past developers have had with accumulating too much technical debt
\citep{KruchtenEtAl2012}.  For LBM software, \citet{SmithEtAl2024} identifies
technical debt as one of the developer pain points.

To improve maintainability, the most popular (with five out of nine interviewees
mentioning it) strategy is to use a modular approach, with often repeated
functions in a library.  Other strategies that were mentioned for improving
maintainability include supporting third-party extensions, an easy-to-understand
architecture, a dedicated architect, starting from simple solutions, and
documentation.  The \textit{3D Slicer} team used a well-defined structure for
the software, which they named as an ``event-driven MVC pattern''. Moreover,
\textit{3D Slicer} discovers and loads necessary modules at runtime, according
to the configuration and installed extensions. The \textit{BioImage Suite Web}
team had designed and re-designed their software multiple times in the last 10+
years. They found that their modular approach effectively supports
maintainability \citep{Joshi2011}. 

\item[Q\refstepcounter{qnum}\theqnum \label{Q_Reproducibility}:]
\textbf{Reproducibility:}  Although the MI developers did not mention
reproducibility explicitly as a pain point, they did mention the need to improve
documentation.  Good documentation does not just address the pain points of lack
of developer time (\ppref{P_LackDevTime}), technology hurdles
(\ppref{P_TechnologyHurdles}), usability \ppref{P_Usability}, and
maintainability.  Documentation is also necessary for reproducibility. The
challenges of inadequate documentation are a known problem for research software
\citep{PintoEtAl2018, WieseEtAl2019} and for non-research software
\citep{LethbridgeEtAl2003}. 

In our interviews, we discussed threats to reproducibility and strategies for
improving it.  The threats that were mentioned include closed-source software,
no user interaction tests, no unit tests, the need to change versions of some
common libraries, variability between CPUs, and misinterpretation of how
manufacturers create medical images. 

The most commonly cited (by 6 teams) strategy to improve reproducibility was
testing (regression tests, unit tests, having good tests). The second most
common strategy (mentioned by 5 teams) is making code, data, and documentation
available, possibly by creating open-source libraries.  Other ideas that were
mentioned include running the same tests on all platforms, a dockerized version
of the software to insulate it from the OS environment, using standard
libraries, monitoring the upgrades of the library dependencies, clearly
documenting the version information, bringing along the exact versions of all
the dependencies with the software, providing checksums of the data, and
benchmarking the software against other software that overlaps in functionality.
Specifically one interviewee suggested using \textit{3D Slicer} as the benchmark
to test their reproducibility.

\end{enumerate}

\section{Recommendations} \label{ch_recommendations}

In this section we provide recommendations to address the pain points from
Section~\ref{painpoints} to answer~\rqref{RQ_Recommend}.  Our recommendations
are not lists of criticisms for what should have been done in the past, or what
should be done now; they are suggestions for consideration in the future. We
expand on some of the ideas that came out of our interviews with developers
(Section~\ref{painpoints}), including continuous integration, moving to web
applications, and enriching the test data sets. We also bring in new ideas from
our experience like employing linters, peer review, design for change and
assurance cases.  Our aim is to mention ideas that are at least somewhat beyond
conventional best practices. The ideas listed here have the potential to become
best practices in the medium to long-term. We list the ideas roughly in the
order of increasing implementation effort.

\subsection{Use Continuous Integration} \label{Sec_ContinuousIntegration}

Continuous integration involves frequent pushes to a code repository.  With
every push the software is built and tested \citep[p.\ 13]
{HumbleAndFarley2010}, \citep{ShahinEtAl2017, Fowler2006}.  CI can take
significant time and effort to set up and integrate into a team's workflow, but
the benefits are significant, as follows:

\begin{itemize}
	\item Elimination of headaches associated with a separate integration phase
	\citep{Fowler2006}, \citep[p.\ 20]{HumbleAndFarley2010}. If developers
	postpone integration, integration problems are inevitable.  Continuous
	integration means that problems are immediately obvious and the source of
	the problem can be isolated to the small increment that was just committed.
	\item Detection and removal of bugs \citep{Fowler2006} via
	automated testing.  To improve productivity, defects are best discovered and
	fixed at the point where they are introduced \citep[p.\
	23]{HumbleAndFarley2010}.  Code is not the only source of errors; they are
	also found in the files and scripts related to configuration management
	\citep[p.\ 18]{HumbleAndFarley2010}.
	\item Everyone is always working on a stable base, since the rejection of
	inadequate commits means that the main branch will always be working.  A
	stable base will always pass all tests.  If the CI system uses generators
	and linters, it will also have current documentation and standard compliant
	code.  A stable base improves developer productivity, allowing them to focus
	on coding, testing, and documentation.
\end{itemize}

CI consists of the following elements:

\begin{itemize}
	\item A version control system \citep{Fowler2006}. To be effective, all
	files should be under version control, not just code files.  Anything that
	is needed to build, install and run the software should be under version
	control, including configuration files, build scripts, test harnesses, and
	operating system configuration files \citep[p. 19]{HumbleAndFarley2010}.
	Fortunately for the MI, as shown in Section~\ref{sec_score_maintainability}
	all our measured projects use version control.
	\item A fully automated build system \citep{Fowler2006}.  As \citet[p.\
	5]{HumbleAndFarley2010} point out, deploying software manually is an
	anti-pattern.  For MI software, Table~\ref{artifactspresent} shows 18 of 29
	packages (62\%) were observed to include build scripts.  Projects without a
	build system will need to add one to pursue using CI.
	\item An automated test system \citep{Fowler2006}. Building quality software
	involves creating automated tests at the unit, component, and acceptance
	test level, and executing these tests whenever someone makes a change to the
	code, its configuration, the environment, or the software stack that it runs
	on \citep[p.\ 83]{HumbleAndFarley2010}. As Table~\ref{artifactspresent}
	shows, test cases are in the uncommon category for MI software artifacts,
	which means that some MI projects will need to increase their testing
	automation if they wish to pursue CI.
	\item An automated system for other tasks, such as code checking,
	documentation building and web-site updating.  These other tasks are not
	essential to CI, but they can be incorporated to improve the quality
	of the code and the communication between developers and users. For
	instance, a static analysis (possibly via linters) of the code may find poor
	programming practice or lack of adherence to adopted coding standards.
	\item An integrated build system to pull everything together.  Every time
	there is a check-in (for instance a pull request), the integration server
	automatically checks out the sources onto the integration machine, starts a
	build, runs tests, and informs the committer of the results. 
\end{itemize}

To enable incorporation into a team's workflow, \citet[p.\
60]{HumbleAndFarley2010} explain that the usual approach for CI is to keep the
build and test process short. Since MI files are large, the tests run with every
check-in may need to focus on simple code interface tests, saving large tests
for less frequent execution.  A more sophisticated option to address the
bottleneck for merges is CIVET (Continuous Integration, Verification,
Enhancement, and Testing), which solves this problem by intelligently pinning,
cancelling, and if necessary, restarting jobs as merges occur
\citep{SlaughterEtAl2021}. A more sophisticated process management system can
also enforce rules for pull requests, like checking that a test specification
includes the test's motivation, a test description, and a design description for
all changes \citet{SlaughterEtAl2021}. 

Setting up a CI system has never been easier than it is today.  A dedicated CI
server (either physically or virtually) can be installed with tools such as
\href{https://www.jenkins.io/} {Jenkins}, \href{http://buildbot.net/}
{Buildbot}, \href{https://www.gocd.org/} {Go}, and
\href{http://integrity.github.io/} {Integrity}. However, installation on your
own server is often unnecessary since there are many hosted CI solutions, such
as: \href{https://travis-ci.org/} {Travis CI},
\href{https://github.com/features/actions} {GitHub Actions} and
\href{https://circleci.com/} {CircleCI}.  All that is required to begin using a
hosted CI is to select the service and then edit a few lines of a YAML
configuration file in the project's root directory.

\citet{ShahinEtAl2017} highlights the following challenges for adopting CI: lack
of awareness and transparency, lack of expertise and skills, coordination and
collaboration challenges, more pressure and workload for team members, general
resistance to change, scepticism and distrust on continuous practices. The most
common reason given for not adopting CI is that developers are not familiar
enough with CI \citep{HiltonEtAl2016}.  \citet{ShahinEtAl2017} observes that
these problems can be mitigated via improving testing activities, planning and
documentation, promoting a team mindset, adopting new rules and policies, and
decomposing development into smaller units.

Continuous integration and delivery helps with addressing several pain points.
For instance, CI/CD helps reduce development time (\ppref{P_LackDevTime}) by
removing the need for a time-consuming integration stage and by automating
regression testing.  Automated regression tests also help with ensuring
correctness (\ppref{P_Correctness}) and the quality of reproducibility
(\qref{Q_Reproducibility}).

\subsection{Move To Web Applications} \label{sec_webapps}

Section~\ref{painpoints} describes the pain point of technology hurdles
(\ppref{P_TechnologyHurdles}), which motivates considering the use of web
applications. Here we give further advice to help with deciding whether to adopt
a web application. The decision will be based on whether, on balance, the web
application improves the four factors identified by developers: compatibility,
maintainability, performance, and security. To enable decision-making, a team
will need to prioritize between these factors, based on their objectives and
experience. The suggestions are intended to provide ideas and avenues for
exploration; a web application will not be the right fit for all projects and
all teams.

\begin{itemize}

\item \textbf{Modern technologies may improve frontend performance.} Web
applications with only a frontend usually perform worse than native
applications. However, new technologies may ease this difference. For example,
some JavaScript libraries can help the frontend harness the power of the
computer's GPU and accelerate graphical computing. In addition, there are new
frameworks helping developers with cross-platform compatibility. For example,
the \href{https://flutter.dev/}{Flutter} project enables support for web,
mobile, and desktop OS with one codebase.  Other options include
\href{https://vuejs.org/} {Vue}, \href{https://angular.io/} {Angular} and
\href{https://reactjs.org/} {React}, and \href{https://elm-lang.org/}{Elm}.  

\item \textbf{Backend servers can potentially deliver high performance.} Web
applications with backend servers may perform even better than native
applications. If a team needs to support lower-end computers, it is good to use
back-end servers for heavy computing tasks.  For backend servers where traffic
and latency is not an issue, options include
\href{https://www.django-rest-framework.org/} {Django}, \href{https://laravel.com/} {Laravel} and
\href{https://nodejs.org/en/} {Node.js}.  The advantage of Django is that it provides access to Python
libraries.  For backend servers where traffic and latency is an issue,
\href{https://github.com/gin-gonic/gin} {Gin} is an option.

\item \textbf{Backend servers can have low costs.} Serverless solutions from
major cloud service providers (like Amazon Web Services (AWS) and Google Cloud
Platform) may be worth exploring. Serverless solutions still use a server, but
the server provider only charges the team when they use the server. The solution
is event-driven, and costs the team by the number of requests processed. Thus,
serverless can be very cost-effective for less intensively used functions.

\item \textbf{Web transmission may diminish security.} Transferring sensitive
data on-line can be a problem for projects requiring high security. Regulations
for some MI applications may forbid doing web transmissions. In this case, a web
application with a backend may not be an option.

\end{itemize}

\subsection{Enrich the Testing Datasets} 
\label{sec_recommendations_testing_dataset}

As described in Section~\ref{painpoints}, ensuring correctness
(\ppref{P_Correctness}) via testing can be problematic because of limited access
to real-world medical imaging datasets.  We build on the suggestions we heard
from our interviewees as follows:

\begin{itemize}
\item \textbf{Build and maintain good connections to datasets.} A team can build
connections with professionals working in the medical domain, who may have
access to private datasets and can perform tests for the team. If a team has
such professionals as internal members, the process can be simplified.

\item \textbf{Collect and maintain datasets over time.} A team may face problems
caused by various unique inputs over the years of software development. This
data should be collected and maintained over time to form a good, comprehensive,
dataset for testing.

\item \textbf{Search for open data sources.} In general, there are many open MI
datasets.  For instance, there are
\href{https://nihcc.app.box.com/v/ChestXray-NIHCC}{Chest X-ray Datasets} by
National Institute of Health \citep{WangEtAl2017},
\href{https://www.cancerimagingarchive.net/}{Cancer Imaging Archive}
\citep{PriorEtAl2017}, \href{https://medpix.nlm.nih.gov/home}{MedPix} by
National Library of Medicine \citep{Smirniotopoulos2014}, and datasets for liver
\citep{BilicEtAl2019} and brain \citep{MenzeEtAl2015} tumor segmentation
benchmarks.  A team developing MI software should be able to find more open
datasets according to their needs.

\item \textbf{Create sample data for testing.} If a team can access tools
creating sample data, they may also self-build datasets for testing. For
example, an MI software development team can use an MRI scanner to create images
of objects, animals, and volunteers. The team can build the images based on
specific testing requirements.

\item \textbf{Remove privacy from sensitive data.} For data with sensitive
information, a team can ask the data owner to remove such information or add
noise to protect privacy. One example is using de-identified copies of medical
images for testing.

\item \textbf{Establish community collaboration in the domain.} During our
interviews with developers in the MI domain, we heard many stories of asking for
supports from other professionals or equipment manufacturers. However, we
believe that broader collaboration between development teams can address this
problem better. Some datasets are too sensitive to share, but if the community
has some kind of ``group discussion'', teams can better express their needs, and
professionals can better offer voluntary support for testing. Ultimately, the
community can establish a nonprofit organization as a third party, which
maintains large datasets, tests Open Source Software (OSS) in the domain, and
protects privacy. 

\end{itemize}

\subsection{Employ Linters} \label{Sec_Linters}

A linter is a tool that statically analyzes code to find programming errors,
suspicious constructs, and stylistic inconsistencies \citep{Wikipedia2022_Lint}.
Linters can be used as an ad hoc check for code files, but they really come into
their own when used as part of a CI system, as discussed in
Section~\ref{Sec_ContinuousIntegration}. Almost none of the research software
guidelines that we consulted, summarized in Section~\ref{Sec_CompareArtifacts},
mention linters.  The one exception is \citet{ThielEtAl2020}.  Despite the lack
of mention in the guidelines, we believe that linters have the potential to
improve code quality at a relatively low cost.  

Linters have the following benefits: finding potential bugs, finding memory
leaks, improving performance, standardizing code with respect to formatting,
removing silly errors before code reviews, and catching potential security
issues \citep{SourceLevel2022_Lint}. Most popular programming languages have an
accompanying linter.  For example, Python has the options of PyLint, flake8 and
Black \citep{Zadka2018}.

We recommend the use of linters because they are relatively easy to incorporate
into a developer's workflow, and they address several MI pain points
(Section~\ref{painpoints}).  For instance, linters address the lack of
development time (\ppref{P_LackDevTime}) by increasing the developer's
productive time via guarding against making frustrating, time-consuming, mundane
mistakes.  Moreover, since a linter can include rules that capture the wisdom of
senior programmers, it can help newer developers avoid common mistakes. With
respect to the technology hurdle pain point (\ppref{P_TechnologyHurdles}),
linters can assist with the move toward web applications
(Section~\ref{sec_webapps}).  For instance, ESLint in React is a pluggable
linter that lets the developer know if they have imported something and not used
it, if a function could be short-handed, if there are indentation
inconsistencies, etc. \citep{Whitehouse2018}. By insisting on code
standardization linters can reduce technical debt and thus improve
maintainability (\qref{Q_Maintainability}). Although linters are tools for code
analysis, the idea of statically checking for adherence to basic rules can be
extended to check documentation. \citet{SmithEtAl2018_StatSoft} shows how the
use of tools to enforce documentation standards partially explains the
relatively higher quality of statistical tools that are part of the
Comprehensive R Archive Network (CRAN).

\subsection{Conduct a Mix of Rigorous and Informal Peer Reviews} \label{Sec_PeerReview}

We advocate incorporating peer review into the development process, as
frequently recommended for research software \citep{HerouxEtAl2008, Givler2020,
OrvizEtAl2017, USGS2019}. In most cases a modern, lightweight review, should be
adequate.  Modern code review is informal, tool-based, asynchronous, and focused
on reviewing code changes \citep{SadowskiEtAl2018}. Managing a project via
GitHub pull requests is an example of a modern approach to reviewing code.
Software development organizations have moved to this lightweight style of code
review because of the inefficiencies of rigorous inspections
\citep{RigbyAndBird2013}.  However, for important parts of the code, developers
may benefit from mixing in a more rigorous approach. 

\citet{Fagan1976} began work on rigorous review via code inspection.  Elements
of a typical inspection include reviewing the code against a checklist (checking
the consistency of variable names, look for terminating loops, etc.), performing
specific review tasks (such as summarizing the code's purpose, cross-referencing
the code to the technical manual, creating a data dictionary for a given module,
etc.) Rigorous inspection finds 60-65\% of latent defects on average, and often
tops 85\% in defect removal efficiency \citep{Jones2008}. The success rate of
code inspection is generally higher than most forms of testing, which average
between 30 --- 35\% for defect removal efficiency \citep{EbertAndJones2009,
Jones2008}. For research software, \citep{KellyAndShepard2000} show a task based
inspection approach can be effective. Task based inspection is an ideal fit with
an issue tracking system, like GitHub.  The review tasks can be issues, so that
they can be easily assigned, monitored and recorded. Potential issues include
assigning junior developers to test getting-started tutorial and installation
instructions.

As indicated in Section~\ref{Sec_CompareMethodologies} some MI projects use
modern code review, via issue tracking and the use of GitHub.  Those MI projects
not incorporating modern code review would likely benefit by adopting it.
Although a rigorous code inspection is likely not worth the required resources,
for critical parts of the code, developers may want to adopt a more rigorous
approach. For instance, developers may drop the modern trend of
asynchronous review and instead occasionally use synchronous review to help
uncover errors and disseminate best practices throughout the team.  For
instance, teams could periodically meet, either in-person or virtually, and have
junior members walk through their code.  In-person reviews will likely help
realize the benefits of modern code review noticed by
\citet{BirdAndBacchelli2013}: defect detection, knowledge transfer, increased
team awareness, and creation of alternative solutions to problems.

Due to improving code quality and increasing knowledge transfer, peer review
addresses the same pain points and qualities as linters
(Section~\ref{Sec_Linters}): \ppref{P_LackDevTime}, \ppref{P_TechnologyHurdles},
and \qref{Q_Maintainability}. Peer review can potentially find misunderstandings
in how the code implements the required theory, which will improve the
software's correctness (\ppref{P_Correctness}). The benefits of peer review for
addressing pain points can be increased by extending the review from just code,
to also reviewing all software artifacts, including documentation, build
scripts, test cases and the development process itself.

\subsection{Design For Change} \label{Sec_DesForChange}

In our ``state of the practice'' assessment exercise for LBM software
\citep{SmithEtAl2024}, we noticed that LBM developers implicitly used
modularization based on the principle of design for change to improve
maintainability (\qref{Q_Maintainability}).  We recommend that MI developers use
the same principle for their modularizations.  Although the advice to modularize
research software to handle complexity is common \citep{WilsonEtAl2014,
StewartEtAl2017, Storer2017}, specific guidelines on how to divide the software
into modules is less prevalent.  Not every decomposition is a good design for
supporting change, as shown by \citet{Parnas1972a}.  For instance, a design with
low cohesion and high coupling \citep[p.\ 48]{GhezziEtAl2003} will make change
difficult. Especially in research software, where change is inevitable,
designers need to produce a modularization that supports change.
\citet{JungEtAl2022} points out that ocean modelling software is currently
feeling the pain of not emphasizing modularization in legacy code.

Specific examples of design for change for LBM software \citep{SmithEtAl2024}
include the following:

\begin{itemize}
\item \href{https://github.com/pylbm/pylbm}{pyLBM} has decoupled geometries and
models of their system using abstraction and modularization of the source code,
to make it easy to add new features.  The pyLBM design allows for independent
changes to the geometry and the model.  pyLBM also redeveloped data structures
to ease future change. 
\item \href{https://github.com/CFD-GO/TCLB}{TCLB} \citep{rokicki2016adjoint} is
designed to allow for the addition of some LBM features, but changes to major
aspects of the system would be difficult. For example, ``implementing a new
model will be an easy contribution'', but changes to the ``Cartesian mesh … will
be a nightmare'' \citep{SmithEtAl2024}. The design of TCLB highlights that not
every conceivable change needs to be supported, only the likely changes.  
\end{itemize}

As the LBM examples above illustrate, developers can accomplish design for
change by first identifying likely changes, either implicitly or explicitly, and
second by hiding each likely change behind a well-defined module interface. This
approach mirrors the recommendations from \citet{Parnas1972a}.
Section~\ref{Sec_CompareArtifacts} lists ideas for how to document the design,
including the likely changes, so that they are more visible to others.

\subsection{Assurance Case} \label{AssuranceCases}

To ensure correctness (\ppref{P_Correctness} and to achieve the quality of
reproducibility (\qref{Q_Reproducibility}), we recommend considering the use of
assurance cases.  \citet{RinehartEtAl2015} defines an assurance case as ``[a]
reasoned and compelling argument, supported by a body of evidence, that a
system, service, or organization will operate as intended for a defined
application in a defined environment.''  An assurance cases provide an organized
and explicit argument that the software and its documentation achieves desired
qualities, such as correctness and reproducibility.  Although assurance cases
have been successfully employed for safety critical systems
\citep{RinehartEtAl2015}, this technique is relatively new for research software
\citep{SmithEtAl2020_AC, Smith2018}.

One way to present an assurance case is through the Goal Structuring Notation
(GSN) \citep{Spriggs2012}, which make arguments clear, easy to read and, hence,
easy to challenge. GSN starts with a Top Goal (Claim), like ``Program X delivers
correct outputs when used for its intended use/purpose in its intended
environment.''  We then decompose this top goal into Sub-Goals, which themselves
may be further decomposed.  The purpose of the decomposition is to take the
abstract higher level goals and bring them down to something concrete that can
be proven.  The decomposition ends with the terminal Sub-Goals that are
supported by Solutions (Evidence). Typical evidence will consist of documents,
expert reviews, test case results, peer review, etc.  Within the GSN framework,
there are also strategy blocks, which describe the rationale for decomposing a
Goal or Sub-Goal into more detailed Sub-Goals. A common tool for creating,
editing, and presenting, a GSN argument is \href{https://astah.net/} {Astah}.  

\citet{SmithEtAl2020_AC} shows the example of arguing for the correctness of the
Analysis of Functional NeuroImages (AFNI) package 3dfim+ \citep{Ward2000}.
3dfim+ analyzes the activity of the brain by computing the correlation between
an ideal signal and the measured brain signal for each voxel. The assurance case
for the correctness of 3dfim+ has the top level decomposed into four sub-goals,
as shown in Figure~\ref{TopGoal}.  This example follows the same pattern as used
for medical devices \citep{Wassyng2015}.  The first sub-goal (GR) argues for the
quality of the documentation of the requirements.  The second sub-goal (GD) says
that the design complies with the requirements and the third proposes that the
implementation also complies with the requirements.  The fourth sub-goal (GA)
claims that the inputs to 3dfim+ will satisfy the operational assumptions, since
we need valid input to make an argument for the correctness of the output.

\begin{figure}[!ht]
\centering
\includegraphics[width=1.0\textwidth]{TopGoal.pdf}
\caption{Top Goal of the assurance case and its sub-goals}
\label{TopGoal}
\end{figure}

Preparing an assurance case for the pre-existing 3dfim+ software shows the value
of an assurance cases for research software. Although \citet{SmithEtAl2020}
found no errors in the output of the existing software, the rigour of the
proposed approach did lead to finding ambiguities and omissions in the existing
documentation, such as missing information on the coordinate system convention.
In addition, a potential concern for the software itself was identified from the
GA argument: running the software does not produce any warning about the
obligation of the user to provide data that matches the parametric statistical
model employed for the correlation calculations.

\subsection{Generate All Things} \label{Sec_GenAllThings}

To address developer pain points, we propose automatically generating MI code
and its documentation via a ``Generate All Things'' (GAT) approach. A GAT
approach uses models to capture knowledge in domains such as physics, computing,
mathematics, documentation, and certification.  Developers combine and transform
knowledge via explicit ``recipes'', which weave it together to generate the
desired code, documentation, test cases, inspection reports and build scripts. A
recipe can even potentially be written to generate an assurance case
(Section~\ref{AssuranceCases}). Our definition of GAT implies generation of all
software artifacts, not just the code. GAT moves development to a higher level
of abstraction so that domain experts can work without concern for low-level
implementation details. GAT allows developers to optimally generate code and
documentation, reduce the likelihood of errors, eliminate redundancy, and
automate maintenance. With a GAT approach MI developers can experiment with
different algorithm choices, different input formats, different outputs formats,
etc.

Part of GAT is code generation.  In the future, some believe that code
generation will transform coding, documentation, design, and verification
\citep{JohansonAndHasselbring2018, Smith2018}. GAT removes the distraction of
writing software, allowing developers to focus on their science. A GAT approach
removes the maintenance headaches of documentation duplicates and near
duplicates \citep{LucivEtAl2018}, since developers capture knowledge once and
transform it as needed.  Code generation has previously been applied to improve
research software, such as linear algebra software packages like Blitz++
\citep{Veldhuizen1998}, and ATLAS (Automatically Tuned Linear Algebra Software)
\citep{WhaleyEtAl2001}.  Software/hardware generation has been applied for
digital signal processing in Spiral \citep{Pueschel2001}. A generative approach
has also been used for a family of efficient, type-safe Gaussian elimination
algorithms \citet{Carette2008}. \citep{LoggEtAl2012} use code generation when
solving partial differential equations in FEniCS (Finite Element and
Computational Software). \citet{MatkerimEtAl2013} and \citet{OberEtAl2018} use
code generation for High Performance Computing (HPC), using UML (Unified Modelling
Language) for their domain models. \citet{SzymczakEtAl2016} presents initial work on
the GAT approach, \citet{SmithAndCarette2021-BRIC} presents a motivating example,
and \citep{CaretteEtAl2021-Drasil} provides a prototype.

A GAT approach addresses multiple pain points.  For example, a generative
approach can decrease development time (\ppref{P_LackDevTime}) by automation,
once the necessary infrastructure is in place.  The GAT approach addresses the
technology related pain point (\ppref{P_TechnologyHurdles}) because technology
information can be captured in the models and transformed as needed. To ensure
correctness (\ppref{P_Correctness}), a GAT approach should be correct by
construction.  If there are mistakes, GAT has the advantage that they are
propagated throughout the generated artifacts, which greatly increases the
chance that someone will notice the mistake.  Maintainability
(\qref{Q_Maintainability}) is addressed because developers write the recipes
used for generation at a high level making them relatively easy to change.
Usability (\ppref{P_Usability}) is addressed because of the emphasis on
generating up-to-date documentation.  GAT facilitates reproducibility
(\qref{Q_Reproducibility}) because at any time all the code and documentation
can be regenerated.  The generator can include explicit traceability to show the
dependence of the software on specific versions of software libraries.

\section{Threats to Validity} \label{sec_threats_to_validity}

Below we categorize and list the threats to validity that we have identified.
Our categories come from an analysis of software engineering secondary studies
by \citet{AmpatzoglouEtAl2019}, where a secondary study analyzes the data from a
set of primary studies.  \citet{AmpatzoglouEtAl2019} is appropriate because a
common example of a secondary study is a systematic literature review. Our
methodology is a systematic software review --- the primary studies are the
software packages, and our work collects and analyzes these primary studies.  We
identified similar threats to validity in our assessment of the state of the
practice of Lattice Boltzmann Solvers \citep{SmithEtAl2024}.

\subsection{Reliability}

A study is reliable if repetition of the study by different researchers using
the original study's methodology would lead to the same results
\citep{RunesonAndHost2009}. Reliability means that data and analysis are
independent of the specific researcher(s) doing the study.  For the current
study the identified reliability related threats are as follows:

\begin{itemize}
\item One individual does the manual measures for all packages. A different
evaluator might find different results, due to differences in abilities,
experiences, and biases.
\item The manual measurements for the full set of packages took several months.
Over this time the software repositories may have changed and the reviewer's
judgement may have drifted.
\end{itemize}

In \citet{SmithEtAl2016} we reduced concern over the reliability risk associated
with the reviewer's judgement by demonstrating that the measurement process is
reasonably reproducible.  In \citet{SmithEtAl2016} we graded five software
products by two reviewers. Their rankings were almost identical. As long as each
grader uses consistent definitions, the relative comparisons in the AHP results
will be consistent between graders.

\subsection{Construct Validity}

\citet{RunesonAndHost2009} defines construct validity as the adopted
metrics representing what they are intended to measure. Our construct threats are
often related to how we assume our measurements influences the various software
qualities, as summarized in Section~\ref{sec_grading_software}. Specifically,
our construct validity related threats include the following:

\begin{itemize}
\item We make indirect measurement of software qualities since meaningful direct
measures for qualities like maintainability, reusability and verifiability, are
unavailable.  We follow the usual assumption that developers achieve higher
quality by following procedures and adhering to standards \citep[p.\
112]{VanVliet2000}.
\item As mentioned in Section~\ref{sec_result_installability}, we could not
install or build \textit{dwv}, \textit{GATE}, and \textit{DICOM Viewer}. We used
a deployed on-line version for \textit{dwv}, a VM version for \textit{GATE}, but
no alternative for \textit{DICOM Viewer}. We might underestimate their rank due
to these technical issues.
\item Measuring software robustness only involved two pieces of data. This is
likely part of the reason for limited variation in the robustness scores
(Figure~\ref{fg_robustness_scores}). We could add more robustness data by
pushing the software to deal with more unexpected situations, like a broken
Internet connection, but this would require a larger investment of measurement
time. 
\item We may have inaccurately estimated maintainability by assuming a higher
ratio of comments to source code improves maintainability. Moreover, we assumed
that maintainability is improved if a high percentage of issues are closed, but
a project may have a wealth of open issues, and still be maintainable.
\item We assess reusability by the number of code files and LOC per file. This
measure is indicative of modularity, but it does not necessarily mean a good
modularization. The modules may not be general enough to be easily reused, or
the formatting may be poor, or the understandability of the code may be low.
\item The understandability measure relies on 10 random source code files, but
the 10 files will not necessarily be representative. 
\item As discussed in Section~\ref{Sec_OverallQ}, our overall AHP ranking makes
the unrealistic assumption of equal weighting.
\item We approximated popularity by stars and watches
(Section~\ref{Sec_VsCommunityRanking}), but this assumption may not be valid. 
\item As mentioned in Section~\ref{sec_interview_methods}, one interviewee was
too busy to participate in a full interview, so they provided written answers
instead. Since we did not have the chance to explain our questions or ask them
follow-up questions, there is a possibility of misinterpretation of the
questions or answers.
\item In building Table~\ref{Tbl_Guidelines} some judgement was necessary on our
part, since not all guidelines use the same names for artifacts that contain
essentially the same information.
\end{itemize}

\subsection{Internal Validity} \label{Sec_InternalValidity}

Internal validity means that discovered causal relations are trustworthy and
cannot be explained by other factors \citep{RunesonAndHost2009}. In our
methodology the internal validity threats include the following:

\begin{itemize}
\item In our search for software packages
(Section~\ref{sec_software_selection}), we may have missed a relevant package.
\item Our methodology assumes that all relevant software development activities
will leave a trace in the repositories, but this is not necessarily true. For
instance, the possibility exists that CI usage was higher than what we observed
through the artifacts (Section~\ref{Sec_CompareTools}). As another example,
although we saw little evidence of requirements
(Section~\ref{Sec_CompareTools}), maybe teams keep this kind of information
outside their repos, possibly in journal papers or technical reports.
\item We interviewed a relatively small sample of 8 teams.  Their pain points
(Section~\ref{painpoints}) may not be representative of the rest of their
community.
\end{itemize}

\subsection{External Validity}

If the results of a study can be generalized (applied) to other
situations/cases, then the study is externally valid \citep{RunesonAndHost2009}.
We are confident that our search was exhaustive.  We do not believe that we
missed any highly popular examples.  Therefore, the bulk of our validity
concerns are internal (Section~\ref{Sec_InternalValidity}).
However, our hope is that the trends observed, and the lessons learned for MI
software can be applied to other research software.  With that in mind we
identified the following threat to external validity:

\begin{itemize}
\item We cannot generalize our results if the development of MI software is
fundamentally different from other research software.
\end{itemize}

Although there are differences, like the importance of data privacy for MI data,
we found the approach to developing LBM software \citep{SmithEtAl2024} and MI
software to be similar.  Except for the domain specific aspects, we believe that
the trends observed in the current study are externally valid for other research
software.

\section{Future Work} \label{Sec_FutureWork}

The following recommendations for future state of the practice measurement
exercises, for MI or for other domains, could address some threats to validity
mentioned above.  Moreover, some ideas may make the data collection more
efficient.  

\begin{itemize}
    \item We would like to make surface measurements less shallow. For example:
    \begin{itemize}
        \item Surface reliability: our current measurement relies on the
        processes of installation and getting started tutorials. However, not
        all software needs installation or has a getting started tutorial. We
        could devise a list of operation steps (with the help of the Domain
        Expert), perform the same operations with each software, and record any
        errors.
        \item Surface robustness: we used damaged images as inputs for this
        measuring MI software. This process is similar to fuzz testing
        \citep{enwiki:1039424308}, which is one type of fault injection
        \citep{enwiki:1039005082}. We may adopt more fault injection methods,
        and identify tools and libraries to automate this process.
        \item Surface usability: we can design usability tests and test all
        software projects with end-users. The end-users can be volunteers and
        domain experts. Ideas for getting started are available in
        \citet{SmithEtAl2021}.
        \item Surface understandability: our current method does not require
        understanding the source code. As software engineers, perhaps we can
        select a small module of each project, read the source code and
        documentation, try to understand the logic, and score the ease of the
        process.  
        \item Maintainability: we can add a measure modifiability as part of the
        measurement of maintainability.  An experiment could be conducted asking
        participants to make modifications, observing the study subjects during
        the modifications, testing the resulting software and surveying the
        participants \citep{SmithEtAl2021}.
    \end{itemize}
	\item We can further automate the measurements on the grading template. For
	example, with automation scripts and the GitHub API, we may save significant
	time on retrieving the GitHub metrics through a GitHub Metric Collector.
	This Collector can take GitHub repository links as input, automatically
	collect metrics from the GitHub API, and record the results.
	\item We can improve some interview questions. Some examples are:
	\begin{itemize}
	    \item In one question we ask, ``Do you think improving this process can
	    tackle the current problem?''  The problem is that this is a yes-or-no
	    question, which is not informative. We could change the question to ``By
	    improving this process, what current problems can be tackled?''; 
	    \item We can ask for more details about the modular approach, such as
	    ``What principles did you use to divide code into modules? Can you
	    describe an example of using your principles?''.
	\end{itemize}
	\item We can better organize the interview questions. Since we use audio
	conversion tools to transcribe the answers, we should make the transcription
	easier to read. For example, we can order them together for questions about
	the five software qualities and compose a similar structure for each.
	\item We can mark the follow-up interview questions with keywords. For
	example, say ``this is a follow-up question'' every time asking one. Thus,
	we record this sentence in the transcription, and it will be much easier to
	distinguish the follow-up questions from the 20 designed questions.
\end{itemize}

\section{Conclusions} \label{ch_conclusions}

We analyzed the state of the practice for the MI domain with the goal of
understanding current practice, answering our ten research questions
(Section~\ref{sec_motivation}) and providing recommendations for current and
future projects.  Our methods in Section~\ref{ch_methods} form a general process
to evaluate domain-specific software, that we apply to the specific domain of MI
software. We identified 48 MI software candidates, then, with the help of the
Domain Expert selected 29 of them to our final list. 

Section~\ref{ch_results} lists our measurement results for ranking the 29
projects for nine software qualities. Our ranking results appear credible since
they are mostly consistent with the ranking from the scientific community
implied by the GitHub stars-per-year metric. As discussed in
Section~\ref{Sec_VsCommunityRanking}, four of the top five software projects
appear in both our list and in the GitHub popularity list.  Moreover, our top
five packages appear among the first eight positions on the GitHub list.  The
noteworthy discrepancies between the two lists are for the packages that we were
unable to install (\textit{dwv} and \textit{Dicom Viewer}).

Based on our grading scores \textit{3D Slicer}, \textit{ImageJ}, \textit{Fiji}
and \textit{OHIF Viewer} are the top four software performers.  However, the
separation between the top performers and the others is not extreme.  Almost all
packages do well on at least a few qualities, as shown in
Table~\ref{topperformerstable}, which summarizes the packages ranked first and
second for each quality. Almost 70\% (20 of 29) of the software packages appear
in the top two for at least two qualities.  The only packages that do not appear
in Table~\ref{topperformerstable}, or only appear once, are \textit{Papaya},
\textit{MatrixUser}, \textit{MRIcroGL}, \textit{XMedCon}, \textit{dicompyler},
\textit{DicomBrowser}, \textit{AMIDE}, \textit{3DimViewer}, and
\textit{Drishti}. The shortness of this list suggests parity with respect to
adoption of best practices for MI software overall.

\begin{table}[ht!]
	\begin{center}
	\renewcommand{\arraystretch}{1.8}
	\begin{tabular}{ p{3cm}p{13cm} }
		\toprule

		Quality & Ranked 1st or 2nd\\

		\midrule

		Installability & 3D Slicer, BioImage Suite Web, Slice:Drop, INVESALIUS\\

		\pbox{3.0cm}{Correctness \\ and Verifiability} & OHIF Viewer, 3D
		Slicer, ImageJ\\

		Reliability & SMILI, ImageJ, Fiji, 3D Slicer, Slice:Drop, OHIF
		Viewer\\

		Robustness & XMedCon, Weasis, SMILI, ParaView, OsiriX Lite,
		MicroView, medInria, ITK-SNAP, INVESALIUS, ImageJ, Horos, Gwyddion,
		Fiji, dicompyler, DicomBrowser, BioImage Suite Web, AMIDE, 3DimViewer,
		3D Slicer, OHIF Viewer, DICOM Viewer\\

		Usability & 3D Slicer, ImageJ, Fiji, OHIF Viewer, ParaView,
		INVESALIUS, Ginkgo CADx, SMILI, OsiriX Lite, BioImage Suite Web,
		ITK-SNAP, medInria, MicroView, Gwyddion\\

		Maintainability & 3D Slicer, Weasis, ImageJ, OHIF Viewer, ParaView\\

		Reusability & 3D Slicer, ImageJ, Fiji, OHIF Viewer, SMILI, dwv, BioImage
		Suite Web, GATE, ParaView\\

		\pbox{3.0cm}{Understandability} & 3D Slicer, ImageJ, Weasis,
		Fiji, Horos, OsiriX Lite, dwv, Drishti, OHIF Viewer, GATE, ITK-SNAP,
		ParaView, INVESALIUS\\

		\pbox{3.0cm}{Visibility and \\Transparency} & ImageJ, 3D Slicer, Fiji\\

		Overall Quality & 3D Slicer, ImageJ\\

		\bottomrule		
	\end{tabular}
	\caption{Top performers for each quality (sorted by order of quality
	measurement)} \label{topperformerstable}
	\end{center}
\end{table} 

For insight into devising future methods and tools, we interviewed nine
developers (from eight teams) to learn about their pain points
(Section~\ref{painpoints}).  We also discussed qualities of potential concern.
The identified pain points and qualities of concern include: 

\begin{description}
\item [\ppref{P_LackDevTime}] Lack of development time, %P1
\item [\ppref{P_LackFunding}] Lack of funding, %P2
\item [\ppref{P_TechnologyHurdles}] Technology hurdles, %P3
\item [\ppref{P_Correctness}] Ensuring correctness, %P4
\item [\ppref{P_Usability}] Usability, %P5
\item [\qref{Q_Maintainability}] Quality of maintainability, and %Q1
\item [\qref{Q_Reproducibility}] Quality of reproducibility. %Q2
\end{description}  

Despite the pain points, overall MI software is in a healthy state for software
development practices.  In our survey of the selected projects we observed 88\%
of the documentation artifacts recommended by research software development
guidelines (Section~\ref{Sec_CompareArtifacts}).  With respect to tools, MI is
keeping pace with other research software with 100\% of the projects using
version control (with 93\% specifically using git)
(Section~\ref{Sec_CompareTools}).  We observed that the MI developers tend to
follow the typical research software trend of using a quasi-agile software
development process (Section~\ref{Sec_CompareMethodologies}).

Although the state of the practice for MI software is healthy, we did notice
areas where practice seems to lag behind the research software development
guidelines.  For instance, the guidelines recommend three artifacts that were
not observed: uninstall instructions, test plans, and requirements
documentation. We observed the following recommended artifacts, but only rarely:
contributing file, developer code of conduct, code style guidelines, product
roadmap, design documentation, and API documentation
(Section~\ref{Sec_CompareArtifacts}). Although software development tool use
seems healthy, we found the use of CI/CD behind typical usage rates (17\% of the
projects used CI/CD) (Section~\ref{Sec_CompareTools}).  With respect to the
development process, developer identified areas for improvement included testing
(only 50\% of projects were identified to have unit testing) and documentation
(only three out of nine developers felt their documentation was clear enough)
(Section~\ref{Sec_CompareMethodologies}).

Our interviewees proposed strategies to improve the state of the practice, to
address the identified pain points, and to improve software quality.  To their
list (Section~\ref{painpoints}) we added some of our own recommended strategies
(Section~\ref{ch_recommendations}).  Below we summarize the proposed strategies,
with traceability to where we discuss the strategy, and to the relevant pain
points.

\begin{enumerate}

\item Increase documentation to address \ppref{P_LackDevTime},
\ppref{P_TechnologyHurdles}, \ppref{P_Usability}, \qref{Q_Maintainability},
\qref{Q_Reproducibility} (Section~\ref{painpoints})

\item Increase testing by enriching datasets to address \ppref{P_Correctness},
\qref{Q_Reproducibility}
(Section~\ref{painpoints},~\ref{sec_recommendations_testing_dataset})

\item Increase modularity to address \qref{Q_Maintainability} (Section~\ref{painpoints})

\item Use continuous integration to address \ppref{P_LackDevTime},
\ppref{P_Correctness}, \qref{Q_Reproducibility}
(Section~\ref{painpoints},~\ref{Sec_ContinuousIntegration})

\item Move to web applications to address \ppref{P_TechnologyHurdles}
(Section~\ref{painpoints},~\ref{sec_webapps})

\item Employ linters to address \ppref{P_LackDevTime},
\ppref{P_TechnologyHurdles}, \qref{Q_Maintainability}
(Section~\ref{Sec_Linters})

\item Peer reviews to address \ppref{P_LackDevTime},
\ppref{P_TechnologyHurdles}, \ppref{P_Correctness}, \qref{Q_Maintainability}
(Section~\ref{Sec_PeerReview})

\item Design for change to address \qref{Q_Maintainability} (Section~\ref{Sec_DesForChange})

\item Assurance case to address \ppref{P_Correctness}, \qref{Q_Reproducibility}
(Section~\ref{AssuranceCases})

\item Generate all things to address \ppref{P_LackDevTime},
\ppref{P_TechnologyHurdles}, \ppref{P_Correctness}, \ppref{P_Usability},
\qref{Q_Maintainability} and \qref{Q_Reproducibility}.
(Section~\ref{Sec_GenAllThings})

\end{enumerate}

\section*{Acknowledgements}

We would like to thank Peter Michalski and Oluwaseun Owojaiye for fruitful
discussions on topics relevant to this paper.  We would also like to thank Jason
Balaci for advice on web applications.

\section*{Conflict of Interest}

On behalf of all authors, the corresponding author states that there is no
conflict of interest.

\bibliographystyle{ACM-Reference-Format}
\bibliography{PainPoints_OS_MI}

\end{document}