\documentclass[12pt, notitlepage]{article}
\usepackage{amsmath, mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{xr}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{xfrac}
\usepackage{tabularx}
\usepackage{float}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{pdflscape}
\usepackage{afterpage}
\usepackage{titling}
\usepackage{blindtext}
\usepackage{multicol}
\usepackage{indentfirst}
\usepackage[toc,page]{appendix}

\usepackage[]{natbib}

%\usepackage{refcheck}

\hypersetup{
    bookmarks=true,         % show bookmarks bar?
      colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=red,          % color of internal links (change box color with linkbordercolor)
    citecolor=green,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}


% For easy change of table widths
\newcommand{\colZwidth}{1.0\textwidth}
\newcommand{\colAwidth}{0.24\textwidth}
\newcommand{\colBwidth}{0.76\textwidth}
\newcommand{\colCwidth}{0.28\textwidth}
\newcommand{\colDwidth}{0.72\textwidth}
\newcommand{\colEwidth}{0.33\textwidth}
\newcommand{\colFwidth}{0.67\textwidth}
\newcommand{\colGwidth}{0.5\textwidth}
\newcommand{\colHwidth}{0.28\textwidth}

\newcounter{comnum} %Commonality Number
\newcommand{\cthecomnum}{C\thecomnum}
\newcommand{\cref}[1]{C\ref{#1}}

\newcounter{varnum} %Variability Number
\newcommand{\vthevarnum}{V\thevarnum}
\newcommand{\vref}[1]{V\ref{#1}}

\newcounter{parnum} %Parameter of Variation Number
\newcommand{\ptheparnum}{P\theparnum}
\newcommand{\pref}[1]{P\ref{#1}}


% Used so that cross-references have a meaningful prefix
\newcounter{defnum} %Definition Number
\newcommand{\dthedefnum}{GD\thedefnum}
\newcommand{\dref}[1]{GD\ref{#1}}
\newcounter{datadefnum} %Datadefinition Number
\newcommand{\ddthedatadefnum}{DD\thedatadefnum}
\newcommand{\ddref}[1]{DD\ref{#1}}
\newcounter{theorynum} %Theory Number
\newcommand{\tthetheorynum}{T\thetheorynum}
\newcommand{\tref}[1]{T\ref{#1}}
\newcounter{tablenum} %Table Number
\newcommand{\tbthetablenum}{T\thetablenum}
\newcommand{\tbref}[1]{TB\ref{#1}}
\newcounter{assumpnum} %Assumption Number
\newcommand{\atheassumpnum}{P\theassumpnum}
\newcommand{\aref}[1]{A\ref{#1}}
\newcounter{goalnum} %Goal Number
\newcommand{\gthegoalnum}{P\thegoalnum}
\newcommand{\gsref}[1]{GS\ref{#1}}
\newcounter{instnum} %Instance Number
\newcommand{\itheinstnum}{IM\theinstnum}
\newcommand{\iref}[1]{IM\ref{#1}}
\newcounter{reqnum} %Requirement Number
\newcommand{\rthereqnum}{P\thereqnum}
\newcommand{\rref}[1]{R\ref{#1}}
\newcounter{lcnum} %Likely change number
\newcommand{\lthelcnum}{LC\thelcnum}
\newcommand{\lcref}[1]{LC\ref{#1}}

\usepackage{fullpage}
\usepackage{setspace}
\usepackage{thesis}

\doublespacing

\newcommand{\fulltitle}{State of the Practice for Lattice Boltzmann Method Software}
\newcommand{\authorname}{Peter Michalski}

\begin{document}
\begin{singlespace}

\thesistitle
	{\MakeUppercase{\fulltitle}}
	{\MakeUppercase{\authorname}.}
	{Master of Engineering in Computing and Software}
	{\copyright \ Copyright by \authorname, August 2021}

\newpage
\thispagestyle{empty}

\noindent\textbf{TITLE:} State of The Practice for Lattice Boltzmann Method Software

\noindent\textbf{DEGREE:} Master of Engineering (Computing and Software)

\noindent\textbf{INSTITUTION:} McMaster University

\noindent\textbf{AUTHOR:} Peter Michalski

\noindent\textbf{SUPERVISOR:} Dr. Spencer Smith and Dr. Jacques Carette

\noindent\textbf{NUMBER OF PAGES:} 125


\newpage
\thispagestyle{empty}
\begin{center}
	\textbf{\large Abstract}
\end{center}

We analyze the state of the practice of software development in the Lattice Boltzmann Methods software domain by quantitatively and qualitatively measuring and comparing 23 software packages. We present a general methodology for assessing the state of the practice of software development in scientific computing software domains. We present a domain analysis of the Lattice Boltzmann Methods software family, and identify candidate software packages. We assess these packages to answer software development related research questions to understand how software quality is impacted by software development choices, including principles, processes, and tools. We interview software developers to identify development pain points, and to identify how software quality is ensured. We use quantitative data to rank the software packages using the Analytical Hierarchy Process, and rank Ludwig, ESPResSo, and Palabos as the top three packages. Each of them score high in at least several of the individual qualities that we quantitatively measure. We compare these rankings with rankings from the software development community. We make recommendations, such as providing a detailed user manual and tutorials, explicitly stating the limits of the software, using user-friendly software languages, considering a peer review process, communicating development and contribution information, and using continuous integration and project management tools.\\ 

\noindent\textbf{Keywords}: Lattice Boltzmann Methods, Scientific Computing, Software Engineering, Software Family, Software Quality 


\newpage
\pagenumbering{roman}
\tableofcontents
\addtocontents{toc}{}

~\newpage

\listoffigures

\listoftables

~\newpage

\clearpage
\section*{Reference Material}

\subsection*{Software Engineering Related Definitions and Acronyms}

\noindent\textbf{AHP}: Analytical Hierarchy Process\\

\noindent\textbf{Commonality}: A requirement or goal common to all family members.\\

\noindent\textbf{CPU}: Central Processing Unit\\

\noindent\textbf{Goal}: Goals capture, at different levels of abstraction, the various objectives the system under consideration should achieve \citep{van2001goal}.\\

\noindent\textbf{GPU}: Graphics Processing Unit\\

\noindent\textbf{LOC}: Lines of Code\\

\noindent\textbf{OS}: Operating System\\

\noindent\textbf{OTS}: Off-The-Shelf\\

\noindent\textbf{Requirements}: A software requirement is: \textit{i}) a condition or capability needed by a user to solve a problem or achieve an objective; \textit{ii}) a condition or capability that must be met or possessed by a system or system component to satisfy a contract, standard, specification, or other formally imposed document; or, \textit{iii}) a documented representation of a condition or capability as in the above two definitions \citep{thayer2000ieee}.\\

\noindent\textbf{SCS}: Scientific Computing Software\\

\noindent\textbf{Software Family}: A set of programs with an extensive amount of common properties \cite{parnas1976design}.\\

\noindent\textbf{Variability}: A requirement or goal that varies between family members.

\newpage
\subsection*{Lattice Boltzmann Related Definitions and Acronyms}

\noindent\textbf{1D} 1-Dimensional\\

\noindent\textbf{2D}: 2-Dimensional\\

\noindent\textbf{3D}: 3-Dimensional\\

\noindent\textbf{BGK}: Bhatnagar-Gross-Krook \citep{bhatnagar1954model}\\

\noindent\textbf{CFD}: Computational Fluid Dynamics\\

\noindent\textbf{MRT}: Multi-Relaxation-Time\\

\noindent\textbf{SRT}: Single-Relaxation-Time\\

\noindent\textbf{TRT}: Two-Relaxation-Time\\

\noindent\textbf{LBM}: Lattice Boltzmann Methods\\

\noindent\textbf{LBS}: Lattice Boltzmann Solvers\\

\noindent\textbf{Velocity Directions}: The number of links connecting to each lattice node in the chosen model from neighbouring nodes. All nodes in a chosen lattice model will have the same number of links. A single link will connect between two adjacent nodes.


\end{singlespace}
\newpage
\pagenumbering{arabic}
\section{Introduction}

We analyze the development of Computational Fluid Dynamics (CFD) software packages that use Lattice Boltzmann Methods (LBM). These LBM algorithms consider the behaviours of a collection of particles as a single unit of an intermediate size, and predict their positional probability as they move through a lattice structure. Figure \ref{circularflow} presents the streamlines of converged flow past a stationary circular cylinder with varied Reynolds number, a common task of LBM.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{circularflow}
		\caption{Streamlines of flow past a stationary circular cylinder at Reynolds number = 10, 20, and 40 \citep{chen2021phase}}
		\label{circularflow}
	\end{center}
\end{figure}

LBM have several advantages over conventional CFD methods, including a simple calculation procedure, improved parallelization, and robust handling of complex geometries \citep{ganji2015application}. LBM are further detailed in Section \ref{domainanalysis}. 

Scientific Computing Software (SCS) is defined as the use of computer tools to analyze or simulate mathematical models of real world systems \citep{smith2006systematic}. Given the importance of such software, scientists and engineers desire methods and tools to sustainably develop it with high quality. This has led to the formation of groups like the Software Sustainability Institute (\href{https://www.software.ac.uk/}{SEI}) and Better Scientific Software (\href{https://bssw.io/}{BSS}). 

The goal of this report is to analyze the current state of development of LBM SCS to provide insight into its best practices and to offer guidance for future development. We want to understand how software quality in the LBM SCS family is impacted by software development choices, including principles, processes, and tools.

In this report red text denotes internally linked sections of this document. Cyan text denotes an external URL link. Green text denotes a link to the bibliography.

\subsection{Research Questions}\label{purpose}

The LBM software packages are assessed to answer the following research questions:

\begin{enumerate}
	\item What artifacts are present in current software packages? 
	\item What tools (development, dependencies, project management) are used by current software packages?
	\item What principles, processes, and methodologies are used in the development
	of current software packages?
	\item What are the pain points for developers working on research software
	projects?  What aspects of the existing processes, methodologies and tools do
	they consider as potentially needing improvement?  How should processes,
	methodologies and tools be changed to improve software development and
	software quality?
	\item For research software developers, what specific actions are taken to
	address the following:
	\begin{enumerate}
		\item installability
		\item correctness and verifiability
		\item reliability
		\item robustness
		\item performance
		\item usability
		\item maintainability
		\item modifiability
		\item reusability
		\item understandability
		\item traceability
		\item visibility and transparency
		\item reproducibility
		\item unambiguity
	\end{enumerate} 
	\item How does software designated as high quality by this methodology compare	with top rated software by the community?
\end{enumerate}

The packages are assessed for a given set of quality attributes using quantitative and qualitative data. The qualities are defined in Section \ref{softwarequalities}. Quantitative data is measured using quality metrics. The findings are presented in Section \ref{AHPresults}. Qualitative data is gathered from interviews with software package developers. The interview findings are presented in Section \ref{interviewresults}. In this context the purposes of the report are as follows:

\begin{itemize}
	\item Develop and test an updated methodology for assessing the state of the practice of SCS projects
	\item Describe the specific example of LBM software structure and goals
	\item Report on the measure of a subset of LBM software packages along quality metrics
	\item Evaluate the state of the practice of LBM software development along quality attributes
	\item Make suggestions for improving LBM software along quality attributes
	\item Make suggestions for improving state of the practice of SCS assessments
\end{itemize}

\subsection{Motivation}

The purpose of state of the practice assessments is to understand how software quality is impacted by software development choices, including principles, processes, and tools, within SCS communities. This knowledge can be used to guide future development of SCS, specifically along quality attributes, and to reduce software quality failures. This work reports on the state of the practice of LBM software development and makes suggestions on improving the quality of software in this domain.

This assessment of the state of the practice of LBM software development builds off of prior work on assessing the state of research software development. Updates to the methodology used in prior assessments are detailed below. Those assessments include domains such as Geographic Information Systems \citep{SmithEtAl2018_arXivGIS}, Mesh Generators \citep{SmithEtAl2016}, Oceanographic Software \citep{smith2015state}, Seismology software \citep{SmithEtAl2018}, and statistical software for psychology \citep{SmithEtAl2018_StatSoft}.

In the course of this assessment we updated the methodology that was used during previous assessments. Details of the new methodology are in \citep{methodology2020}. The previous set of research questions were critically assessed and modified. The updated questions are listed in Section \ref{purpose}. In this re-boot we collected more quantitative and qualitative data, focused on software quality attributes, and added more measures, including collecting empirical data and interviewing developers. We have also added a domain analysis to better characterize the functionality provided by the software, and have leveraged the expertise of a domain expert. As in past assessments, the collected data was combined to rank the software using the Analytical Hierarchy Process (AHP). The domain expert was consulted to verify the ordering.

\subsection{Scope}

We analyze a filtered set of LBM software packages along quantitative and qualitative measures. Many of these measures are captured using surface measurements, which can be categorized as initial and easy to capture measurements, of the underlying quality; they may not represent the true quality of the software. Surface measurements are taken as they allow us to apply the same measurement, with reasonable effort, along all software packages despite technical and functional variabilities among the set of software packages. Not all qualities are quantitatively measured. For example, performance is not measured by running the code. A surface investigation of the documentation of each software package for performance information is conducted instead. The measured software packages are open source. Some recommendations may not apply to closed-source software. Practices surrounding close source software development may differ.

The report addresses what was done during the development of the software packages to address the quality attributes listed in Section \ref{softwarequalities}. It then provides general guidance on how to improve these qualities when developing LBM software. It does not make suggestions on what should have been done or should be done for any one specific package. Best practices may differ among software packages due to their inherent organizational and technical differences. 

\subsection{Organization}

The report is organized as follows:

\begin{itemize}
	\item \textbf{Introduction} to the report, including its purpose, motivation, scope, and organization.
	\item LBM software \textbf{Domain Analysis}. A domain analysis consists of systematically identifying and documenting the commonalities, variabilities, and terminology of a software family \citep{Weiss1997}.
	The purpose of this analysis in this state of the practice assessment is to better classify the different software products based on their functionality.
	\item \textbf{Methodology} of this state of the practice assessment, including the steps of the methodology, software quality definitions and how these qualities are assessed in this report, an overview of candidate software selection, empirical measures gathered, and the AHP. This follows a general methodology for assessing the state of the practice for SCS domains.
	\item \textbf{Quantitative Results}. 
	\item \textbf{Qualitative Results}. 
	\item An analysis of the results and \textbf{Answers To Research Questions}.
	\item \textbf{Conclusion} to the report including comments on suggestions for future state of the practice assessments.
	\item \textbf{Appendix}: This section includes our Research Questions, the Measurement Template, Grading Template, Ethics Approval, and Developer Interview Questions.
\end{itemize}

\newpage

\section{Domain Analysis}\label{domainanalysis}

A domain analysis consists of systematically identifying and documenting the commonalities, variabilities, and parameters of variation of a software family \citep{Weiss1997}. A software family is defined by \cite{parnas1976design} as ``a set of programs whose common properties are so extensive that it is advantageous to study the common properties of the programs before analyzing individual members''. 

We added this domain commonality and variability analysis to our original methodology because the first time we did the State of the Practice exercise we found we weren't comparing ``apples to apples". Software packages can have considerable variation, even when the packages appear to be in the same software family. Viewing our final ranking of the software packages as a means for selecting the right tool for a job, functionality is important. A 2D Lattice Boltzmann solver is not a substitute for a 3D Lattice Boltzmann solver when the latter is required. For example, an analysis of a vascular system may require a 3D model to convey all required information to a researcher. This domain analysis helps ensure that the packages that we compare are appropriately similar, and highlights potential differences. This information further helps when analyzing the appropriateness of the final rank of the packages.  This exercise seeks to classify the different software products based on their functionality. Nonfunctional qualities are assessed through other means in our process. For instance, through usability experiments as noted in the \href{https://github.com/smiths/AIMSS/blob/master/StateOfPractice/Methodology/Methodology.pdf}{Methodology for Assessing the State of the Practice for Domain X} document. This exercise was not part of the methodology of the previous State of the Practice exercise.

This domain analysis of the LBM SCS family is organized into the four subsection located below. The first subsection reviews the basics of Lattice Boltzmann systems. The next three subsections consist of lists of commonalities, variabilities, and parameters of variation, respectively. These three sections form the heart of the domain analysis and include an extensive set of cross-references to demonstrate the relationships between the different items.

\subsection{Lattice Boltzmann Systems}

LBM are a family of fluid dynamics algorithms for simulating single-phase and multiphase fluid flows, often incorporating additional physical complexities \citep{chen1998lattice} such as reflective and non-reflective boundaries. They consider the behaviours of a collection of particles as a single unit at the mesoscopic scale, between the nanoscopic and microscopic scales. These methods predict the positional probability of a collection of particles moving through a lattice structure. Off-the-shelf (OTS) Lattice Boltzmann Solvers (LBS) allow for a range of fluid and physical model input parameters, computational parameters, and output parameters.

LBS model fluid dynamics within a boundary using a predefined lattice structure and a two step calculation process. The first process is streaming, where the particles move along the lattice via links. The second process is collision, where energy and momentum is transferred among particles that collide \citep{bao2011lattice}.
There are many standardized lattice models - individual solvers within the family might only use a subset of them.
LBM use the initial parameters of the fluid to find the probability of where along the lattice linkages a group of particles are most likely to travel. It then moves the particles into the next node, and transfers the energy and momentum if a collision occurs. Then the process repeats for the duration of the modeling instance.

\subsection{Commonalities}\label{comm_sec}

This section lists common features among potential family members. These features were chosen after a review of LBM literature and software package specifications. The commonalities are organized using the following abstraction of the system, which can be used to describe all Lattice Boltzmann systems: input information, generate the simulation, output the results. Section \ref{comm_lbm} describes the commonalities for the simulation step. Section \ref{comm_in} highlights the input information that is required for Lattice Boltzmann systems. The next section, Section \ref{comm_out}, shows the common features for the output of Lattice Boltzmann systems, such as the requirement that mesh information be written to files. Although the output information could simply be written to the memory, in all practical applications it is desirable to have a persistent record of the output that was created.

Each commonality below uses the same structure. All of the commonalities are assigned a unique item number, which takes the form of a natural number with the prefix ``C''. Following this, a description of the commonality is provided along with a list of related variabilities, which are given as hyperlinks that allow navigation of the document to the text describing the variability.  

\subsubsection{Lattice Boltzmann Method Solvers}\label{comm_lbm}
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colAwidth} | p{\colBwidth}|}
		\hline
		\bf Item Number& C\refstepcounter{comnum}\thecomnum{}\label{lattice} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colAwidth} | p{\colBwidth}|}		
		\hline
		\bf Description & A lattice discretizes a computational domain into a finite number of points. All LBS discretize the computational domain using a regular, evenly spaced grid within a boundary.\\
		\hline
		\bf Related Variability & \vref{numveldir} \vref{varcolop} \vref{varboundary}\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colAwidth} | p{\colBwidth}|}
		\hline
		\bf Item Number& C\refstepcounter{comnum}\thecomnum\label{collisionoperator} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colAwidth} | p{\colBwidth}|}		
		\hline
		\bf Description & All Lattice Boltzmann versions use a collision operator which concerns collisions between particles. Collision operators map collisions of particles within the lattice space. The Bhatnagar-Gross-Krook Collision Operator is a common LBM collision operator that preserves continuity for a discretized model, for each velocity direction $i$. Its equation is $\mathrm{\Omega_i} = \frac{1}{\tau}(f_{i}^{eq}-f_{i})$, where $\tau$ is the relaxation rate towards equilibrium, $f^{eq}$ is the equilibrium particle probability distribution function, and $f$ is the particle probability distribution function.\\
		\hline
		\bf Related Variability & \vref{dimensions} \vref{numveldir}\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colAwidth} | p{\colBwidth}|}
		\hline
		\bf Item Number& C\refstepcounter{comnum}\thecomnum\label{probabilitydensityfunc} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colAwidth} | p{\colBwidth}|}		
		\hline
		\bf Description & All Lattice Boltzmann versions use a probability density function to give the probability that fluid has moved into a specific domain.\\
		\hline
		\bf Related Variability & \vref{dimensions}\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colAwidth} | p{\colBwidth}|}
		\hline
		\bf Item Number& C\refstepcounter{comnum}\thecomnum\label{equildistfunc} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colAwidth} | p{\colBwidth}|}		
		\hline
		\bf Description & Every Lattice Boltzmann version uses an equilibrium distribution function to capture the probability distribution of particles.\\
		\hline
		\bf Related Variability & \vref{varedf} \vref{edfstorage} \vref{edfcoeff}\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colAwidth} | p{\colBwidth}|}
		\hline
		\bf Item Number& C\refstepcounter{comnum}\thecomnum\label{Boltzmanntransequ} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colAwidth} | p{\colBwidth}|}		
		\hline
		\bf Description & Every Lattice Boltzmann version uses a Boltzmann transport equation to describe the statistical behaviour of a system that does not have collisions. The equation is $(\frac{d}{dt} + \mathrm{e} \cdot \nabla_\mathrm{x} + \frac{F}{m} \cdot \nabla_\mathrm{e})f = \Omega(t)$, where $\mathrm{e}$ is the velocity, $\nabla_\mathrm{x}$ is the position vector gradient, $F$ is the force of the fluid, $m$ is the mass, $\nabla_\mathrm{e}$ is the velocity gradient, $f$ indicates the probability density function, and $\Omega(t)$ is the collision operator as a function of time.\\
		\hline
		\bf Related Variability & \vref{vartraneq}\\
		\hline
	\end{tabular}
\end{minipage}\\
\subsubsection{Input}\label{comm_in}
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colAwidth} | p{\colBwidth}|}
		\hline
		\bf Item Number& C\refstepcounter{comnum}\thecomnum\label{fluidandboundaryinfo} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colAwidth} | p{\colBwidth}|}		
		\hline
		\bf Description & The LBS require fluid, model, and boundary information for the problem. This includes, but is not limited to, fluid acceleration rate, velocity, and viscosity, as well as the number of dimensions in the lattice model, and the number of velocity directions in the lattice.\\
		\hline
		\bf Related Variability & \vref{interface}\\
		\hline
	\end{tabular}
\end{minipage}\\
\subsubsection{Output}\label{comm_out}
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colAwidth} | p{\colBwidth}|}
		\hline
		\bf Item Number& C\refstepcounter{comnum}\thecomnum\label{predictionstomemory} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colAwidth} | p{\colBwidth}|}		
		\hline
		\bf Description & LBS write fluid predictions, such as the output of the transport equation, to memory.\\
		\hline
		\bf Related Variability & \vref{varvisual} \vref{formatoutput} \\
		\hline
	\end{tabular}
\end{minipage}\\


\subsection{Variabilities}\label{var_sec}
This section provides a list of characteristics that may vary among family members. These features were chosen after a review of LBM literature and software package specifications. As in Section \ref{comm_sec}, the first three subsections on variabilities are organized into the following sublists: Simulation Models, Input and Output. The final subsection lists variabilities that can be characterized as system constraints. 

As for the commonalities, each variability is labelled with a unique item number. In this case the numbers are prepended with the letter ``V''. The other three headings provided for each variability are: Description, Related Commonality, and Related Parameter. The related commonalities and parameters are given as a set of identifiers that respectively refer back to the previous section on commonalities or refer forward to the next section on parameters of variation.

\subsubsection{Lattice Boltzmann Method Solvers}
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number& V\refstepcounter{varnum}\thevarnum\label{parallel} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline
		\bf Description & LBS may use a framework for parallel processing of the model.\\
		\hline
		\bf Related Commonality & None\\
		\hline
		\bf Related Parameter & \pref{parparallel}\\
		\hline
	\end{tabular}
\end{minipage}\\
~\newline\\
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number& V\refstepcounter{varnum}\thevarnum\label{varedf} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline
		\bf Description & Different versions of an equilibrium distribution function can capture the probability distribution of particles.\\
		\hline
		\bf Related Commonality & \cref{equildistfunc}\\
		\hline
		\bf Related Parameter & \pref{paredf}\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number& V\refstepcounter{varnum}\thevarnum\label{edfstorage} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline
		\bf Description & Storage patters for distribution function can vary.\\
		\hline
		\bf Related Commonality & \cref{equildistfunc}\\
		\hline
		\bf Related Parameter & \pref{parstorage}\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number& V\refstepcounter{varnum}\thevarnum\label{edfcoeff} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline
		\bf Description & Coefficients used with the distribution function can vary. These are based on the number of velocity directions in the model.\\
		\hline
		\bf Related Commonality & \cref{equildistfunc}\\
		\hline
		\bf Related Parameter & \pref{paredfcoeff}\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number& V\refstepcounter{varnum}\thevarnum\label{dimensions} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline
		\bf Description & The number of dimensions in the lattice of the model can vary.\\
		\hline
		\bf Related Commonality & \cref{lattice} \cref{probabilitydensityfunc}\\
		\hline
		\bf Related Parameter & \pref{pardimensions}\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number& V\refstepcounter{varnum}\thevarnum\label{numveldir} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline 
		\bf Description & The number of velocity directions in the lattice of the model can vary.\\
		\hline
		\bf Related Commonality & \cref{lattice} \cref{collisionoperator}\\
		\hline
		\bf Related Parameter & \pref{parveldir}\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number& V\refstepcounter{varnum}\thevarnum\label{varcolop} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline
		\bf Description & Various collision operators can be used.\\
		\hline
		\bf Related Commonality & \cref{lattice} \cref{collisionoperator}\\
		\hline
		\bf Related Parameter & \pref{parcollisop}\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number& V\refstepcounter{varnum}\thevarnum\label{vartraneq} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline
		\bf Description & Various transport equations can be used to describe the statistical behaviour of the system\\
		\hline
		\bf Related Commonality & \cref{Boltzmanntransequ}\\
		\hline
		\bf Related Parameter & \pref{partransequ}\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number&  V\refstepcounter{varnum}\thevarnum\label{numberoffluids} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline
		\bf Description & The number of fluids allowed in a multifluid simulation.\\
		\hline
		\bf Related Commonality & \cref{fluidandboundaryinfo}\\
		\hline
		\bf Related Parameter & \pref{parfluids}\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number&  V\refstepcounter{varnum}\thevarnum\label{typefluidparm} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline
		\bf Description & The type of fluid parameters.\\
		\hline
		\bf Related Commonality & \cref{fluidandboundaryinfo}\\
		\hline
		\bf Related Parameter & \pref{partypeparm}\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number&  V\refstepcounter{varnum}\thevarnum\label{varboundary} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline
		\bf Description & The boundary of the lattice can have various conditions.\\
		\hline
		\bf Related Commonality & \cref{lattice}\\
		\hline
		\bf Related Parameter & \pref{parboundary}\\
		\hline
	\end{tabular}
\end{minipage}\\
\subsubsection{Input}
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number& V\refstepcounter{varnum}\thevarnum\label{interface} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline
		\bf Description & The input interface can vary between LBS.\\
		\hline
		\bf Related Commonality & \cref{fluidandboundaryinfo}\\
		\hline
		\bf Related Parameter & \pref{parinterface}\\
		\hline
	\end{tabular}
\end{minipage}\\
\subsubsection{Output}
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number& V\refstepcounter{varnum}\thevarnum\label{varvisual} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline
		\bf Description & Visual presentation of the prediction.\\
		\hline
		\bf Related Commonality & \cref{predictionstomemory}\\
		\hline
		\bf Related Parameter & \pref{parvarvisual}\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number& V\refstepcounter{varnum}\thevarnum\label{formatoutput} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline
		\bf Description & Format of prediction information.\\
		\hline
		\bf Related Commonality & \cref{predictionstomemory}\\
		\hline
		\bf Related Parameter & \pref{parformatoutput}\\
		\hline
	\end{tabular}
\end{minipage}\\
\subsubsection{System Constraints}
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number& V\refstepcounter{varnum}\thevarnum\label{processing} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline
		\bf Description & Hardware that processes the calculations\\
		\hline
		\bf Related Commonality & None \\
		\hline
		\bf Related Parameter & \pref{parprocessing}\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number& V\refstepcounter{varnum}\thevarnum\label{operatingsystem} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline
		\bf Description & Operating systems on which LBS run.\\
		\hline
		\bf Related Commonality & None \\
		\hline
		\bf Related Parameter & \pref{paropsystem}\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}
		\hline
		\bf Item Number& V\refstepcounter{varnum}\thevarnum\label{storage} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colCwidth} | p{\colDwidth}|}		
		\hline
		\bf Description & Amount of storage and memory needed for the LBS.\\
		\hline
		\bf Related Commonality & None \\
		\hline
		\bf Related Parameter & \pref{parstorageconst}\\
		\hline
	\end{tabular}
\end{minipage}\\
\newpage
\subsection{Parameters of Variation}
This section specifies the parameters of variation for the variabilities listed in Section \ref{var_sec}. They are organized into the same five subcategories as employed previously: Simulation Models, Input, Output, and System Constraints. 

Each parameter of variation is given a unique identifier of the form “P” followed by a natural number. The corresponding variability is listed and a hyperlink is provided that allows navigation back to the appropriate item in Section \ref{var_sec}. The final entry for each parameter of variation is the binding time, which is the time in the software lifecycle when the variability is fixed. The binding time could be during specification, or during building
of the system (build time), or during execution of the system (run time). It is possible to have a mixture of binding times. For instance, a parameter of variation could have a binding time of “specification or building” to represent that the parameter could be set at specification time, or it could be postponed until the given family member is built. The choice of postponing the decision until the build could be associated with the presence of a domain specific language that would allow postponing decisions on the values of the parameter of variation.

\subsubsection{Lattice Boltzmann Method Solvers}
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{parparallel} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{parallel}\\
		\hline
		\bf Range of Parameters & OpenMP, OpenCL, CUDA, MPI are used if the execution of the LBS is parallelized.\\
		\hline
		\bf Binding Time & Build Time\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{paredf} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{varedf}\\
		\hline
		\bf Range of Parameters & Equilibrium approximation varies between incompressible or compressible models.\\		
		\hline
		\bf Binding Time & Build Time\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{parstorage} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{edfstorage}\\
		\hline
		\bf Range of Parameters & Various data structures can be used to store function output, including single and multi-dimensional arrays, depending on the problem model and developer preferences.\\
		\hline
		\bf Binding Time & Build Time\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{paredfcoeff} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{edfcoeff}\\
		\hline
		\bf Range of Parameters & Numerous coefficients for equilibrium distribution function based on number of velocity directions. The number of velocity directions is typically 2, 3, 5, 9, 13, 15, 19, or 27.\\
		\hline
		\bf Binding Time & Build Time\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{pardimensions} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{dimensions}\\
		\hline
		\bf Range of Parameters & LBS model has 1, 2, or 3 dimensions.\\
		\hline
		\bf Binding Time & Build Time or Run Time\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{parveldir} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{numveldir}\\
		\hline
		\bf Range of Parameters & One dimensional models include options of 2, 3, and 5 velocity directions. Two dimensional models include options of 9, 13, and 15 velocity directions. Three dimensional models include options of 15, 19, and 27 velocity directions. Figure \ref{dimension_model} from \citep{thurey2009stable} illustrates two commonly used LBM models in two and three dimensions.\\
		\hline
		\bf Binding Time & Build Time or Run Time\\
		\hline
	\end{tabular}
\end{minipage}
~\newline

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{dimension_model}
		\caption{Commonly used LBM models in two and three dimensions}
		\label{dimension_model}
	\end{center}
\end{figure}

~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{parcollisop} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{varcolop}\\
		\hline
		\bf Range of Parameters & SRT, TRT, MRT, BGK collision operators.\\
		\hline
		\bf Binding Time & Build Time\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{partransequ} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{vartraneq}\\
		\hline
		\bf Range of Parameters & Collision and collision free transport equations. LBS sometimes use one or the other, often the collision transport equation. A Boolean parameter could be used to select between these equations in systems that can apply either equation.\\
		\hline
		\bf Binding Time & Build Time\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{parfluids} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{numberoffluids}\\
		\hline
		\bf Range of Parameters & LBS can model a natural number of fluids.\\
		\hline
		\bf Binding Time & Build Time\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{partypeparm} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{typefluidparm}\\
		\hline
		\bf Range of Parameters & LBS fluid parameters include Reynolds Number, density, viscosity, time, pressure, force, direction, relaxation rate, turbulence. All of these parameters have the type of real number.\\
		\hline
		\bf Binding Time & Build Time\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{parboundary} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{varboundary}\\
		\hline
		\bf Range of Parameters & Lattice boundary can have reflective or non-reflective conditions. Some LBS will only model reflective or non-reflective conditions. If there is a choice then this can be indicated by a Boolean parameter.\\
		\hline
		\bf Binding Time & Build Time\\
		\hline
	\end{tabular}
\end{minipage}\\
\subsubsection{Input}
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{parinterface} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{interface}\\
		\hline
		\bf Range of Parameters & Input can be graphical, text or file.\\
		\hline
		\bf Binding Time & Build Time\\
		\hline
	\end{tabular}
\end{minipage}\\
\subsubsection{Output}
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{parvarvisual} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{varvisual}\\
		\hline
		\bf Range of Parameters & LBS can provide 1D, 2D, and 3D rendering of  the model.\\
		\hline
		\bf Binding Time & Build Time or Run Time\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{parformatoutput} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{formatoutput}\\
		\hline
		\bf Range of Parameters & LBS prediction information is output in either text or binary format.\\
		\hline
		\bf Binding Time & Build Time\\
		\hline
	\end{tabular}
\end{minipage}\\
\subsubsection{System Constraints}
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{parprocessing} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{processing}\\
		\hline
		\bf Range of Parameters & The LBS model can be calculated on the CPU or GPU.\\
		\hline
		\bf Binding Time & Build Time\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{paropsystem} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{operatingsystem}\\
		\hline
		\bf Range of Parameters & LBS can be run on Windows, MacOS, or Linux versions.\\
		\hline
		\bf Binding Time & Build Time\\
		\hline
	\end{tabular}
\end{minipage}
~\newline
~\newline
\noindent
\begin{minipage}{\textwidth}
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}
		\hline
		\bf Item Number& P\refstepcounter{parnum}\theparnum\label{parstorageconst} \\
		\hline
	\end{tabular}\\
	
	\begin{tabular}{| p{\colEwidth} | p{\colFwidth}|}		
		\hline
		\bf Corresponding Variability & \vref{storage}\\
		\hline
		\bf Range of Parameters & The amount of memory and storage varies between LBS.\\
		\hline
		\bf Binding Time & Run Time\\
		\hline
	\end{tabular}
\end{minipage}\\

\newpage
\section{Methodology}

In this project we set out to answer the research questions listed in Section \ref{purpose} for the LBM SCS domain. The process involved systematically measuring and analyzing members of this software family along the quality attributes listed in the research questions and described in Section \ref{softwarequalities}. The methodology includes gathering quantitative and qualitative measurements. A goal of the project is to produce a quality assessment for LBM software packages.  

The methodology used in this LBM software assessment is a general state of the practice methodology that can be applied to any SCS domain. It was developed as an update to previous state of the practice exercises. 

We collected quantitative data using the measures found in the measurement template in Appendix \ref{measurementtemplate2}. Some of this was empirical software engineering related data, such as the number of files, number of lines of code (LOC), percentage of issues that are closed, etc. Most of the data was gathered by manually investigating the software, its source code, and its artifacts, while some was gathered using the empirical measurement tools discussed in Section \ref{empiricalmeasures}.

We also collected qualitative data by interviewing the software package developers and asking them the questions found in Appendix \ref{interviewquestions}. Ethics clearance information can be found in Appendix \ref{ethicsapproval}. Furthermore, we solicited the assistance of domain experts to better assess each software package by leveraging their experience to assess the functional and non-functional requirements for the software domain. 

This section begins by describing the steps of the overall process we used to select, measure and compare LBM software. This is followed by quality definitions and how these qualities were assessed in our project. The rest of the section provides an overview of how candidate software packages were selected and filtered, the empirical measurements and software tools that were used, and the AHP. 

\subsection{Process}

 The following steps provide an overview of how the assessment was conducted: 

\begin{enumerate}
	\item List candidate software packages for the domain. This is discussed in Section \ref{identifysoftware}.
	\item Filter the software package list. This is discussed in Section \ref{filtersoftware}.
	\item Gather the source code and documentation for each software package.
	\item Collect empirical measures. This is discussed in Section \ref{empiricalmeasures}.
	\item Measure using the measurement template. This is discussed in Section \ref{empiricalmeasures}. The measurement template can be found in Appendix \ref{measurementtemplate2}.
	\item Survey the developers. The developers of each software package in the filtered software list were contacted for voluntary interviews. The interview questions can be found in Appendix \ref{interviewquestions}.
	\item Use AHP to rank the software packages. This is discussed in Section \ref{AHP}.
	\item Analyze the results and answer the research questions. The answers can be found in Section \ref{answersquestions}.
\end{enumerate}

These steps are further detailed in the \href{https://github.com/smiths/AIMSS/blob/master/StateOfPractice/Methodology/Methodology.pdf}{Methodology for Assessing the State of the Practice for Domain X} document.

\subsection{Software Qualities}\label{softwarequalities}

Software quality attributes facilitate the measurement and comparison of software packages in this state of the practice assessment. We adopt software quality definitions from various researchers and subject matter expert entities. Some of the definitions are from \cite{Smithetal2020}. The quality measurement results are found in Section \ref{AHPresults} of this report. Section \ref{qualityrecommentations} lists recommendations to address software qualities in LBM software packages. The following are the software quality definitions used in this state of the practice exercise, along with comments regarding their quantitative and qualitative measurement.

\subsubsection{Installability} 

Installability is measured by the effort required for the installation, uninstallation or reinstallation of a software product in a specified
environment \citep{ISO/IEC25010} \citep{lenhard2013measuring}. A good measure of installability correlates with scenarios when low or moderate effort is required to gather and prepare software for its general use on a system for which it was designed. In this case effort includes the time spent finding and understanding the installation instructions, the man-time and resources spent performing the installation procedure, and the absence or ease of overcoming system compatibility issues. The ability to reasonably validate the installation procedure also has a positive effect on the measure of installability. Similarly, the ease of uninstallation has an affect on the measure of installability. 

\subsubsection{Correctness}

 A software program is correct if it behaves according to its stated
specifications \citep{GhezziEtAl2003}. This requires that the specification is available. Software is unlikely to have a formal specification if it is not developed by seasoned or professional software developers. Since some software does not have a specification available, the correctness of software cannot always be verified. Despite an absent specification, the correctness of the output of scientific computing software can sometimes be manually verified by applying domain knowledge. A good measure of correctness correlates with the availability of a requirements specification and reference to domain theory, as well as the explicit use of tools or techniques for building confidence of correctness, such as documentation generators and software analysis tools.

\subsubsection{Verifiability}

Verifiability is measured by the extent to which a set of tests can be written and executed to demonstrate that the delivered system meets the specification \citep{sommerville}. Similarly to correctness, verifiability is correlated with the availability of a specification and with reference to domain knowledge. A good measure of verifiability is further correlated with the availability of well written tutorials that include expected output, with software unit testing documentation, and with evidence of continuous integration during the development process. 

\subsubsection{Reliability}

Reliability is measured by the probability of failure-free operation of a computer program in a specified environment for a specified time, i.e. the average time interval between two failures also known as the mean time to failure (MTTF) \citep{GhezziEtAl2003} \citep{musa1987software}. Reliability is thus positively correlated with the absence of errors during installation and use. Recoverability from errors also improves reliability.

\subsubsection{Robustness}

Software possesses the characteristic of robustness if it behaves “reasonably” in two situations: i) when it encounters circumstances not anticipated in the requirements specification; and ii) when the assumptions in its requirements specification are violated \citep{boehm2007software} \citep{ghezzi1991fundamentals}. A good measure of robustness correlates with a reasonable reaction to unexpected input, including data of the wrong type, empty input, or missing files or links. A reasonable reaction includes an appropriate error message and the ability to recover the system.

\subsubsection{Performance}

Performance is measured by the degree to which a system or component accomplishes its designated functions within given constraints, such as speed (database response times, for instance), throughput (transactions per second), capacity (concurrent usage loads), and timing (hard real-time demands) \citep{IEEEStdGlossarySET1990} \citep{wiegers2003softreq}. In this state of the practice assessment performance was not quantitatively measured. Instead the documentation of each software package was observed for information that alludes to a consideration of performance, such as parallelization tools. 

\subsubsection{Usability}

Usability is measured by the extent to which a software product can be used by specified users to achieve specified goals with effectiveness, efficiency, and satisfaction in a specified context of use \citep{nielsonusability}. A good measure of usability correlates with the presence of documentation, including tutorials, manuals, and defined user characteristics, and user support. Preferably the user support model has avenues to contact developers and report issues.

\subsubsection{Maintainability}

A measure of maintainability is the effort with which a software
system or component can be modified to correct faults, improve performance or other attributes, and satisfy new requirements \citep{IEEEStdGlossarySET1990} \citep{boehm2007software}. In this state of the practice analysis maintainability is measured by the quality of documentation artifacts, and the presence of version control and issue tracking. These artifacts can greatly decrease the effort needed to modify software. There are many documentation artifacts that can improve maintainability, including user and developer manuals, specifications, README files, change logs, release notes, publications, forums, and instructional websites. 

\subsubsection{Modifiability}

Modifiability refers to the ease with which stable changes can be made to a system and the flexibility of the system to adopt such changes \citep{8016712}.
This state of the practice assessment did not quantitatively measure modifiability. Developers were asked in interviews if they considered the ease of future changes when developing the software packages, specifically changes to the structure of the system, modules and code blocks. A follow up question asked if any measures had been taken.

\subsubsection{Reusability}

Reusability refers to the extent to which components of a software package can be used with or without adaptation in other software packages \citep{kalagiakos2003non}. A good measure of reusability results from a large number of easily reusable components. Increased software modularization, defined as the presence of smaller components with well defined interfaces, is important. For this state of the practice assessment, a good measure of reusablity correlates with an increased number of code files, and the availability of API documentation.

\subsubsection{Understandability}

Understandability is measured by the capability of the software package to enable the user to understand its suitability and function \citep{ISO9126}.
It is an artifact-dependent quality. Understandability is different for the user-interface, source code, and documentation. In this state of the practice analysis, understandability focuses on the source code. It is measured by the consistency of a formatting style, the extend of modularization, the explicit identification of coding standards, the presence of meaningful identifiers, and clarity of comments. 

\subsubsection{Traceability}

Traceability refers to the ability to link the software implementation and the software artifacts, especially the requirement specification \citep{McCallEtAl1977}. Similar to the quality of correctness, this requires some form of specification to be available. The quality refers to keeping track of information as it changes forms or relates between artifacts. This state of the practice assessment did not quantitatively measure traceability. Developers were asked in interviews how documentation fits into their development process.

\subsubsection{Visibility and Transparency}

Visibility and transparency refer to the extent to which all of the steps of a software development process, and the current status of it, are conveyed clearly \citep{ghezzi1991fundamentals}. In this state of the practice assessment a good measure of visibility and transparency correlates with a well defined development process, the presence of development process and environment documentation, and software package version release notes. 

\subsubsection{Reproducibility}

Software achieves reproducibility if another developer can take the requirements documentation and re-obtain the same software artifacts \citep{BenureauAndRougier2017}. This includes the output of the software, where the scientific results are compared between software implementations, or between software implementations and manually calculated results. This state of the practice assessment did not quantitatively measure reproducibility. Developers were asked in interviews if they have any concern that their computational results won't be reproducible in the future, and if they have taken any steps to ensure reproducibility.

\subsubsection{Unambiguity}

Unambiguity refers to the extent to which two readers have similar interpretations when reading software artifacts. In other words, artifacts are unambiguous if, and only if, they only have one interpretation \citep{IEEE1998}. This state of the practice assessment did not quantitatively measure unambiguity. Developers were asked in interviews if they think that the current documentation can clearly convey all necessary knowledge to the users, and how they achieved this or what improvements are needed to achieve it.

\subsection{Identify Candidate Software}\label{identifysoftware}

The candidate software was found through search engine queries targeting authoritative lists of software. We found LBM software listed on the websites GitHub and swMATH, as well as through articles found in scholarly journals and databases. Software packages that are not on the authoritative lists that we used were not assessed. Packages that fit our criteria but were found after data collection and analysis was conducted, such as Musubi, will need to be added to future SOP assessments.

The following properties were considered when creating the list and reviewing the candidate software:

\begin{enumerate}
	\item The software functionality must fall within the identified domain.
	\item The source code must be viewable.
	\item The empirical measures should be available, which implies a preference for GitHub-style repositories.
	\item The software cannot be marked as incomplete or in an initial development phase.
\end{enumerate}

The initial list had 45 packages, including a few packages that were later found to not have publicly available source code, or to be in an incomplete state of development. 

\subsection{Filter the Software List}\label{filtersoftware}

To reduce the number of members in the candidate software list to a manageable size, the following filters were applied. The filters were applied in the priority order listed.

\begin{enumerate}
	\item Scope: Software is removed by narrowing what functionality is considered to be within the scope of the domain.
	\item Usage: Software packages were eliminated if their installation procedure was missing or not clear and easy to follow.
	\item Age: The older software packages (age being measured by the last date when a change was made) were eliminated, except in the cases where an older software package appears to be highly recommended and currently in use. 
\end{enumerate}

For the third item in the above filter, software packages were characterized as `alive' if their related documentation had been updated within
the last 18 month. Packages were categorized as `dead' if the last update of this information was more than 18 month ago.\\ 

\begin{onehalfspacing}
\begin{center}
	\begin{tabular}{ p{4cm}p{3cm}p{3cm} }
		\hline
		Name & Released & Updated\\
		\hline
		DL\_MESO (LBE) & unclear & 2020 Mar\\
		ESPResSo & 2010 Nov & 2020 Jun\\
		ESPResSo++ & 2011 Feb & 2020 Apr\\
		lbmpy& unknown  & 2020 Jun  \\
		lettuce & 2019 May & 2020 Jul\\
		Ludwig& 2018 Aug & 2020 Jul\\
		LUMA& 2016 Nov   & 2020 Feb\\
		MechSys & 2008 Jun    & 2020 Jul\\
		OpenLB & 2007 Jul & 2019 Oct\\
		Palabos & unclear & 2020 Jul\\
		pyLBM & 2015 Jun&   2020 Jun\\
		Sailfish & 2012 Nov & 2019 Jun\\
		TCLB & 2013 Jun  & 2020 Apr\\
		waLBerla & 2008 Aug & 2020 Jul\\
		\hline
	\end{tabular}
	\captionof{table}{Alive Software Packages}\label{alivepackages}
\end{center}
\end{onehalfspacing}

While the initial list had 45 packages, filtering by scope, usage, and age decreased the size of the list to 23 packages. Many of the 22 packages that were removed could not be tested as there was no installation guide, they were incomplete, source code was not publicly available, a license was needed, or the project was out of scope or not up to a standard that would support incorporating them into this study. These eliminated software packages are listed in the Appendix in Section \ref{eliminatedpackagessection}. Of the remaining 23 packages that were studied, some were kept on the list despite being marked as dead due to their prevalence on authoritative lists on LBM software and due to their surface excellence, specifically the considerable time that was put into these projects.  

The final list of software packages that were analyzed in this project can be found in the following two tables. Table \ref{alivepackages} lists packages that fell into the `alive' category as of mid 2020, and Table \ref{deadpackages} lists packages that were `dead' at that time. \\

\begin{onehalfspacing}
\begin{center}
	\begin{tabular}{ p{4cm}p{3cm}p{3cm} }
		\hline
		Name & Released & Updated\\
		\hline
		HemeLB & 2007 Jun & 2018 Aug\\
		laboetie & 2014 Nov & 2018 Aug\\		
		LatBo.jl & 2014 Aug & 2017 Feb\\
		LB2D-Prime & 2005 & 2012 Apr\\
		LB3D & unclear & 2012 Mar\\
		LB3D-Prime & 2005 & 2011 Oct\\
		LIMBES & 2010 Nov & 2014 Dec\\
		MP-LABS & 2008 Jun & 2014 Oct\\
		SunlightLB & 2005 Sep & 2012 Nov\\
		\hline
	\end{tabular}
	\captionof{table}{Dead Software Packages}\label{deadpackages}
\end{center}
\end{onehalfspacing}

There is considerable variation among these software packages, including their intended purpose, size, user interfaces, and software languages used. For example, the OpenLB software package is predominantly a C++ package that makes use of hybrid parallelization and was designed to address a range of CFD problems \citep{heuveline2009towards}. The software package pyLBM is an all-in-one Python language package for numerical simulations \citep{graille2017pylbm}. ESPResSo is an extensible simulation package that is specifically for research on soft matter, and is written in C++ and Python \citep{weik2019espresso}. The HemeLB package is used for efficient simulation of fluid flow in several medical domains, and is written predominantly in C, C++, and Python \citep{mazzeo2008hemelb}. 

\subsection{Empirical Measures}\label{empiricalmeasures}

The quality measurements in this assessment rely on the gathering and analyzing of raw and processed empirical data related to the research questions listed in Section \ref{purpose}. 

All of the quality measurements that are part of the AHP analysis are empirical measurements. Qualitative data gathered during interviews with developers is not part of the AHP analysis. This part of the assessment focuses on data that is reasonably easy to collect. Much of the data was gathered by manually reviewing the artifacts of each software package, while some was gathered using the freeware tools discussed below. The data that was gathered is listed in the measurement template found in Appendix \ref{measurementtemplate2}. Some of the data required processing other data within the template, including the status of the software package, which relied on the last commit date; the percentage of issues that are closed, which relied on the number of open and closed issues; and the percentage of code that is comments, which relied on the number of total lines and comment lines in text-based files. The complete measurement template data was then analyzed using the grading template found in Appendix \ref{gradingtemplate2}, the output of which was analyzed using the AHP described in Section \ref{AHP}.

Most of the measurement template data was gathered by observing GitHub repository metrics and software package artifacts, or by processing data gathered using freeware tools. Data in the final three sets of the measurement template was collected using these tools. The tool \href{https://github.com/tomgi/git_stats}{GitStats} was used to measure each software package's GitHub repository for the number of binary files, the number of added and deleted lines, and the number of commits over varying time intervals. The tool \href{https://github.com/boyter/scc}{Sloc Cloc and Code (scc)} was used to measure the number of text based files as well as the number of total, code, comment, and blank lines in each GitHub repository. Details on installing and running these tools can be found in the \href{https://github.com/smiths/AIMSS/blob/master/StateOfPractice/Methodology/A
	Guide to Empirical Measures.pdf} {Guide to Empirical Measures} file in the AIMSS repository. 

\subsection{Analytical Hierarchy Process}\label{AHP}

The Analytical Hierarchy Process (AHP) is a decision-making technique that is used to compare multiple options by multiple criteria. In our work AHP was used for comparing and ranking the LBM software packages using the overall impression quality scores that were gathered in the measurement template found in Appendix \ref{measurementtemplate2} using the grading template found in Appendix \ref{gradingtemplate2}. AHP performs a pairwise analysis using a matrix and generates an overall score as well as individual quality scores for each software package. Smith et al. (2016) shows how the AHP is applied to ranking software based on quality measures \cite{SmithEtAl2016}. 

This project used a tool for conducting this process. The tool includes a sensitivity analysis that was used to ensure that the software package rankings are appropriate with respect to the uncertainty of the quality scores. The \href{https://github.com/smiths/AIMSS/blob/master/StateOfPractice/AHP2020/LBM/README.txt}{README} file of the tool includes requirements and usage information.

\newpage

\section{Quantitative Findings and AHP Results}\label{AHPresults}

This section presents the quantitative findings from data that was gathered using the measurement template found in Section \ref{measurementtemplate2} for qualities listed in Section \ref{softwarequalities}. The results of the AHP analysis based on that data are also presented in this section.

\subsection{Installability}

All of the 23 software packages that were tested have installation instructions. As noted previously, many of the 23 software packages that were part of the original long list of 45 packages were removed due to not including documentation or installation instructions. 

All 23 packages on the short list can be installed on some Unix-like systems. Seven packages could be installed on Windows, and five on macOS. Operating system compatibility is found in the documentation of 19 software packages. All but one of the software packages, TCLB, were tested on Ubuntu for this state of the practice assessment. TCLB was tested on CentOS, since this operating system is mentioned in its installation instructions.

Of the software packages that were tested, most installation instructions are located in one place, often in an instruction manual or on a web-page. Sometimes, like with Ludwig, incomplete installation instructions are found on a home page, with more detailed instructions located on another web-page, or within the documentation. Maintainability and correctness of these instructions could be improved if all the instructions were in one location. 

All but one of the software packages (LatBo.jl) have automated at least some of the installation process. Most of these packages, such as waLBerla and SunlightLB, use Make to automate the installation, and a few of them, like lbmpy, use custom scripts.

Errors encountered during the installation process were often quickly fixed thanks to descriptive error messages. Systems that provided vague error messages, such as messages that did not specify which action or file was at fault, were more difficult to troubleshoot. Only three software packages (HemeLB, LB3D, lbmpy) that displayed a descriptive error message were not recoverable, and most of these instances were due to hardware and operating system incompatibility, such as the requirement of CUDA. Fourteen software packages definitively broke during installation. Some packages, such as LB2D-Prime and LB3D-Prime, did not provide a definitive message of the success or failure of installation. In these instances, validating the installation required performing a tutorial or running a script, as described below, if these were available. 

About half of the installation instructions are written as if the person doing the installation has none of the dependent packages installed. It is common for software packages to not list all of their dependencies despite listing some. This was the case for many packages, including ESPResSo++, Ludwig, and LUMA. Sometimes only an error message during the installation process informs the user of the requirement of these additional packages. A detailed rewrite of the installation instructions from the point of view of installation on a clean operating system is suggested. A clean environment can be achieved for testing purposes by using a virtual machine.

Sixteen software packages require less than 10 dependencies to be installed. All but one software package (LatBo.jl) require less than 20 dependencies. Some packages may automatically install additional dependencies in the background. Eighteen of the software packages do not explicitly indicate software dependency versions. Some software package installation issues, specifically those occurring when manual installation of dependencies is required, may be avoided if versions of dependencies are specified. Fifteen software packages do not have detailed instructions for installing dependencies. 

Sixteen software packages have less than 10 manual installation steps. If dependencies are installed in one command then none of the software packages take more than 20 steps to install. The average number of steps is about eight, and the fewest is two (LB3D-Prime). 

All but six (ESPResSo, HemeLB, laboetie, LB3D-Prime, lbmpy, waLBerla) of the software packages have a way to specifically verify the installation. Most have some sort of tutorial examples that can be run by the user. Some other ways of installation validation include validation scripts (LB2D-Prime, lettuce, Ludwig, LUMA), automatic validation after the installation (LatBo.jl), and instructions to manually review the file system (LIMBES). 

Uninstallation instructions were found for only one of the software packages, pyLBM.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{installability_chart}
		\caption{AHP Installability Score}
		\label{Fig_Installability}
	\end{center}
\end{figure}

Figure \ref{Fig_Installability} shows the installability ranking of the software packages using AHP. Software packages with a higher score (MechSys, Palabos, ESPResSo, OpenLB) tend to have one set of linear installation instructions that are written as if the person doing the installation has none of the dependencies installed. The instructions often list compatible operating system versions and include instructions for the installation of dependencies. The top ranked packages often incorporate some sort of automation of the installation process and have fewer manual installation steps. The number of dependencies a package has does not correlate with a higher score. The ability to validate the installation process, often through tutorials or test examples that include expected output, is correlated with a higher score. Furthermore, the top six ranked packages are noted as being alive. 

Many software packages would benefit from a rewrite or reorganization of installation instructions. A single location for installation instructions would improve their maintainability and correctness. Listing compatible operating system and dependency versions would decrease installation time and errors, as would adding instructions on installing dependencies. Installation process errors should prompt the system to display detailed messages. Once a software package is installed, either an automatic validation needs to be performed or the user needs to be able to perform a manual validation using test examples that include expected output. Finally, uninstallation instructions should be included in the documentation. 
 
\subsection{Surface Correctness and Verifiability}

Sixteen of the software packages include a requirements specification artifact or explicitly reference domain theory, often only the latter. Software packages that distribute requirements specification information, such as DL\_MESO (LBE), generally keep it brief and include it within other documentation. This artifact is often found within a user manual, on a web-page, or is mentioned in related publications. In the latter case the user may need to spend significant time to find this information. 

Document generation tools are explicitly used by 12 software packages. Sphinx is used by eight of them, and Doxygen is used by seven. Several of the packages use both.

Tutorials are available for 18 of the software packages. Generally they are linearly written and easy to follow. However, only eight tutorials provide an expected output. It is not possible to verify the correctness of the output of the software packages that are missing this key information. In these cases the user may need to assume correctness if there are no visible errors.

Unit tests are only explicitly available for one of the software packages, Ludwig. Code modularization of most packages allow for users to create tests with varying degrees of effort. These tests allow developers and users to verify the correctness of fragments of the source code, and in doing so better assess the correctness of the entire package.

The use of continuous integration tools and techniques alludes to a more refined development process where faults are isolated and better recognized. Only two of the packages (ESPResSo, Ludwig) mentioned applying the practice of continuous integration in their development process. 

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{correctnessverifiability_chart}
		\caption{AHP Surface Correctness and Verifiability Score}
		\label{Fig_CorrectnessVerifiability}
	\end{center}
\end{figure}

Figure \ref{Fig_CorrectnessVerifiability} shows the surface correctness and verifiability ranking of the software packages using AHP. Software packages with a higher score tend to have a visible requirements specification or references to theory documentation. They also explicitly use at least one document generation tool that builds confidence of correctness. The top ranked software packages all include an easy to follow getting started tutorial, and most of these include expected output. Only the top ranked package, Ludwig, provided unit testing. It and the second ranked package, ESPResSo, explicitly incorporated continuous integration in the development process. Furthermore, eight of the top 10 ranked packages are noted as being alive.

The inclusion of requirements specification and theory documentation greatly benefits the correctness and verifiability of software packages. The use of document generation tools can help build confidence in correctness. The addition of easy to follow tutorials further helps users verify the software and have confidence in its correctness. Unit testing documentation and capability, as well as the use of continuous integration tools and techniques such as Bamboo, Jenkins, and Travis CI, help verify correctness.

\subsection{Surface Reliability}

The analysis of surface reliability focused on package installation and tutorials. Errors occurred when installing 16 of the software packages. Every instance prompted an error message. These messages indicated unrecognized commands (even when following the installation guide), missing links, missing dependencies, syntax errors in code files. In some instances the error messages were vague. Several automatic installation processes could not find and load dependencies. In these instances the installation tried to access outdated external repositories. Seven of the installations were recovered and verified, and one of the installations (LB3D-Prime) was assumed to be recovered due to the absence of any way to verify it. The installation of eight of the software packages could not be recovered. Most of these broken installations could not find external dependencies, encountered system incompatibilities, or displayed vague error messages. 

Of the 13 software packages that installed correctly and also have tutorials, four (pyLBM, ESPResSo++, LIMBES, Ludwig) broke during tutorial testing. All of these instances resulted in an error message being displayed. One error (pyLBM) was due to a missing tutorial dependency, another (Ludwig) was due to an invalid command despite following the tutorial, and the final two errors were vague execution errors. Of the four broken tutorial instances, only the one that was missing a dependency was recoverable. 

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{reliability_chart}
		\caption{AHP Surface Reliability Score}
		\label{Fig_Reliability}
	\end{center}
\end{figure}

Figure \ref{Fig_Reliability} shows the surface reliability ranking of the software packages using AHP. Software packages with a high score either did not break during installation, or the broken installation was recoverable. All of the top five ranked packages have tutorials. One of these packages, pyLBM, broke during tutorial testing, but a descriptive error message helped in recovery. Furthermore, nine of the top 10 ranked packages are noted as being alive. 

Overall, lower ranked software packages are lacking clear documentation, testing or tutorial examples, and descriptive error messages, and have broken dependencies. Thus, regarding surface reliability, software packages would benefit from clear up-to-date documentation that specifies all dependencies, the inclusion of testing and tutorial examples, and the assurance of descriptive error messages during fault conditions.

\subsection{Surface Robustness}

The software packages were tested for handling unexpected input, including incorrect data types, empty input, and missing files or links. Success predicated on a reasonable response from the system, including appropriate error messages and an absence of unrecoverable system failures. 

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{robustness_chart}
		\caption{AHP Surface Robustness Score}
		\label{Fig_Robustness}
	\end{center}
\end{figure}

Figure \ref{Fig_Robustness} shows the surface robustness ranking of the software packages using AHP. Software packages with a high score behaved reasonably in response to unexpected input as described above. All of the software packages that installed correctly passed this test. They output descriptive error messages or did not crash. Software packages with a lower surface robustness score had not installed correctly, so their robustness score may not be a true reflection of runtime robustness. Similarly, all software packages that installed correctly and require plain text input files correctly handled an unexpected change to these input files, including a replacement of new lines with carriage returns. Furthermore, nine of the top 10 ranked packages are noted as being alive. LIMBES is noted as being dead.

\subsection{Surface Performance}

Although the software packages all apply LBM to solve scientific computing problems, the packages focus on varied CFD problems, with varying parameters, and are technically different from each other. Due to this, a comparison of performance is not appropriate. In this project we instead looked through each software package's artifacts for evidence that performance was considered. The artifacts of 17 software packages mentioned parallelization. This included GPU processing and the CUDA parallel computing platform, which were mentioned in the artifacts of 6 packages (ESPResSo, lbmpy, lettuce, pyLBM, Sailfish, TCLB). GPUs provide superior processing power and speed compared to CPUs, and are often used for scientific computing when a large amount of data is involved. The software package TCLB is implemented in a highly efficient multi-GPU code to achieve performance suitable for model optimization \citep{rutkowski2020open}. In the Ludwig package, a so-called mixed mode approach is used where fine-grained parallelism is implemented on the GPU, and MPI is used for even larger scale parallelism \citep{gray2013ludwig}. While one software package (Sailfish) required CUDA and GPU processing, some (ESPResSo, lbmpy, lettuce, pyLBM, TCLB) have the option of using either the GPU or the CPU. The packages that require GPU and CUDA have better performance at the expense of installability and surface reliability.

\subsection{Surface Usability}

Software package artifacts were reviewed for the presence of a tutorial, a user manual, documented user characteristics, and a user support model. In total 18 software packages have a tutorial, 13 have a user manual, and 11 have both. The tutorials vary in scope and substance, and eight include an expected output. Most user manuals are in the form of a file that can be downloaded, while some are rendered on a web-page. Some packages (waLBerla) do not have a user manual, but do have useful documentation distributed on their web-pages. Expected user characteristics are documented in four software packages (laboetie, LIMBES, Ludwig, Palabos). Users are typically scientists or engineers. Their background is often physics, chemistry, biophysics, or mathematics. All but one of the packages (LIMBES) have a user support model, and many of them have multiple avenues of user support. The most popular avenue of support is Git, followed by email and forums. One software package (OpenLB) has an FAQ page.    

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{usability_chart}
		\caption{AHP Surface Usability Score}
		\label{Fig_Usability}
	\end{center}
\end{figure}

Figure \ref{Fig_Usability} shows the surface usability ranking of the software packages using AHP. Software packages with a high score have a tutorial and user manual, sometimes have documented user characteristics, and have at least one user support model. Many packages have several user support models. Furthermore, four of the top five ranked packages are noted as being alive. 

\subsection{Maintainability}

Software packages were reviewed for the presence of artifacts. Every type of artifact or file that is not a code file was recorded. The software packages were also reviewed for software release and documentation version numbers. This information could be used to better troubleshoot issues and organize documentation. All but three software packages (LatBo.jl, LB3D-Prime, MechSys) have source code release and documentation version numbers.

Information on how code is reviewed, or how to contribute to the project was also noted. In total, 11 software packages have this information, which was found in various artifacts, including in developer guides, contributor guides, user guides, developer web-pages, and README files. 

Issue tracking is used in 22 software packages, 15 of which use Git, six use email, and one (SunlightLB) uses SourceForge. Most software packages that use Git have most of their issues closed, and only three (laboetie, lettuce, Sailfish) have less than 50 percent of their issues closed. Four of the top five overall ranked packages (Ludwig, ESPResSo, Palabos, LUMA) have most of their issues closed. Fourth ranked LUMA does not use Git. Alive packages (11 use Git issue tracking) have 64\% of their issues closed, while dead packages (3 use Git issue tracking) have 71\% of their issues closed. This information is presented in Table \ref{gitrepodata}. Furthermore, 13 packages that use Git for issue tracking use GitHub as a version control system, while two (Palabos, waLBerla) use GitLab. Of the other packages, one package (SunlightLB) uses CVS for issue tracking, and seven packages do not appear to use any issue tracking system.

Software package code files were further measured for the percentage of code that is comments. The findings are presented in Table \ref{gitrepodata}. Packages with a higher percentage of comments were designated as more maintainable. Comments represent more than 10 percent of code files in 15 packages, and the average percentage of code comments is about 14 percent. Four of the top five overall ranked packages (Ludwig, ESPResSo, Palabos, OpenLB) have more than the average. Fifth ranked LUMA has only 0.2 percent comments, the fewest of any package. The package has the most lines of source code, with over four million. The next largest package is ESPResSo++ with one million.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{maintainability_chart}
		\caption{AHP Maintainability Score}
		\label{Fig_Maintainability}
	\end{center}
\end{figure}

Figure \ref{Fig_Maintainability} shows the maintainability ranking of the software packages using AHP. Software packages with a high score provide version numbers on documents and source code releases, have an abundance of high quality artifacts, and use an issue tracking tool and version control system. These packages also appear to reasonably handle tracked issues, having most of their issues closed. Their code files are well commented with more than 10 percent of the code being comments. Furthermore, nine of the top 10 ranked packages are noted as being alive. MP-LABS is noted as being dead.\\

\begin{onehalfspacing}
\begin{center}
	\begin{tabular}{ p{3.5cm}p{3.5cm}p{3.5cm}p{2.5cm} }
		\hline
		Name & $\%$ Issues Closed & $\%$ Code Comments& Status\\
		\hline
		DL\_MESO (LBE) & Not Git & 8.06&Alive\\
		ESPResSo & 89.26 & 21.78&Alive\\
		ESPResSo++ & 66.28 & 17.10&Alive\\
		HemeLB & No Issues & 16.68&Dead\\
		laboetie & 18.75 & 2.47&Dead\\		
		LatBo.jl & 93.33 & 0.40&Dead\\
		LB2D-Prime & Not Git & 13.61&Dead\\
		LB3D & Not Git & 13.76&Dead\\
		LB3D-Prime & Not Git & 14.34&Dead\\
		lbmpy& 58.33  & 2.03  &Alive\\
		lettuce & 33.33 & 8.19&Alive\\
		LIMBES & Not Git & 17.39&Dead\\
		Ludwig& 60.00 & 20.70&Alive\\
		LUMA& 85.71   & 0.20&Alive\\
		MechSys & Not Git & 15.11&Alive\\
		MP-LABS & 100.00 & 26.67&Dead\\
		OpenLB & Not Git & 22.43&Alive\\
		Palabos & 89.47 & 17.76&Alive\\
		pyLBM & 66.67& 16.12&Alive\\
		Sailfish & 22.22 & 9.26&Alive\\
		SunlightLB & Not Git & 17.67&Dead\\
		TCLB & 60.32 & 6.02&Alive\\
		waLBerla & 72.90 & 22.62&Alive\\
		\hline
	\end{tabular}
	\captionof{table}{Git Repository Data}\label{gitrepodata}
\end{center}
\end{onehalfspacing}

\subsection{Reusability}\label{reusabilityresults}

Each software package was measured for the total number of source code files. A larger number of source files was associated with increased reusability due to increased modularization. Some packages have more features than others, consequently contributing to reusablility since they have more source code that can be reused. The software packages were also reviewed for the presence of API documentation, which indicates that a software package was developed with interaction between other software applications in mind. 

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{reusability_chart}
		\caption{AHP Reusability Score}
		\label{Fig_Reusabilty}
	\end{center}
\end{figure}

Figure \ref{Fig_Reusabilty} shows the reusability ranking of the software packages using AHP. Software packages with a high score have thousands of source code files and API documentation. The highest scoring packages, ESPResSo and waLBerla, have extensive functionality, including graphical visualizations as well as modeling that does not use LBM. For this reason a comparison with other software packages is not on a level field. However, these packages do have an abundance of reusable components. Furthermore, nine of the top 10 ranked packages are noted as being alive. HemeLB is noted as being dead.

	\begin{center}
		\begin{onehalfspacing}
		\begin{tabular}{ p{3.5cm}p{2cm}p{2.5cm}p{2cm}p{2.5cm} }
			\hline
			Name & Text Files & Binary Files & LOC & Avg. LOC / Text File\\
			\hline
			DL\_MESO (LBE) & 310 & 51 & 170223& 549\\
			ESPResSo &1309& 86 & 186700&143\\
			ESPResSo++ &5328& 66 & 969196&182\\
			HemeLB &1065& 48 & 95104&89\\
			laboetie &133& 1 & 48403&364\\		
			LatBo.jl &41& 0 & 42172&1029\\
			LB2D-Prime &82& 19 & 54755&668\\
			LB3D &99& 76 & 39766&402\\
			LB3D-Prime &23& 6 & 12944&563\\
			lbmpy&201&  28 & 46489  &231\\
			lettuce &62& 0 & 5529&89\\
			LIMBES &26& 1 & 4872&187\\
			Ludwig&859& 32 & 109811&128\\
			LUMA&312& 19 & 4370670&14000\\
			MechSys &324& 3 & 85543&264\\
			MP-LABS &307& 3 & 43124&140\\
			OpenLB &1104& 5 & 209034&189\\
			Palabos &1829& 67 & 547623&299\\
			pyLBM &258& 85& 32314&125\\
			Sailfish &632& 11 & 69398&110\\
			SunlightLB & 36& 1 & 7646& 212\\
			TCLB &535& 7 & 43226&81\\
			waLBerla & 2395 & 67 & 848146&353\\
			\hline
		\end{tabular}
		\captionof{table}{Module Data}\label{moduledata}
		\end{onehalfspacing}
	\end{center}

Table \ref{moduledata} shows file and line of code data of the software packages. Packages with a high reusability score do not have many LOC per text file, generally having a few hundred lines or less. This suggests that the source code of these packages is likely functionally modularized, and modules could be reused in other projects.

There was a strong focus on modularity when designing the waLBerla framework to enhance productivity, reusability, and maintainability \citep{bauer2021walberla}. Its software design has enabled waLBerla to be successfully applied in several projects as a basis for various extensions \citep{bauer2021walberla}.

\subsection{Surface Understandability}

Ten random source code files of each software package were reviewed for several measures. This assessment of surface understandability may not perfectly reflect each package due to the practical limitation of only being able to test 10 files. 

All of the packages appear to have a consistent indentation and formatting style. Only LUMA and HemeLB explicitly identify coding standards that are used during development. Generally, the software packages use consistent, distinctive, and meaningful code identifiers. Only four packages (LB2D-Prime, LB3D-Prime, LIMBES, MP-LABS) appear to use vague identifiers, such as single letters for variables. Symbolic constants were observed in the source code of 12 packages. The constants are used for various parameters, mathematical constants, and matrix definitions. All of the packages are well commented and the comments clearly indicate what is being done. Domain algorithms are noted in the source code of 11 packages. Table \ref{moduledata} suggests that the software packages are modularized to various degrees. When observing the source code files, it was found that 13 of the packages have a consistent style and order of function parameters.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{understandability_chart}
		\caption{AHP Understandability Score}
		\label{Fig_Understandability}
	\end{center}
\end{figure}

Figure \ref{Fig_Understandability} shows the surface understandability ranking of the software packages using AHP. Software packages with a high score have a consistent indentation and formatting style, and consistent, distinctive, and meaningful code identifiers. They also have symbolic constants, and explicitly identify mathematical and LBM algorithms. Their comments are clear and indicate what is being done in the source code. The source code is well modularized and structured. Furthermore, four of the top five ranked packages are noted as being alive.

\subsection{Visibility and Transparency}

Software package artifacts were reviewed for the identification of a specific development model, like a waterfall of agile development model, and the presence of documentation recording the development process and standard. They were also reviewed for the identification of the development environment, and the presence of release notes. The packages tended to not explicitly use well-known development models. This was also noted in the interviews with developers, as detailed below. The development teams of these packages are fairly small and easily organized without the need for such processes. Seven of the software packages did have some artifacts outlining the general development process, how to contribute, and the status of the package or its components. Eight of the packages have artifacts that note the development environment. While this information could help developers, and would improve transparency, the small close-knit nature of the development teams make explicitly publicly specifying this information practically unnecessary. Version release notes were found in nine of the software packages.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{visibilitytransparency_chart}
		\caption{AHP Visibility and Transparency Score}
		\label{Fig_VisibilityTransparency}
	\end{center}
\end{figure}

Figure \ref{Fig_VisibilityTransparency} shows the visibility and transparency ranking of the software packages using AHP. Software packages with a high score have an explicit development model and defined development process. They also 
had detailed and easy to access notes accompanying software releases. Furthermore, four of the top five ranked packages are noted as being alive. MP-LABS is noted as being dead.

\subsection{Overall Quality}

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{finalscore_chart}
		\caption{AHP Overall Score}
		\label{Fig_OverallScore}
	\end{center}
\end{figure}

Figure \ref{Fig_OverallScore} shows the overall ranking of the software packages using AHP. Software packages with an overall high score had ranked high in at least several of the individual qualities that were quantitatively measured. The overall ranking is found with an assumption of an equal weighting of all qualities. If the qualities were weighed differently, the overall software package ranking would change.

Looking at the top three ranked packages, Ludwig scored high in surface correctness and verifiability, surface robustness, surface usability, maintainability, and visibility and transparency. ESPResSo had achieved a relative high score in installability, surface correctness and verifiability, surface usability, maintainability, reusability, and visibility and transparency. Palabos scored high in installability, surface reliability, surface robustness, maintainability, and understandability. 

Section \ref{qualityrecommentations} further analyzes these findings and offers some software quality recommendations for future development of LBM software.

\newpage

\section{Qualitative Findings From Developer Interviews}\label{interviewresults}

This section presents the qualitative findings from data that was gathered during interviews with developers for qualities listed in Section \ref{softwarequalities}. The interview questions are found in Appendix \ref{interviewquestions}.

\subsection{Surface Correctness and Verifiability}

Interviews with developers confirmed that these software packages are developed by domain experts with backgrounds in physics, mathematics and mechanical engineering. It was noted in these interviews that some of the developers do not have formal software engineering education. Some of the development teams include computer scientists. Despite a lack of visible domain documentation and a resulting lower surface correctness and verifiability score, it is clear that some of the software packages were developed by teams with significant domain knowledge on account of the academic backgrounds of their developers. 

Interviews suggested a more frequent use of both unit testing and continuous integration in the development processes than what was observed from the initial survey. For example, OpenLB, pyLBM, and TCLB use such methods during development despite this not being explicitly clear from an analysis of the material available online. The correctness and verificablity of such packages is not measured well using surface analysis.

Several interviewed developers alluded to difficulty with testing the correctness of large numbers of features, and some even manually tested program output. The use of well defined unit testing tools could decrease the time spent testing some feature.

\subsection{Surface Usability}

Interviews with developers revealed several usability issues. Some users have misunderstood the boundaries of LBM and CFD, and have combined or applied methods that are not physically sound. Sometimes users have applied LBM to poorly defined or inappropriate fluid dynamics problems. For example, they may wish to model flow through or around a structure despite having limited information about the structure or its environment, and having little previous knowledge of CFD. The users do not realize the limitations of the methods, of the software, and do not understand the requirements to properly model a problem with the software. As the developer of TCLB noted, such software packages are not designed to be used ``out of the box'' in a plug and play fashion, and it could take months or more to set up the CFD problems correctly. Developers of some software packages, including ESPResSo, mitigated this by editing the source code to prevent users from ``combining methods that are not physically sound together'', and by updating the documentation to better inform users of LBM limitations, and of the requirements to properly model appropriate problems, including what algorithms and parameters to use. 

Some additional but infrequent software usability issues were commented on by the developers. Users have had trouble with installation and understanding how to maneuver the interfaces and how to set up or run models. These issues are addressed by various user support models, including frequently asked questions sections on the software package websites, user guides, and hardware and software requirements specifications.

One software package (ESPResSo) changed some of its scripting language to Python to make it more usable. The developer commented that this was ``the biggest step in terms of usability over the years'', further commenting that ``most people in the field know [Python]'' and that ``it's easy to learn''. 

\subsection{Maintainability}

Interviews with developers revealed that most projects do have a defined process for accepting contributions from team members. The packages rarely get contributions from outside developers, but the process would be similar as for the aforementioned group.

Contributions are made through GitHub, and are then reviewed and pulled by lead developers, often with consultation with a group of core developers depending on the organizational model. Continuous integration is part of the process for some packages. 

Some developers noted that their software package does not have well defined contributing guide in the repository, but it might be a good idea to add one in the near future. They would be happy to see contribution from outside of their organization, but currently this does not happen.

Furthermore, maintainability has been addressed by increasing source code modularity, reducing duplicate information, and improving abstraction by developing well defined interfaces. This was noted by the developers of ESPResSo and pyLBM. Several software packages have had sections of their code base redeveloped with languages that the developers felt are more understandable and readable, and that are better supported, such as Python. Data structures have also been redeveloped and storage has been improved. A developer of pyLBM mentioned that the geometries and models of their system had been ``decoupled'', using abstraction and modularization of the source code, to make it ``very easy to add [new] features''.

\subsection{Modifiability}

Software packages were not quantitatively measured for modifiability.
In this project we asked developers to comment on modifiability when we interviewed them. Specifically, we asked if ease of future changes to the system, modules, and code blocks was considered when designing the software. We also asked if any measures had been taken to ensure the ease of future changes. 
All of the developers that were interviewed noted that the ease of future changes was considered and that measures to ensure it had been taken, including
requiring the separation of software components in the source code architecture.

A high degree of code modularity and abstraction was noted by developers as a measure to ensure the ease of future changes. This can be ensured by separating components and hiding information behind well defined interfaces. The developer of ESPResSo also noted that some of the code base was transitioned from C to C++, which could ease modifiability of that software package. The developer of TCLB noted that their software package was designed to allow for the addition of some LBM features, but changes to major aspects of the system would be difficult. For example, ``implementing a new model will be an easy contribution'', but changes to the ``Cartesian mesh...will be a nightmare''. Furthermore, the package was designed with flexible data structures and storage in mind. 

Some software packages, like Palabos, provide validation benchmarks for their core fundamental algorithmic ingredients \citep{latt2021palabos}. The stated intent of these benchmarks is to showcase the validity and usefulness of the package to stimulate the development of third-user extensions. The Palabos package identifies as a development framework for modeling problems in various CFD areas. 

\subsection{Surface Understandability}

Software developers noted that they believe users have generally found their packages to be understandable. The interviewed developer of ESPResSo commented that some users have attempted to run physically incompatible LBM methods, and the solution was to edit the code to prevent such combinations, as well as to update the documentation to prevent misunderstanding the methods. Similarly, a developer of pyLBM noted that some users had issues setting up parameters for LBM schemes. The solution to this was to update the interface where these parameters are set, as well as to add functionality to test the stability of the parameters. A developer of OpenLB noted that some users lack the background knowledge to easily model fluid dynamics problems using their software. A frequently asked questions section was added to their package website to help users find answers to common questions. The package also has detailed documentation, including guides and usage requirements specification, to better help users understand the software.

\subsection{Traceability}

Software packages were not quantitatively measured for traceability.
In this project we asked developers to comment on traceability, specifically on their software package's documentation and how it fit into their development process. 

The interviewed developer of ESPResSo noted that all major additions to their package had accompanying changes to artifacts and documentation. They noted that considerable effort had been put into the documentation. They further commented that they want to lower the entry barrier for new developers, and because of that their package has a considerable amount of developer documentation. This documentation informs developers on how to get started, and orients them to the artifacts, source code, and system architecture, as well as how the software package build system works, and how the coupling between the simulation engine and the interface works. 

Developers noted the importance of documentation for both the users and developers of their software. New features are always added to the documentation. The developers use documentation to stay up to date on the status of the software package, and to help expand features, like computational models or algorithms. This is necessary so that the coding standard for these models is kept consistent with new developers.

The importance of documentation for both users and developers was stressed throughout the interviews. However, it was noted several times that a lack of time and funding has a negative affect on the documentation. Most of the developers are scientific researchers evaluated on the scientific papers that they produce. Writing and updating documentation is something that is done in their free time, if that time arises. Sometimes it is a last priority for the developers. Finding ways to hasten updating documentation would increase the frequency of such updates and benefit both users and developers. 

The developer of OpenLB noted the use of documentation generators like Doxygen. It would be advisable for more projects to use such automatic document generation tools, since some projects do not do this.  

\subsection{Visibility and Transparency}

Developers were asked to comment on the obstacles in their development process. The developer of ESPResSo noted that a lot of their source code had been written with a specific application in mind, and that there is too much coupling between components. Addressing this issue would help with code modifiability and reusability. Updating the development process would help resolve this issue and prevent such issues in the future. Improving the visibility of software changes and a peer review process would also help. Improving the software engineering education or experience of developers is also an idea that was brought up by several developers. Specifically, developers should always write code that is decoupled and modular, and should keep in mind the visibility of their contributions by better updating documentation and ensuring that their contributions are transparent to the rest of the developers. This would help catch issues in the contributions, and improve source code maintenance.   

According to the interviewed developer of ESPResSo, some obstacles to the development processes of their package had been overcome by the introduction of continuous integration practices, and a peer review process for contributions. These practices decrease development and maintenance times. The developer of TCLB mentioned that their package had two sets of code, for executing the models on the CPU and GPU, and that maintenance was decreased by introducing macros, a practice which became a common part of the development process. 

Developers were also asked how documentation fits into their development process. Several developers noted that developer documentation plays an important role in familiarizing potential contributors to the software system architecture. Without the guidance that the documentation provides it would be unlikely that contributions would pass the peer review process. 

None of the software packages whose developers were interviewed have a formal software development model. The packages all have fairly small development teams. These teams do accept outside contributors, but generally the teams are tight-knit, often working at the same institution, although one of the packages has an international team. The developer of ESPResSo noted that while no formal model is used, their development model is something similar to a combination of agile and waterfall development models. 

The developers noted similar project management processes. For teams of only a couple of developers, the addition of new features or major changes are discussed with the entire team. Projects with more than a couple of developers have lead developer roles. These lead developers review potential additions to the software. The software packages use GitHub for managing the project. Typically there are several development branches as well as the master branch.

\subsection{Reproducibility}

Software packages were not quantitatively measured for reproducibility.
In this project we asked developers to comment on reproducibility when we interviewed them.

Developers were asked if they have any concern that their computational results would not be reproducible in the future, and if they had taken any steps to ensure reproducibility.

The developer of ESPResSo noted a comparison of the results of their methods against manually calculated results. These comparison tests are automatically run for all source code changes. The tests are run when a pull request is opened on GitHub. Even once these tests are complete, a peer review process is done before changes are fully committed to the appropriate branch. The results for all of the LBM schemes on the software package development branch are also frequently compared for correctness, ensuring that the system output reflects the expected output.  

Several developers noted that they currently do not have a system in place to test for reproducibility, but it is of interest and could be implemented in the future. Generally, the mathematical foundations of the models are verified, but the output of the software package is not compared to other output. Depending on the package and how it outputs solutions, it may not be practical or feasible. A correct output may not be exactly reproducible, as it may be dependent on a probability distribution, so strictly comparing results may not be appropriate. 

The source code and artifacts of some software packages may be reproducible. There is considerable variance in the quality of software specifications and other developer documentation. Some packages are well detailed, and translating the specifications into source code will produce similar results across developers. Developers were not asked to comment on the reproducibility of their source code from their requirements specifications and design documentation. This question should be considered in the next iteration of state of the practice assessments. 

\subsection{Unambiguity}

Software packages were not quantitatively measured for unambiguity.
In this project we asked developers to comment on unambiguity when we interviewed them.

Developers were asked if they thought that the current documentation can clearly convey all necessary knowledge to the users, and if they had taken any steps to ensure clarity. 

The developer of ESPResSo noted that their documentation was meant for users that are already familiar with the underlying physics and CFD methods. These concepts are not explained in detail within the documentation. Users should acquire this knowledge from suitable external sources. The documentation focuses on how to technically use the software package, and includes a user guide and tutorial walk through of how to set up and run a simulation. With this in mind, the developer believes that their documentation is in reasonable shape for users with a minimum knowledge of the underlying physics. If new users have technical questions these can be addressed in further revisions of the documentation. New developers should find that the documentation is reasonably clear and useful. Information that is missing, like detailed explanations of dependencies, is referenced in the documents. 

The developer of pyLBM also noted that their documentation was in reasonable shape, but that they ``need more [user] feedback to improve [it]''. They also noted that they believe a lack of knowledge of the underlying physics and CFD concepts can be an issue for some users. This information can be referenced in the documentation, but it is not something that the documentation needs to detail. 

\newpage


\section{Answers To Research Questions}\label{answersquestions}

This section answers the research questions listed in Section \ref{purpose} using the quantitative data presented in Section \ref{AHPresults}, qualitative data presented in Section \ref{interviewresults}, additional data from software package repositories and artifacts, and domain expert feedback. Each subsection answers one of the research questions. Software package artifacts are examined in Section \ref{artifacts}, tools are listed in Section \ref{tools}, development principles, processes, and methodologies are discussed in Section \ref{prinprocmeth}, development pain points are explored in Section \ref{painpoints}, and our recommendations for improving software qualities are presented in Section \ref{qualityrecommentations}. A comparison between our designations of the software packages to community rankings is made in Section \ref{comparison}. Finally, threats to the validity of this assessment are noted in Section \ref{threats}.

\subsection{Artifacts Present}\label{artifacts}

This subsection answers the research question: What artifacts are present in current software packages?

The software packages were examined for the presence of artifacts, which were then categorized by frequency. We have grouped them into common, less common, and rare artifacts in Table \ref{artifactspresent}. Common artifacts were found in 16 to 23 ($>$70\%) of the software packages. Less common artifacts were found in 8 to 15 (35-70\%) of the software packages. Rare artifacts were found in 1 to 7 ($<$35\%) of the software packages. 

\begin{onehalfspace}
	\begin{center}
		\begin{tabular}{ p{8 cm} }
			\hline
			Common\\
			\hline
			Authors / Developers List\\
			Bug Tracker\\
			Dependency Notes\\
			Installation Guide / Instructions\\
			License\\
			List of Related Articles / Publications\\
			Makefile / Build File\\
			README File\\
			Requirements Specification / Theory Notes\\
			Tutorial\\
			\hline
			Less Common\\
			\hline
			Change Log / Release Notes\\
			Design Documentation\\
			Functional Specification / Notes\\
			Performance Information / Notes\\
			Test Plan / Report / Script / Data / Cases\\
			User Manual/Guide\\
			Version Control\\
			\hline
			Rare\\
			\hline
			API Documentation\\
			Developer / Contributor Manual / Guide\\
			FAQ / Forum\\
			Verification and Validation Plan / Notes\\
			Video Guide (including YouTube)\\
			\hline
		\end{tabular}
		\captionof{table}{Artifacts Present}\label{artifactspresent}
	\end{center}
\end{onehalfspace}

\subsubsection{Common Artifacts}
All of the top four AHP ranked packages, ESPREsSo, Ludwig, OpenLB, and Palabos, have each of the commonly found artifacts, except only three of them have a requirements specification or theory notes. Palabos is the only one of these four packages that does not have an artifact from this category.

The common artifacts contribute to the quality of the software in the following ways. A list of authors and developers helps potential users and contributors contact project members to answer questions affecting many software qualities. A bug tracker helps with organizing improvements to the software. Dependency notes help users install the software, as does an installation guide. Makefiles or other automated build files decrease human error in the installation process. Licenses promote usage of the software. Requirements specifications, linked theory notes, and related articles and publications help make the software more understandable, decrease ambiguity, and help with verifying its correctness. README files also help with understandability. The tutorials help users become familiar with using the software.


\subsubsection{Less Common Artifacts}\label{lesscommon}
The top four AHP ranked packages have most of the less common artifacts.
At the time of data collection, only one (Palabos) of the four packages did not have a user manual or guide, but there was a broken link on the package website indicating that such an artifact might exist. This broken link was later fixed, but this is not reflected in our data because it was not present at the time of data collection. Despite the broken link, Palabos does have a detailed and informative website. Another one (Ludwig) of the top four packages does not appear to have publicly visible design documentation. A third package (OpenLB) from the list does not appear to use a version control system. It is possible that such a system is used by the package since its website notes package version numbers, but the artifacts do not explicitly state the use of such a system.

The less common artifacts also contribute to software quality. A change log or release notes improve traceability of the software. Design documentation helps with maintaining, modifying, and reusing the software. Version control also helps to improve these qualities, as well as with traceability. Functional specifications and notes clarify the software, contributing to usability and understandability. A user manual also helps with those qualities and with installability, and correctness and verificability. Performance notes suggest that performance was considered when developing the software. Test plans, scripts, and cases, help verify correctness. 


\subsubsection{Rare Artifacts}
It is not common for the top four AHP ranked packages to have many of the rare artifacts. None of the top four packages have any explicit API documentation. Three of these packages (ESPREsSo, Ludwig, Palabos) have information on contributing to the project. Two of them (OpenLB, Palabos) have a FAQ section or forum. One (OpenLB) has verification and validation notes, and a video guide of the software. 

Although the artifacts were rarely found in our set of LBM software, they also contribute to software quality. API documentation helps with reusing the software. Developer and contributor manuals and guides help with maintainability, visibility and transparency. FAQs and forums improve usability of software. Verification and validation notes can be used to help check if a system meets specifications. Video guides can contribute to many software qualities, depending on the content of the video. 

\subsection{Tools Used}\label{tools}

This subsection answers the research question: What tools (development, dependencies, project management) are used by current software packages?

Software tools are used to support the development, verification, maintenance, and evolution of software, software processes, and artifacts \citep{ghezzi1991fundamentals}. Many tools are used by LBM software packages. The tools noted here are subdivided into development tools, dependencies, and project management tools.

\subsubsection{Development Tools}

Development tools support the development of end products, but do not become part of them, unlike dependencies that remain in the application once it is released \citep{ghezzi1991fundamentals}. The following type of development tools were explicitly noted in the artifacts or web-pages of the 23 LBM packages that were assessed. It is likely that other tools, such as debuggers, were used but are not specified in our sources.

	\begin{multicols}{2}	
		\begin{itemize}
			\begin{singlespace}
			\item Continuous Integration Tools
			\item Code Editors
			\item Development Environment
			\item Runtime Environments
			\item Compilers
			\item Unit Testing Tools
			\item Correctness Verification Tools
			\end{singlespace}
		\end{itemize}
	\end{multicols}

The above tools can verify the correctness of software during its development. Only two (ESPResSo, Ludwig) of the software packages that were assessed mentioned using continuous integration tools, like Travis CI. Code editors and compilers were explicitly noted to have been used by several packages, and were likely used by all of them. One of the packages (Ludwig) explicitly noted the use of proprietary unit testing code written in C. Likewise, the use of proprietary code for verifying the correctness of output was noted by one (pyLBM) of the developers. It is likely that similar tools were used when developing other software packages. 

\subsubsection{Dependencies}

The following types of dependencies were explicitly noted in the artifacts or web-pages of the 23 LBM packages that were assessed. It is possible that other types of dependencies are part of these software packages, but are not clearly specified in their artifacts or web sites and because of that they are not listed here.

	\begin{multicols}{2}	
		\begin{itemize}
			\begin{singlespace}
			\item Build Automation Tools
			\item Technical Libraries
			\item Domain Specific Libraries
		\end{singlespace}
		\end{itemize}
	\end{multicols}


Most of the software packages use some sort of build automation tools, most commonly Make. They also all use various technical and domain specific libraries. Technical libraries include visualization (e.g. Matplotlib, ParaView, Pygame, VTK), data analysis (e.g. Anaconda, Torch), and message passing libraries (e.g. MPICH, Open MPI, PyZMQ). Domain specific libraries are scientific computing libraries (e.g. SciPy). Libraries that are not explicitly stated in artifacts, or were not noted during our observations, may fall outside of these categories. 

\subsubsection{Project Management Tools}

Many of the software packages that were assessed were developed by teams of two or more people. Their work needed to be coordinated and managed. The following types of project management tools were explicitly noted in the artifacts, web-pages, or interviews with the developers of the 23 LBM packages that were assessed. As with development tools and dependencies, it is possible that other types of project management tools were used to coordinate and manage the projects but are not specified and because of that they are not listed here.

	\begin{multicols}{2}	
		\begin{itemize}
			\begin{singlespace}
			\item Collaboration Tools
			\item Email
			\item Change Tracking Tools
			\item Version Control Tools
			\item Document Generation Tools
		\end{singlespace}
		\end{itemize}
	\end{multicols}


Collaboration tools are noted as being used when developing the software projects. Most often email and video conferencing is used. Project management software was not explicitly mentioned, but it is possible that some of the projects use such software. Many of the projects are located on GitHub, and its developers use the platform to help manage their projects, especially bug related issues. Most of the projects appear to use change tracking and version control tools. They often use GitHub or GitLab for this. One package (SunlightLB) uses CVS. Document generation tools are mentioned in the artifacts of 12 of the projects. The tools Sphinx and Doxygen are explicitly used in this capacity. 

\subsection{Principles, Processes, and Methodologies}\label{prinprocmeth}

This subsection answers the research question: What principles, processes, and methodologies are used in the development of current software packages?

The points and conclusions in this subsection come from developer interviews and reviews of software package artifacts.

Most of the software packages do not explicitly state in their artifacts the motivations or design principles that were considered when developing the software. One package, Sailfish, indicates in its artifacts that shortening the development time was considered in early stages of design, with the developers opting for using Python and CUDA/OpenCL to achieve this without sacrificing any computational performance. The goals of that project are explicitly listed as performance, scalability, agility and extendability, maintenance, and ease of use. The project scored well in these categories during our assessment.

Processes, like methods, are ways of doing things, especially in an orderly way; while methodologies are defined as systems of methods \citep{ghezzi1991fundamentals}. It is not explicitly indicated in the artifacts of most of the packages that development involved following any specific model, like a waterfall or agile development model. One developer (ESPResSo) noted that while no formal model is used, their development model is something similar to a combination of agile and waterfall development models. The developer teams of the LBM packages are fairly small, so it is feasible for them to be organized without the need for such models. 

Seven of the software packages contain artifacts outlining the general development process, like basic instructions on how to contribute. Eleven of the packages explicitly convey that they would accept outside contributors, but generally the teams are centralized, often working at the same institution. 

The developers that were interviewed all noted similar project management processes. In teams of only a couple of developers, additions of new features or major changes are discussed with the entire team. Projects with more than a couple developers have lead developer roles. These lead developers review potential additions to the software. One of the developers (ESPResSo) that was interviewed noted that an ad hoc peer review process is used to assess major changes and additions.

Thirteen (57\%) of the 23 software packages use GitHub for managing the project, including nine (64\%) of the 14 alive packages, and four of the nine (44\%) dead packages. Two projects (Palabos, WaLBerla) use GitLab. This could be indicative of a transition to such software development and version control tools for SCS. Typically there are several simultaneous development branches in these projects.

Documentation was also noted as playing a significant role in the development process, specifically with on-boarding new developers. A goal of documentation is to lower the entry barrier for these new contributors. The documentation provides information on how to get started, orients the user to artifacts and the source code, and explains how the system works, including the so-called simulation engine and interface. The use of document generation tools is mentioned in the artifacts of 12 software packages, and was noted during interviews with developers. Sphinx and Doxygen are the tools that were mentioned. 

Two types of software changes were discussed during interviews with developers. One is feature additions, which arise from a scientific or functional need. These changes involve formal discussions within the development team, and lead developer participation is mandatory. The other change type is code refactoring, which only sometimes involves formal discussions with the development team. New developers were noted to play an increased role in these changes compared to the former changes. Software bugs are typically addressed in a similar fashion as code refactoring, and issue tracking is commonly used to manage these changes. 

Interviews with the developers of software packages also revealed a more frequent use of both unit testing and continuous integration in the development process than was found by only assessing the artifacts. The use of automatic installation processes is also common. Most often this involved a Make script.


\subsection{Pain Points}\label{painpoints}

This subsection answers the research question: What are the pain points for developers working on research software projects? What aspects of the existing processes, methodologies and tools do they consider as potentially needing improvement? How should processes, methodologies and tools be changed to improve software development and software quality?

Developers were asked to comment on obstacles in their development process, obstacles encountered by users, and potential future obstacles.

\subsubsection{Lack of Development Time}

A developer of pyLBM noted that their small development team has a lack of time to implement new features. Small development teams are common for LBM software packages. Team members are almost always part of the same institute or already know each other from other projects. External contributions are rare despite many of the projects accepting them. Aside from on-boarding new developers, time constraints could be mitigated by increasing developer efficiency, which could be addressed in several ways, including by improving the quality of documentation, or incorporating automatic code generation.

\subsubsection{Lack of Software Development Experience}

A lack of software development experience was noted by the developer of TCLB. Many of the team members on their project are domain experts and there can be a steep learning curve before these team members contribute good quality source code. It was further noted that this has been somewhat addressed, as the code has been re-written to best ensure ease of future contributions.

\subsubsection{Lack of Incentive and Funding}

The same developer also noted that there is a lack of incentive and funding in academia for developing widely used scientific software. The importance of funding for scientific software has been discussed in \cite{gewaltig2012quality}, which notes that ``software tools are developed and maintained only for as long as there is explicit or implicit funding''. The developer further commented that there are no journals that publish such scientific software source code. However, there are ways to get such source code cited. Work has been done to address this in \citep{smith2016software}, which presents a set of software citation principles and discusses ``how they could be used to implement software citation in the scholarly community'' \citep{katz2019software}. 

\subsubsection{Lack of External Support}

Another raised concern was that there are no organizations helping with the development of good quality software; but some do exist, including \href{https://bssw.io/}{Better Scientific Software (BSSw)}, \href{https://www.software.ac.uk/}{Software Sustainability Institute}, and \href{https://software-carpentry.org/}{Software Carpentry}. Some SCS developers may not be familiar with these organizations. 

Scientific software is often developed in-house by the very researchers that temporarily use it in their own research. Empirical studies of such ``professional end-user development'' of SCS is noted in \citep{segal2007end}. This kind of software has a defined user and purpose, and often does not meet the standards that would be required by external users. It has been categorized as a ``private tool'' by \citep{gewaltig2012quality}, which notes that despite often being made freely available, ``it is not always clear that it is sufficiently mature in terms of domain coverage, validity, documentation or usability, to be useful to other researchers''. It is less common for such ad hoc scientific software to become ``user-ready software'', which ``is not only research-ready, but should have most of the attributes commonly expected of commercial software products including broadness of scope, robustness, demonstrable correctness and adequate documentation'' \citep{gewaltig2012quality}. As software becomes user-ready, it can become commercialized and closed-source. 

\subsubsection{Parallelization and Continuous Integration}

Setting up parallelization was also noted as a technical pain point by one of the developers, and the introduction of continuous integration by another. Software development knowledge, and automatic code generation, could mitigate such pain points. As already noted, many of the developers are domain experts and not professional software developers. The developer of TCLB noted that eliminating equivalent statements using macros had helped improve the quality of their source code, specifically helping with reusing code to run on both the CPU and GPU. 

\subsubsection{Ensuring Correctness}

Difficulties with ensuring correctness were also noted by several developers. They indicated that tests are run on all new source code additions, testing both individual modules and the system to verify correctness. These tests compare the package output to known correct output using test cases. The developer of TCLB commented that the amount of testing data that is needed for some cases is a problem as free testing services do not offer capabilities to store and process such large amounts of data, and in-house testing solutions needed to be created to address this limitation. The solution for this has been to limit the size of the testing problems, and to run tests in small batches with few iterations.

\subsubsection{Usability}

A few obstacles related to users were found. Several developers noted that users sometimes try to use incorrect LBM method combinations to solve their problems. Furthermore, some users think that the packages will work out of the box to solve their cases, while the packages require both a good understanding of CFD and an understanding of the requirements for formulating problems in the individual packages, which can be a significant endeavor. These software packages are not like commercial software packages. They are generally set up to solve specific research problems, and are often primarily used by their developers. While they are modifiable to solve similar problems, these modifications are not trivial. Better documentation, with attention on traceability, and automatic code generation are suggested when designing software for change, and would help with these modifications. So far this problem of on-boarding new users has been addressed by updating the documentation to better inform users of the underlying LBM theory and package requirements. Similar issues with LBM parameters were noted by another developer. Updating the user interface to better explain theoretical principles, as well as test user input for compatibility, was the implemented solution. As noted above, sometimes frequently asked questions on the underlying theory and on how to use the software are answered in the documentation.

The interviewed developer of ESPResSo commented that parts of their package's source code had been refactored to Python to help address usability issues. Python was perceived as a much more usable language, and it would be easy for future users and developers to learn and understand the source code. 

\subsubsection{Technical Debt}

A few potential future obstacles were noted. The developer of ESPResSo noted that their source code had been written with a specific application in mind and that due to this there was too much coupling between components of the source code. This results in technical debt, having an impact on future modifiability and reusability when trying to extend the software, and the code would need to be refactored.

As noted above, difficulties with ensuring future correctness could also arise. As new methods and functionality is added into the software, new test cases and test data will need to be developed.

\subsubsection{Quality of Documentation}

Interviewees commented that documentation is important and that its quality could be improved. As already noted, there is often no time or funding for maintaining quality documentation for software that is rarely used outside of the development team. Furthermore, the documentation generally only provides a shallow overview of the underlying CFD theory. Users would be well advised to already be familiar with these topics, or they should spend significant time referencing theory resources. The documentation instead generally focuses on explaining how to use the software. It is of course not feasible for package documentation to address the underlying physics topics in detail, so it is advised that the package documentation links to resources that better explain the underlying theory. Sometimes, frequently asked questions about the underlying theory are answered in the documentation. OpenLB has an artifact for such questions.

The developer comments emphasized an importance on source code, while documentation seems to be of secondary importance. It must be stressed that improving documentation could benefit development and help eliminate some of the developer concerns that were raised. The use of automatic document generation tools that capture scientific and computing knowledge, and transform it into software artifacts, is advised. Drasil is an automatic document generation tool that is further discussed in Section \ref{highlightedrecommendations}.

\subsection{Quality Recommendations}\label{qualityrecommentations}

This subsection answers the research question: For research software developers, what specific actions are taken to address software qualities?

The following points regarding software quality should be considered when developing LBM software packages. These points are based on developer interviews, SCS literature, what was found to have worked for packages that were designated as high quality in this assessment.

\subsubsection{Installability}

\begin{itemize}
	\item Include OS compatibility, including specific OS versions. This was done by several top ranked packages, including ESPResSo and LUMA.
	\item Provide complete installation instructions. This was done by all of the top five ranked packages.
	\item Installation instructions should be written as if the user does not have any dependencies installed and is installing on a clean OS. This was done by several top ranked packages, including ESPResSo, OpenLB, and Palabos. This can be tested on a clean environment using a virtual machine.
	\item List all dependencies in the installation instructions, dependency versions, and how to install them. This was done by several top ranked packages, including ESPResSo, LUMA, OpenLB, and Palabos.
	\item If possible, automate the installation of dependencies. Use tools such as Make. This was done by all of the top five ranked packages.
	\item Automate the installation process as much as possible. Use tools such as Make. This was done by all of the top five ranked packages.
	\item The installation instruction should only be in one location. This was done by several top ranked packages, including ESPResSo, OpenLB, and Palabos.
	\item Include descriptive error messages for errors encountered during installation. This was done by several top ranked packages, including Ludwig and LUMA
	\item Provide a way to validate the installation. This can be done using a custom script, or a test case that specifies expected output. This was done by several top ranked packages, including OpenLB, Ludwig, LUMA, and Palabos.
	\item Provide instructions for uninstalling the software. These were included with pyLBM.
\end{itemize}

\subsubsection{Surface Correctness and Verifiability}

\begin{itemize}
	\item Use a requirements specification document. This is suggested in SCS literature, and several of the top ranked packages had such a document or reference to theory manuals. A potential template is presented in \citep{smith2005new}. 
	\item Make public (on package website or GitHub) the requirements specification document, or explicitly reference the domain theory that the software is designed from.  This was done by several top ranked packages, including ESPResSo, Ludwig, LUMA, and OpenLB.
	\item Ensure the above information is easy to find. Consider adding it to the user manual. The information is easy to find in most top ranked packages.
	\item Development teams should include both domain experts and experienced software developers. This suggestion is based on developer comments.
	\item Use and make public (on package website or GitHub) detailed documentation. Consider using automatic document generation tools like Doxygen, Drasil, or Sphinx. This was done by all of the top five ranked packages.
	\item Provide detailed tutorials that include expected output, like the \href{https://www.walberla.net/doxygen/index.html}{waLBerla tutorials}.
	\item Use unit tests during development and make them public (on package website or GitHub). This was available for Ludwig.
	\item Modularize the source code, separate components, hide information  behind well defined interfaces. This is suggested in SCS literature, and in developer comments.
	\item Use continuous integration tools (Bamboo, Jenkins, and Travis CI) and processes during development. This was done by top ranked packages ESPResSo and Ludwig.
\end{itemize}

\subsubsection{Surface Reliability}

\begin{itemize}
	\item Include descriptive error messages where appropriate. This was done by most packages that encountered a fault.
	\item In case automatic installation of dependencies fails, the system should indicate to the user what dependencies need to be installed manually. This was done by many packages, including ESPResSo++, Ludwig, LUMA, pyLBM, TCLB.
	\item The packages should include detailed tutorials, including dependencies, expected output, and any additional supplementary documentation that may be required. This was done by many packages, including waLBerla, Palabos, MechSys, LUMA, and pyLBM.
\end{itemize}

\subsubsection{Surface Robustness}

\begin{itemize}
	\item The system must provide descriptive error messages when it encounters unexpected input, including incorrect data types, empty input, and missing files or links. Eighteen of the software packages behaved reasonably when tested with unexpected input.
\end{itemize}

\subsubsection{Surface Performance}

\begin{itemize}
	\item Integrate parallelization tools and techniques to reduce processing time. Consider GPU processing, CUDA, and MPI. Seventeen software packages mentioned parallelization in their artifacts.
	\item The user should be able to choose to process their model on either the CPU or GPU. ESPResSo, lbmpy, lettuce, pyLBM, and TCLB allow for either CPU or GPU processing.
\end{itemize}

\subsubsection{Surface Usability}

\begin{itemize}
	\item Include user hardware and software requirements documentation. Hardware requirements are rarely listed in the software packages that were assessed. Only those offering GPU processing (ESPResSo, lbmpy, lettuce, pyLBM, Sailfish, TCLB) mentioned any hardware requirements. On the other hand, some sort of software requirements were available for all packages, even if only some dependencies or a compatible operating system were mentioned.
	\item Provide a user tutorial that indicates the expected output. This was done by many packages, including waLBerla, Palabos, MechSys, LUMA, pyLBM.
	\item Provide a detailed user manual. It should identify elements of user interfaces, and identify all requirements to model a system. Thirteen packages have a user manual. ESPResSo has a well detailed manual.
	\item State appropriate fluid dynamics problems that the software is designed to model in the documentation, and explicitly state the limits of the software. This is done in the ESPResSo user guide.
	\item Provide documentation that details the background theory information, or provide a reference to such information. This was done by several top ranked packages, including ESPResSo, Ludwig, LUMA, and OpenLB.
	\item Identify expected user characteristics. LIMBES, Ludwig, laboetie, and Palabos did this. The importance of specifying user characteristics is discussed in \citep{smith2007requirements}.
	\item Keep all documentation in one location. This was done by top ranked packages.
	\item Do not duplicate artifact information. This is suggested as good software development practice. Duplicate information is difficult to maintain. 
	\item Maintain a user support model (Git, email, forum, FAQ). This was done by all of the top five ranked packages.
	\item If possible, consider using popular user-friendly software languages like Python. Especially consider this for parts of the source code that is likely to be modified or reviewed by users. ESPResSo and Sailfish use Python to shorten development time and improve usability.
\end{itemize}

\subsubsection{Maintainability}

\begin{itemize}
	\item Keep artifacts updated. This is suggested as good software maintenance practice.
	\item Ideally include most of the common and less common artifacts listed in Section \ref{artifacts}.
	\item Include version numbers and release notes for all major source code and artifact releases. The top five ranked software packages include release notes.
	\item Have a defined process for accepting contributions, and make public documentation for making contributions to the project. The top five ranked software packages include information on how to contribute. 
	\item Use an issue tracker (Git, email, SourceForge, other) to manage bugs and changes. Issues should be regularly reviewed and closed. All but three (laboetie, lettuce, Sailfish) of the software packages that use Git have most of their issues closed.
	\item Use a version control system (GitHib, CVS). Four (ESPREsSo, Ludwig, LUMA, Palabos) of the top five ranked packages use a version control system.
	\item Source code needs to be well commented. Typically, more than 10 percent of LBM package source code is comments, as presented in Table \ref{gitrepodata}. Four of the top five overall ranked packages (Ludwig, ESPResSo, Palabos, OpenLB) have about 20 percent of their source code as comments.
	\item Modularize the source code, separate components, hide information behind well defined interfaces. This is suggested in SCS literature, and in developer comments.
	\item Eliminate code duplication. This is suggested as good software development practice. Duplicate code is difficult to maintain.
	\item If possible, consider using popular user-friendly software languages like Python. Especially consider this for parts of the source code that is likely to be modified or reviewed by users. ESPResSo and Sailfish use Python to address several software qualities, including maintainability.
	\item Consider the recommended points for addressing traceability.
\end{itemize}

\subsubsection{Modifiability}

\begin{itemize}
	\item Modularize the source code, separate components, hide information behind well defined interfaces. This is suggested in SCS literature, and in developer comments.
	\item If possible, consider using popular user-friendly software languages like Python. Especially consider this for parts of the source code that is likely to be modified or reviewed by users. ESPResSo and Sailfish use Python to address several software qualities, including modifiability.
	\item Consider future source code modifiability as early as the design stage of development. This is suggested as good software development practice.
	\item Consider flexibility of data structures and data storage in the design stage. The package pyLBM redeveloped data structures to ease future changes.
\end{itemize}

\subsubsection{Reusability}

\begin{itemize}
	\item Modularize the source code, separate components, hide information behind well defined interfaces. This is suggested in SCS literature, and in developer comments.
	\item Document module interfaces in design and developer documentation. This is suggested as good software development practice.
	\item Provide API documentation, if applicable. Only one (ESPResSo) of the top five ranked packages provided API documentation.
\end{itemize}

\subsubsection{Surface Understandability}

\begin{itemize}
	\item Adopt a coding standard and document it in artifacts, including some examples. A coding standard needs to be part of continuous integration. Only
	LUMA and HemeLB explicitly identify coding standards. 
	\item Use consistent, distinctive, and meaningful code identifiers. Nineteen of the 23 software packages use such code identifiers.
	\item Use symbolic constants. Twelve of the 23 software packages use such constants.
	\item Identify algorithms that are used in source code comments. Document them in the artifacts. Cite relevant external sources if needed. Domain algorithms are noted in the source code of 11 packages.
	\item Add meaningful comments. Indicate what is being done in each section of source code. All of the top ranked packages have meaningful comments in their source code.
	\item Modularize the source code, separate components, hide information behind well defined interfaces. This is suggested in SCS literature, and in developer comments.
	\item Provide a user manual that identifies all technical and problem modeling requirements. Thirteen software packages have a user manual.
	\item State appropriate fluid dynamics problems that the software is designed to model in the documentation, and explicitly state the limits of the software. This is done in the ESPResSo user guide. 
	\item Consider adding a FAQ section to the documentation. This helped resolve some usability issues for OpenLB.
\end{itemize}

\subsubsection{Traceability}

\begin{itemize}
	\item Update all relevant documentation when a change to the software is made. This is suggested as good software development practice.
	\item Use automatic document generation tools (Doxygen, Drasil, Sphinx) to limit the time spent on updating documentation. This was done by all of the top five ranked packages.
	\item Provide a developer's guide to help orient developers to the artifacts, source code, and system architecture so that they can better document changes. Top ranked package OpenLB has a developer guide. 
\end{itemize}

\subsubsection{Visibility and Transparency}

\begin{itemize}
	\item Summarize the development process that is used. Provide information on how new users can contribute. Identify the development model by name (waterfall, agile, etc.), if appropriate.  Seven of the software packages have some artifacts outlining the general development process. Eleven packages have information on how to contribute. 
	\item Identify the development environment. Eight of the 23 software packages identify the development environment.
	\item Update all relevant documentation when a change to the software is made. This is suggested as good software development practice.
	\item Include notes with all releases. Nine of the packages include release notes.
	\item Communicate all changes within the development team. This suggestion is based on developer comments.
	\item Use continuous integration processes and tools (Bamboo, Jenkins, and Travis CI). This was done by top ranked packages ESPResSo and Ludwig.
	\item Consider peer review processes to assess contributions and ensure the tracking of information. High ranked package ESPResSo uses a peer review process for contributions. 
	\item Use project management tools, including change and version control tools (GitHub, GitLab, CVS), collaboration tools (GitHub, GitLab), and document generation tools (Doxygen, Drasil, Sphinx). This was done by all of the top five ranked packages. 
\end{itemize}

\subsubsection{Reproducibility}

\begin{itemize}
	\item Test output against automatically calculated or known correct results. High ranked package ESPResSo does this.
	\item Automate the testing of output. Run tests after all code changes. High ranked package ESPResSo does this.
	\item Consider peer review processes and task based inspection to assess contributions. High ranked package ESPResSo does this.
	\item Update all relevant design documentation when a change to the software is made. This is suggested as good software development practice.
\end{itemize}

\subsubsection{Unambiguity}

\begin{itemize}
	\item Documentation must state all technical requirements and software dependencies. A missing dependency was a frequent cause of fault conditions during this assessment.
	\item Provide a detailed user manual. Thirteen packages have a user manual. ESPResSo has a well detailed manual.
	\item Provide a table of symbols in the developer's guide that maps to names used in the source code. This is suggested as good software development practice.
	\item State appropriate fluid dynamics problems that the software is designed to model in the documentation, and explicitly state the limits of the software. This is done in the ESPResSo user guide.
	\item The documentation should either explain the underlying CFD theories or provide a reference to appropriate resources. This was done by several top ranked packages, including ESPResSo, Ludwig, LUMA, and OpenLB.
	\item Consider asking users for feedback on the documentation. The developer of pyLBM noted that such feedback would be appreciated.
\end{itemize}


\subsection{Designation Comparison}\label{comparison}

This subsection answers the research question: How does software designated as high quality by this methodology compare with top rated software by the community?

This comparison helps evaluate the methodology that was used, and assess the validity of the findings. Threats to the validity of this exercise are noted in Section \ref{threats}. The software package designations of this report are compared to package repository ranking metrics in Section \ref{repmetrics}, and to software recommended by a domain expert in Section \ref{domainexpertrecommend}.

\subsubsection{Repository Ranking Metrics}\label{repmetrics}

Table \ref{repometrics} presents our LBM software package rankings along with the repository ranking metrics of each software package. 

Eight packages do not use GitHub or GitLab, so they do not have a measure of repository stars. Looking at the repository stars of the other 15 packages, we can observe a slight pattern where packages that have been highly ranked by our assessment do have more stars than lower ranked packages. The second ranked package (ESPResSo) has the second most number of stars. The ninth ranked package (Sailfish) has the most number of stars. Packages designated as lower quality often do not use GitHub or GitLab, or have few stars, except for HemeLB which has 22 stars. Our assessment of this package might not be accurate. The number of stars does not necessarily represent the perceptions of the community, but for lack of an alternative measure we will use it this way. 

The repository watches column contains even less data to compare since two of the packages (Palabos, waLBerla) use GitLab, which does not include this metric. The pattern that can be observed with this metric is very similar to that of the stars metric. The ninth ranked package (Sailfish) has the most number of watches. The second ranked package (ESPResSo) has the second most number of watches.

Besides missing data, another threat to the validity of this comparison is the varying ages of the repositories. Older packages have been able to accumulate stars and watches for longer than newer packages. The true quality of new packages may not be reflected in their stars and watches. \\

\begin{center}
	\begin{onehalfspacing}
		\begin{tabular}{ p{3.5cm}p{2.5cm}p{2cm}p{2cm}p{2cm} }
			\hline
			Name & Our Ranking & Repository Stars & Repository Star Rank & Repository Watches \\
			\hline
			Ludwig&1& 19 &8& 6\\
			ESPResSo &2& 115 &2& 23\\
			Palabos &3& 14 &10& GitLab\\
			OpenLB &4& No Git &No Git & No Git\\
			LUMA&5& 26 &6& 10\\
			pyLBM &6& 57&4& 8\\
			DL\_MESO (LBE) & 7 & No Git &No Git & No Git \\
			waLBerla & 8 & 17 &9& GitLab\\
			Sailfish &9& 154 &1& 39\\
			laboetie &10& 4 &13& 5\\
			TCLB &11& 61 &3& 16\\
			MechSys &12& No Git &No Git & No Git\\
			lettuce &13& 8 &11& 2\\
			ESPResSo++ &14& 31 &5& 13\\
			MP-LABS &15& 7 &12& 2\\			
			SunlightLB & 16& No Git &No Git & No Git\\
			LB3D &17& No Git &No Git & No Git\\			
			LIMBES &18& No Git &No Git & No Git\\
			LB2D-Prime &19& No Git &No Git & No Git\\		
			HemeLB &20& 22 &7& 15\\
			lbmpy&21&  4 &13& 2  \\	
			LB3D-Prime &22& No Git &No Git & No Git\\	
			LatBo.jl &23& 4 &13& 5\\			
			\hline
		\end{tabular}
		\captionof{table}{Repository Ranking Metrics}\label{repometrics}
	\end{onehalfspacing}
\end{center}

\subsubsection{Domain Expert Recommended Software}\label{domainexpertrecommend}

Table \ref{rankingcomparison} compares the top 10 ranked LBM software packages with a LBM package ranking made by a domain expert. Five (Palabos, OpenLB, LUMA, waLBerla, Sailfish) of the top 10 ranked packages in this assessment are also found in the domain expert's list. Interestingly, these five packages are listed in the same order on both lists. Looking at the remaining packages, the top two ranked packages in this state of the practice assessment (Ludwig, ESPResSo) are not on the domain expert's list. Perhaps the intended applications of these packages, complex fluids for Ludwig and soft matter research for ESPResSo, do not align with the research interests of the domain expert, and this is why they did not make it onto their list. Moving down the list, pyLBM and DL\_MESO(LBE) did not make the domain expert's top 10 list but were mentioned by the domain expert. The 10th ranked package, laboetie, was not mentioned by the domain expert. Looking at the remainder of the domain expert's list, ASL is a general purpose tool for solving partial differential equations and may have fallen out of scope of the authoritative lists that were used to identify the initial LBM software list. The domain expert's fifth ranked package (ch4-project) was on the initial software list of this assessment, but was removed due to a lack of documentation. Open FSI is a new project that uses Palabos. It was made public the year before data was collected for this state of the practice exercise. It may not have been on authoritative lists due to its young age. Similarly, LIFE was also made public recently. Finally, the domain expert's 10th ranked package (HemeLB) was ranked 20th in this state of the practice assessment. 

\begin{center}
	\begin{onehalfspacing}
		\begin{tabular}{ p{2cm}p{4.5cm}p{3.5cm}}
			\hline
			Rank & This Assessment & Domain Expert\\
			\hline
			1&Ludwig (N/A) &Palabos\\
			2&ESPResSo (N/A) &OpenLB\\
			3&Palabos (1)&LUMA\\
			4&OpenLB (2)&ASL\\
			5&LUMA (3)&ch4-project\\
			6&pyLBM (17)&Open FSI\\
			7&DL\_MESO (LBE) (12)& WaLBerla\\
			8&waLBerla (7)& LIFE\\
			9&Sailfish (9)&Sailfish\\
			10&laboetie (N/A)&HemeLB\\		
			\hline
		\end{tabular}
		\captionof{table}{Ranking Comparison}\label{rankingcomparison}
	\end{onehalfspacing}
\end{center}

\subsection{Threats To Validity}\label{threats}

This section examines potential threats to the validity of this state of the practice assessment. These can be categorized into methodology and data collection issues. 

The goal of this assessment isn't to rank the software, but to use the ranking exercise as a means to understand the state of the practice of LBM software development.

The measures listed in our measurement template may not be broad enough to accurately capture some qualities. For example, there are only two measures of surface robustness. The measurement of robustness could be expanded, as it currently only measures unexpected input. Other faults could be introduced, but could require a large investment of time to develop, and might not be a fair measure for all packages. Similarly, reusability is assessed along the number of code files and LOC per file. While this measure is indicative of modularity, it is possible that some packages have many files, with few LOC, but the files do not contain source code that is easily reusable. The files may be poorly formatted, or the source code may be vague and have ambiguous identifiers. Furthermore, the measurement of understandability relies on 10 random source code files. It is possible that the 10 files that were chosen to represent a software package may not be a good representation of the understandability of that package.

Regarding data collection, a risk to the validity of this assessment is missing or incorrect data. Some software package data may not have been measured due to technology issues like broken links. This issue arose with the measurement of Palabos, which had a broken link to its user manual, as noted in Section \ref{lesscommon}. 

Some pertinent data may not have been specified in public artifacts, or may be obscure within an artifact or web-page. The use of unit testing and continuous integration was mentioned in the artifacts of only two (ESPResSo, Ludwig) packages. However, interviews suggested a more frequent use of both unit testing and continuous integration in the development processes than what was observed from the initial survey of the artifacts. For example, OpenLB, pyLBM, and TCLB use such methods during development despite this not being explicitly clear from an analysis of the material available online. 

Furthermore, design documentation was measured to be a ``less common'' artifact in this assessment, but it is probable that such documentation is part of all LBM packages. After all, developing SCS is not a trivial endeavor. It is likely that many packages have such documentation but did not make it public, and due to this the measured data is not a true reflection of software package quality.

\newpage
\section{Conclusion}\label{conclusion}

We analyzed the state of the practice of software development in the Lattice Boltzmann Methods software domain by quantitatively and qualitatively measuring, and comparing, 23 software packages along quality attributes. The software qualities that were assessed in this report are listed in Section \ref{softwarequalities}. A methodology for assessing the state of the practice of software development in SCS domains was presented. Domain packages were assessed to answer the software development related research questions listed in Section \ref{purpose} to understand how software quality is impacted by software development choices, including principles, processes, and tools. Software developers were interviewed to identify development pain points, and to identify how software quality is ensured. Quantitative data was used to rank the software packages using the AHP. The ranking designations were compared with rankings from the software development community, and we found that many of our top 10 ranked packages are ranked highly by a domain expert. Recommendations for improving software along quality metrics were made, and highlights are presented in Section \ref{highlightedrecommendations} of this conclusion. The findings of this report can be used to guide future development of SCS, specifically along quality attributes, and to reduce software quality failures.

We understand that software packages vary in goals, developers, funding, and other aspects. Our goal was to highlight software quality successes through an evaluation of the entire domain software family. Furthermore, threats to the validity of the findings are highlighted in Section \ref{threats}. 

Recommendations for future state of the practice assessments are made in Section \ref{futuresop}.

\subsection{Highlighted Recommendations}\label{highlightedrecommendations}

The following recommendations improve software quality along multiple attributes, and provide the greatest return on investment:

\begin{itemize}
	\item Provide a detailed user manual. It should identify elements of user interfaces, and identify all requirements to model a system. Thirteen packages have a user manual. ESPResSo has a well detailed manual.
	\item State appropriate fluid dynamics problems that the software is designed to model in the documentation, and explicitly state the limits of the software. This is done in the ESPResSo user guide.
	\item Include detailed tutorials, including dependencies, expected
	output, and any additional supplementary documentation that may be required. This was done by many packages, including waLBerla, Palabos, MechSys, LUMA, and pyLBM.
	\item Keep all documentation in one location. This was done by top ranked packages.
	\item If possible, consider using popular user-friendly software languages like Python. Especially consider this for parts of the source code that is likely to be modified or reviewed by users. ESPResSo and Sailfish use Python to address several software qualities, including maintainability, modifiability and usability.
	\item Modularize the source code, separate components, hide information behind well defined interfaces. This is suggested in SCS literature, and in developer comments.
	\item Include descriptive error messages where appropriate. This was done by most packages that encountered a fault.
	\item Summarize the development process that is used. Provide information on how new users can contribute. Seven of the software packages have some artifacts outlining the general development process. Eleven packages have information on how to contribute.
	\item Consider peer review processes and task based inspection to assess contributions. High ranked package ESPResSo does this.
	\item Use continuous integration tools (Bamboo, Jenkins, and Travis CI) and processes during development. This was done by top ranked packages ESPResSo and Ludwig.
	\item Use project management tools, including change and version control tools (GitHub, GitLab, CVS), collaboration tools (GitHub, GitLab), and document generation tools (Doxygen, Drasil, Sphinx). This was done by all of the top five ranked packages.
\end{itemize}

Ensuring software quality can take significant time. An inability to develop high quality documentation due to time constraints could be mitigated in the future by the use of automatic document generation tools like Drasil. The Drasil Framework consists of a collection of Domain Specific Languages (DSL) for capturing scientific documents, structures, and computing knowledge, and then transforming this knowledge into relevant software artifacts without having to manually duplicate knowledge into multiple artifacts \citep{zhao2018}.

\subsection{Future State Of The Practice Assessments}\label{futuresop}

This section notes recommendations for future state of the practice assessments.  

As mentioned in Section \ref{threats}, the measures listed in our measurement template may not be broad enough to accurately capture some qualities. Adding or extending measures is worth considering. For example, usability experiments and performance benchmarks could be incorporated into the assessment.

Section \ref{interviewresults} noted that developers were not asked to comment on the reproducibility of their source code from their requirements specifications and design documentation. Adding this question to the interview guide should be considered in the next iteration of state of the practice assessments. Furthermore, as mentioned in Section \ref{threats}, it was found that some pertinent information was not specified in public artifacts. The use of unit testing and continuous integration by several packages (OpenLB, pyLBM, TCLB) was only discovered during interviews with developers. Adding further questions to the interview guide regarding the measures that are on the measurement template could reduce instances of incorrect data being collected. This additional interview data could be analyzed and incorporated into the AHP ranking, ensuring quality designations more accurate represent the true quality of the software packages.

\newpage
\begin{appendices}

\begin{singlespace}	
	
\section{Measurement Template}\label{measurementtemplate2}
The table below lists the set of measures that are used to assess each software product. The first set identifies summary information, followed by 9 sets for software qualities and 3 sets for raw metrics. Each measure is followed by the type for a valid result. A superscript ∗ indicate that a response of this type needs to be accompanied by explanatory text.\\ 
\newpage
\captionof{table}{Measurement Template}\label{measurementtemplate}
\def\arraystretch{1.22}
\begin{tabular}{p{14cm}}
	\hline
	\textbf{Summary Information}\\
	\hline
	Software name? (string)\\
	URL? (URL)\\
	Affiliation (institution(s)) (string or {N/A})\\
	Software purpose (string)\\
	Number of developers (all developers that have contributed at least one commit to the project) (use repo commit logs) (number)\\
	How is the project funded? (unfunded, unclear, funded*) where * requires a string to say the source of funding\\
	Initial release date? (date)\\
	Last commit date? (date)\\
	Status? (alive is defined as presence of commits in the last 18 months) ({alive, dead, unclear})\\
	License? ({GNU GPL, BSD, MIT, terms of use, trial, none, unclear, other*}) * given via a string \\
	Platforms? (set of {Windows, Linux, OS X, Android, other*}) * given via string\\
	Software Category? The concept category includes software that does not have an officially released version. Public software has a released version in the public domain. Private software has a released version available to authorized users only. ({concept, public, private})\\
	Development model? ({open source, freeware, commercial, unclear})\\
	Publications about the software? Refers to publications that have used or mentioned the software. (number or {unknown})\\
	Source code URL? ({set of url, n/a, unclear})\\
	Programming language(s)? (set of {FORTRAN, Matlab, C, C++, Java, R, Ruby, Python, Cython, BASIC, Pascal, IDL, unclear, other*}) * given via string \\
	Is there evidence that performance was considered? Performance refers to either speed, storage, or throughput. ({yes∗, no})\\
	Additional comments? (can cover any metrics you feel are missing, or any other thoughts you have) \\
	\hline
\end{tabular}

\def\arraystretch{1.5}
\begin{tabular}{p{14cm}}
	\hline		
	\textbf{Installability  (Measured via installation on a virtual machine.) }\\
	\hline
	Are there installation instructions? ({yes, no})\\
	Are the installation instructions in one place? Place referring to a single document or web-page. ({yes, no, n/a})\\
	Are the installation instructions linear? Linear meaning progressing  in a single series of steps. ({yes, no, n/a})\\
	Are the instructions written as if the person doing the installation has none of the dependent packages installed? ({yes, no, unclear})\\
	Are compatible operating system versions listed? ({yes, no})\\
	Is there something in place to automate the installation (makefile, script, installer, etc)? ({yes*, no})\\
	If the software installation broke, was a descriptive error message displayed? ({yes, no, n/a})\\
	Is there a specified way to validate the installation? ({yes*, no})\\
	How many steps were involved in the installation? (Includes manual steps like unzipping files) Specify OS. (number, OS)\\
	What OS was used for the installation? ({Windows, Linux, OS X, Android, other* }) *given via string\\
	How many extra software packages need to be installed before or during installation? (number)\\
	Are required package versions listed? ({yes, no, n/a})\\
	Are there instructions for the installation of required packages / dependencies? ({yes, no, n/a})\\
	Run uninstall, if available. Were any obvious problems caused? ({yes∗ , no, unavail})\\
	Overall impression? ({1 .. 10})\\
	Additional comments? (can cover any metrics you feel are missing, or any other thoughts you have)\\
	\hline
\end{tabular}

\def\arraystretch{1.33}
\begin{tabular}{p{14cm}}
	\hline		
	\textbf{Correctness and Verifiability}\\
	\hline
	Any reference to the requirements specifications of the program or theory manuals? ({yes∗ , no, unclear})\\
	What tools or techniques are used to build confidence of correctness? ({literate programming, automated testing, symbolic execution, model checking, assertions used in the code, Sphinx, Doxygen, Javadoc, confluence, unclear, other*}) * given via string\\
	If there is a getting started tutorial? ({yes, no})\\
	Are the tutorial instructions linear? ({yes, no, n/a})\\
	Does the getting started tutorial provide an expected output? ({yes, no*, n/a})\\
	Does your tutorial output match the expected output? ({yes, no, n/a})\\
	Are unit tests available?  ({yes, no, unclear})\\
	Is there evidence of continuous integration? (for example mentioned in documentation, Jenkins, Travis CI, Bamboo, other) ({yes*, no, unclear})\\
	Overall impression? ({1 .. 10})\\
	Additional comments? (can cover any metrics you feel are missing, or any other thoughts you have) \\
	\hline	
	\textbf{Surface Reliability}\\
	\hline
	Did the software “break” during installation? ({yes∗ , no})\\
	If the software installation broke, was the installation instance recoverable? ({yes, no, n/a})\\
	Did the software “break” during the initial tutorial testing? ({yes∗, no, n/a})\\
	If the tutorial testing broke, was a descriptive error message displayed? ({yes, no, n/a})\\
	If the tutorial testing broke, was the tutorial testing instance recoverable? ({yes, no, n/a})\\
	Overall impression? ({1 .. 10})\\
	Additional comments? (can cover any metrics you feel are missing, or any other thoughts you have)\\
	\hline		
\end{tabular}

\def\arraystretch{1.4}
\begin{tabular}{p{14cm}}
	\hline		
	\textbf{Surface Robustness}\\
	\hline
	Does the software handle unexpected/unanticipated input (like data of the wrong type, empty input, missing files or links) reasonably? (a reasonable response can include an appropriate error message.) ({yes, no∗ })\\
	For any plain text input files, if all new lines are replaced with new lines and carriage returns, will the software handle this gracefully? ({yes, no∗, n/a})\\
	Overall impression? ({1 .. 10})\\
	Additional comments? (can cover any metrics you feel are missing, or any other thoughts you have)\\
	\hline		
	\textbf{Surface Usability}\\
	\hline
	Is there a getting started tutorial? ({yes, no})\\
	Is there a user manual? ({yes, no})\\
	Are expected user characteristics documented? ({yes, no})\\
	What is the user support model? FAQ? User forum? E-mail address to direct questions? Etc. (string)\\
	Overall impression? ({1 .. 10})\\
	Additional comments? (can cover any metrics you feel are missing, or any other thoughts you have)\\
	\hline
\end{tabular}

\def\arraystretch{1.4}
\begin{tabular}{p{14cm}}
		\hline	
	\textbf{Maintainability}\\
	\hline
	What is the current version number? (number)\\
	Is there any information on how code is reviewed, or how to contribute? ({yes*, no})\\
	Are artifacts available? (List every type of file that is not a code file – for examples please look at the ‘Artifact Name’ column of https://gitlab.cas.mcmaster.ca/SEforSC/se4sc/-/blob/git-svn/GradStudents/Olu/ResearchProposal/Artifacts\_MiningV3.xlsx) ({yes*, no, unclear}) *list via string\\
	What issue tracking tool is employed? (set of {Trac, JIRA, Redmine, e-mail, discussion board, sourceforge, google code, git, BitBucket, none, unclear, other*}) * given via string\\
	What is the percentage of identified issues that are closed? (percentage)\\
	What percentage of code is comments? (percentage)\\
	Which version control system is in use? ({svn, cvs, git, github, unclear, other*}) * given via string\\
	Overall impression? ({1 .. 10})\\
	Additional comments? (can cover any metrics you feel are missing, or any other thoughts you have)\\
	\hline		
	\textbf{Reusability}\\
	\hline
	How many code files are there? (number)\\
	Is API documented? ({yes, no, n/a})\\
	Overall impression? ({1 .. 10})\\
	Additional comments? (can cover any metrics you feel are missing, or any other thoughts you have)\\
	\hline		
\end{tabular}

\def\arraystretch{1.4}
\begin{tabular}{p{14cm}}
		\hline		
	\textbf{Surface Understandability (Based on 10 random source files)}\\
	\hline
	Consistent indentation and formatting style? ({yes, no, n/a})\\
	Explicit identification of a coding standard? ({yes∗, no, n/a})\\
	Are the code identifiers consistent, distinctive, and meaningful? ({yes, no∗ , n/a})\\
	Are constants (other than 0 and 1) hard coded into the program? ({yes, no∗ , n/a})\\
	Comments are clear, indicate what is being done, not how? ({yes, no∗ , n/a})\\
	Is the name/URL of any algorithms used mentioned? ({yes, no∗ , n/a})\\
	Parameters are in the same order for all functions? ({yes, no∗ , n/a})\\
	Is code modularized? ({yes, no∗ , n/a})\\
	Overall impression? ({1 .. 10})\\
	Additional comments? (can cover any metrics you feel are missing, or any other thoughts you have)\\
	\hline		
	\textbf{Visibility/Transparency}\\
	\hline
	Is the development process defined? If yes, what process is used. ({yes∗, no, n/a})\\
	Are there any documents recording the development process and status?  ({yes∗, no}))\\
	Is the development environment documented? ({yes∗, no})\\
	Are there release notes? ({yes∗, no})\\
	Overall impression? ({1 .. 10})\\
	Additional comments? (can cover any metrics you feel are missing, or any other thoughts you have)\\
		\hline		
	\textbf{Raw Metrics (Measured via git\_stats)}\\
	\hline
	Number of text-based files. (number)\\
	Number of binary files. (number)\\
	Number of total lines in text-based files. (number)\\
	Number of total lines added to text-based files. (number)\\
	Number of total lines deleted from text-based files. (number)\\
	Number of total commits. (number)\\
	Numbers of commits by year in the last 5 years. (Count from as early as possible if the project is younger than 5 years.) (list of numbers)\\
	Numbers of commits by month in the last 12 months. (list of numbers)\\
	\hline
\end{tabular}

\def\arraystretch{1.4}
\begin{tabular}{p{14cm}}
		\hline		
	\textbf{Raw Metrics (Measured via scc)}\\
	\hline
	Number of text-based files. (number)\\
	Number of total lines in text-based files. (number)\\
	Number of code lines in text-based files. (number)\\
	Number of comment lines in text-based files. (number)\\
	Number of blank lines in text-based files. (number)\\
	\hline
	\textbf{Repo Metrics (Measured via GitHub)}\\
	\hline
	Number of stars. (number)\\
	Number of forks. (number)\\
	Number of people watching this repo. (number)\\
	Number of open pull requests. (number)\\
	Number of closed pull requests. (number)\\
	\hline
\end{tabular}

\newpage	

\section{Developer Interview Questions}\label{interviewquestions}

Information about these interview questions:  This gives you an idea what I would like to learn about the development of {domain} software. Interviews will be one-to-one and will be open-ended (not just “yes or no” answers). Because of this, the exact wording may change a little. Sometimes I will use other short questions to make sure I understand what you told me or if I need more information when we are talking such as: “So, you are saying that …?), to get more information (“Please tell me more?”), or to learn what you think or feel about something (“Why do you think that is…?”). 

\begin{enumerate}
	\item Interviewees’ current position/title? degrees?
	\item Interviewees’ contribution to/relationship with the software?
	\item Length of time the interviewee has been involved with this software?
	\item How large is the development group?
	\item Do you have a defined process for accepting new contributions into your team?
	\item What is the typical background of a developer?
	\item What is your estimated number of users? How did you come up with that estimate?
	\item What is the typical background of a user?
	\item Currently, what are the most significant obstacles in your development process?
	\item How might you change your development process to remove or reduce these obstacles?
	\item How does documentation fit into your development process? Would improved documentation help with the obstacles you typically face?
	\item In the past, is there any major obstacle to your development process that has been solved? How did you solve it?
	\item What is your software development model? For example, waterfall, agile, etc.
	\item What is your project management process? Do you think improving this process can
	tackle the current problem? Were any project management tools used?
	\item Was it hard to ensure the correctness of the software? If there were any obstacles, what methods have been considered or practiced to improve the situation? If practiced, did it work?
	\item When designing the software, did you consider the ease of future changes? For example, will it be hard to change the structure of the system, modules or code blocks? What measures have been taken to ensure the ease of future changes and maintains?
	\item Provide instances where users have misunderstood the software. What, if any, actions were taken to address understandability issues?
	\item What, if any, actions were taken to address usability issues?
	\item Do you think the current documentation can clearly convey all necessary knowledge to the users? If yes, how did you successfully achieve it? If no, what improvements are needed?
	\item Do you have any concern that your computational results won’t be reproducible in the future? Have you taken any steps to ensure reproducibility? 
	
\end{enumerate}

\newpage

\section{Grading Template}\label{gradingtemplate2}
The table below lists how each quality measure of the measurement template is used to calculate an overall impression in each software quality set.\newline

\captionof{table}{Grading Template}\label{gradingtemplate}
\def\arraystretch{1.5}
\begin{tabular}{p{14cm}}
	\hline		
	\textbf{Installability  (Measured via installation on a virtual machine.) }\\
	\hline
	Are there installation instructions? ({yes=1, no=-1})\\
	Are the installation instructions in one place? Place referring to a single document or web-page. ({yes=1, no=0, n/a=0})\\
	Are the installation instructions linear? Linear meaning progressing  in a single series of steps. ({yes=1, no=0, n/a=0})\\
	Are the instructions written as if the person doing the installation has none of the dependent packages installed? ({yes=1, no=0, unclear=0})\\
	Are compatible operating system versions listed? ({yes=1, no=0})\\
	Is there something in place to automate the installation (makefile, script, installer, etc)? ({yes*=1, no=-1})\\
	If the software installation broke, was a descriptive error message displayed? ({yes=0, no=-2, n/a=1})\\
	Is there a specified way to validate the installation? ({yes*=1, no=0})\\
	How many steps were involved in the installation? (Includes manual steps like unzipping files) Specify OS. ($<$10 = 1)\\
	What OS was used for the installation? (does not count)\\
	How many extra software packages need to be installed before or during installation? ($<$10 = 1)\\
	Are required package versions listed? ({yes=1, no=0, n/a=1})\\
	Are there instructions for the installation of required packages / dependencies? ({yes=1, no=0, n/a=1})\\
	Run uninstall, if available. Were any obvious problems caused? ({yes∗=0, no=1, unavail=1})\\
	Overall impression? (a sum of $>$10 is rounded down to 10)\\
	\hline
\end{tabular}

\def\arraystretch{1.33}
\begin{tabular}{p{14cm}}
	\hline		
	\textbf{Correctness and Verifiability}\\
	\hline
	Any reference to the requirements specifications of the program or theory manuals? ({yes∗=2, no=0, unclear=0})\\
	What tools or techniques are used to build confidence of correctness? (any=1, unclear=0)\\
	If there is a getting started tutorial? ({yes=2, no=0})\\
	Are the tutorial instructions linear? ({yes=1, no=0, n/a=0})\\
	Does the getting started tutorial provide an expected output? ({yes=1, no*=0, n/a=0})\\
	Does your tutorial output match the expected output? ({yes=1, no=0, n/a=0})\\
	Are unit tests available?  ({yes=1, no=0, unclear=0})\\
	Is there evidence of continuous integration? (for example mentioned in documentation, Jenkins, Travis CI, Bamboo, other) ({yes*=1, no=0, unclear=0})\\
	\hline	
	\textbf{Surface Reliability}\\
	\hline
	Did the software “break” during installation? ({yes∗=0, no=5})\\
	If the software installation broke, was the installation instance recoverable? ({yes=5, no=0, n/a=0})\\
	Did the software “break” during the initial tutorial testing? ({yes∗=0, no=5, n/a=0})\\
	If the tutorial testing broke, was a descriptive error message displayed? ({yes=2, no=0, n/a=0})\\
	If the tutorial testing broke, was the tutorial testing instance recoverable? ({yes=3, no=0, n/a=0})\\
	\hline		
	\textbf{Surface Robustness}\\
	\hline
	Does the software handle unexpected/unanticipated input (like data of the wrong type, empty input, missing files or links) reasonably? (a reasonable response can include an appropriate error message.) ({yes=5, no∗=0})\\
	For any plain text input files, if all new lines are replaced with new lines and carriage returns, will the software handle this gracefully? ({yes=5, no∗=0, n/a=5})\\
	\hline		
\end{tabular}

\def\arraystretch{1.4}
\begin{tabular}{p{14cm}}
		\hline		
	\textbf{Surface Usability}\\
	\hline
	Is there a getting started tutorial? ({yes=3, no=0})\\
	Is there a user manual? ({yes=4, no=0})\\
	Are expected user characteristics documented? ({yes=1, no=0})\\
	What is the user support model? FAQ? User forum? E-mail address to direct questions? Etc. (one=1, two+=2, none=0)\\
	\hline
	\textbf{Maintainability}\\
	\hline
	What is the current version number? (provided=1, nothing=0)\\
	Is there any information on how code is reviewed, or how to contribute? ({yes*=1, no=0})\\
	Are artifacts available? (List every type of file that is not a code file – for examples please look at the ‘Artifact Name’ column of https://gitlab.cas.mcmaster.ca/SEforSC/se4sc/-/blob/git-svn/GradStudents/Olu/ResearchProposal/Artifacts\_MiningV3.xlsx) (Rate 0 – 2 depending on how many and perceived quality)\\
	What issue tracking tool is employed? (nothing=0, email of other private=1, anything public or accessible by all devs (eg git) = 2)\\
	What is the percentage of identified issues that are closed? (50$\%$+=1, $<$50$\%$=0)\\
	What percentage of code is comments? (10$\%$+=1, $<$10$\%$=0)\\
	Which version control system is in use? (anything=2, nothing=0)\\
	\hline		
	\textbf{Reusability}\\
	\hline
	How many code files are there? (0-9=0, 10-49=1, 50-99=3, 100-299=4, 300-599=5, 600-999=6, 1000+=8)\\
	Is API documented? ({yes=2, no=0, n/a=0})\\
	\hline
\end{tabular}

\def\arraystretch{1.4}
\begin{tabular}{p{14cm}}
\hline	
\textbf{Surface Understandability (Based on 10 random source files)}\\
\hline
Consistent indentation and formatting style? ({yes=1, no=0, n/a=0})\\
Explicit identification of a coding standard? ({yes∗=1, no=0, n/a=0})\\
Are the code identifiers consistent, distinctive, and meaningful? ({yes=2, no∗=0, n/a=0})\\
Are constants (other than 0 and 1) hard coded into the program? ({yes=1, no∗=0, n/a=0})\\
Comments are clear, indicate what is being done, not how? ({yes=2, no∗=0, n/a=0})\\
Is the name/URL of any algorithms used mentioned? ({yes=1, no∗=0, n/a=0})\\
Parameters are in the same order for all functions? ({yes=1, no∗=0, n/a=0})\\
Is code modularized? ({yes=1, no∗=0, n/a=0})\\
\hline		
\textbf{Visibility/Transparency}\\
\hline
Is the development process defined? If yes, what process is used. ({yes∗=3, no=0, n/a=0})\\
Are there any documents recording the development process and status?  ({yes∗=3, no=0}))\\
Is the development environment documented? ({yes∗=2, no=0})\\
Are there release notes? ({yes∗=2, no=0})\\
\hline
\end{tabular}

\newpage

\section{Eliminated Software Packages}\label{eliminatedpackagessection}

The table below lists the software packages that were filtered out of the candidate software list. 

\begin{center}
	\begin{tabular}{ p{4cm}p{4cm}}
		\hline
		Name & Reason(s) For Removal\\
		\hline
		ch4-project&Scope \& Usage\\
		CUDA-LBM-simulator&Scope \& Age\\
		elbe&Scope \& Usage\\
		eLBM&Usage\\
		firesim&Scope \& Usage \& Age\\
		fvLBM&Scope \& Usage \& Age\\
		JFlowSim&Scope \& Usage \& Age\\
		LatticeBoltzmann&Scope \& Usage\\
		LBM&Scope \& Usage \& Age\\
		LBM-Cplusplus&Scope \& Usage\\
		LBM-EP&Usage\\
		LBM\_MATLAB&Scope \& Usage \& Age\\
		LBSim&Scope \& Usage \& Age\\
		listLBM&Scope \& Usage \& Age\\
		loliverhennigh&Scope \& Usage \& Age\\
		openLBMflow&Scope \& Usage \& Age\\
		ParallelLbmCranfield&Usage \& Age\\
		PowerFLOW&Usage\\
		ProLB&Usage\\
		Taxila-LBM&Usage \& Age\\
		turbulent\_lbm\_multigpu&Usage \& Age\\
		wlb&Scope \& Usage \& Age\\
		\hline
	\end{tabular}
	\captionof{table}{Eliminated Software Packages}\label{eliminatedpackages}
\end{center}

\newpage

\section{Ethics Approval}\label{ethicsapproval}
This project received ethics clearance from the McMaster Research Ethics Board on February 20, 2021.\newline


Project Title: AIMSS - State of the Practice\newline


MREB\#: 5219

\end{singlespace}
\end{appendices}
\newpage
\begin{singlespace}
\bibliographystyle {plainnat}
\bibliography {References}

\end{singlespace}

\end{document}