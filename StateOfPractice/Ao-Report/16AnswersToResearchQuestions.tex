\chapter{Answers to Research Questions}
\label{ch_answers}

This section answers our research questions from Section \ref{sec_research_questions}.  Sections \ref{sec_rq_artifacts}-- \ref{sec_rq_comparison} summarize the answers to the six questions, respectively. Section \ref{sec_threats_to_validity} lists the threats to the validity of our research. The answers are based on our quality measurements in Section \ref{ch_results} and developer interviews in Section \ref{ch_interview}. We refer to these sections to avoid repetition, and organize the references in tables (e.g. Table \ref{tab_reference_rq1}).

\section{Artifacts in the Projects}
\label{sec_rq_artifacts}
\begin{description}
\item[RQ1.] What artifacts are present in current software projects?
\end{description}

We answer this question by examining the documentation, scanning the source code, and interviewing the developers of the projects. Table \ref{tab_reference_rq1} shows the sections and tables containing answers to this research question.

\begin{table}[H]
\centering
\begin{tabular}{ll}
\hline
Section or table & Description \\ \hline
Table \ref{tab_maintainability_docs} & Maintainability documents \\
Table \ref{tab_Visibility/Transparency_docs} & Visibility/transparency documents \\ \hline
\end{tabular}
\caption{\label{tab_reference_rq1}Sections and tables with answers to RQ1}
\end{table}

As mentioned in Section \ref{sec_grading_template}, we search for all the artifacts in a project when measuring \textit{maintainability}. The detailed records of the existing artifacts in the 29 MI projects are at \hyperlink{https://data.mendeley.com/datasets/k3pcdvdzj2/1}{https://data.mendeley.com/datasets/k3pcdvdzj2/1}. Table \ref{tab_artifacts_frequency} summarizes the frequency of the artifacts. This table also contains part of the answers to \hyperlink{rq3}{RQ3}.

\begin{table}[H]
\centering
\begin{tabular}{ll}
\hline
Artifact & Number of projects \\ \hline
README & 29 \\
Version Control & 29 \\
License & 28 \\
Bug tracker & 28 \\
Change request & 28 \\
User Manual & 22 \\
Release notes & 22 \\
Build file & 18 \\
Tutorials & 18 \\
Installation Guide & 16 \\
Test cases & 15 \\
Authors & 14 \\
FAQ & 14 \\
Acknowledgements & 12 \\
Executable files & 10 \\
Developer's Manual & 8 \\
API documentation & 7 \\
Troubleshooting guide & 6 \\
Project Plan & 5 \\ \hline
\end{tabular}
\caption{\label{tab_artifacts_frequency}Artifacts by their frequency in the 29 MI projects}
\end{table}

We summarized the definitions of the artifacts in \hyperlink{https://github.com/smiths/AIMSS/blob/master/StateOfPractice/Methodology/Artifacts_MiningV3.xlsx}{https://github.com/.../Artifacts\_MiningV3.xlsx}. Source code is a type of artifact. Since we only included OSS on the final list (Section \ref{sec_mi_software_selection}), every project's source code was available. Thus, we excluded it in the above table.

\section{Tools in the Projects}
\label{sec_rq_tools}
\begin{description}
\item[RQ2.] What tools are used in the development of current software packages?
\end{description}

We answer this question by measuring the qualities and interviewing the developers. This section summarizes the tools used for CI/CD, user support, version control, documentation, contribution management, and project management. Table \ref{tab_reference_rq2} shows the sections and tables containing answers to this research question.

\begin{table}[H]
\centering
\begin{tabular}{ll}
\hline
Section or table & Description \\ \hline
Section \ref{sec_result_correctness_verifiability} & CI/CD tools \\
Table \ref{tab_user_support_model} & User support tools \\
Section \ref{sec_score_maintainability} & Version control tools \\
Table \ref{tab_doc_tools} & Documentation tools \\
Table \ref{tab_method_receive_contributions} & Contribution management tools \\
Table \ref{tab_pm_tools} & Project management tools\\ \hline
\end{tabular}
\caption{\label{tab_reference_rq2}Sections and tables with answers to RQ2}
\end{table}

As mentioned in Section \ref{sec_result_correctness_verifiability}, we identified five projects using CI/CD tools. \textit{3D Slicer} and \textit{OHIF Viewer} used CircleCI; \textit{ImageJ}, \textit{Fiji}, and \textit{dwv} used Travis. We identified the above projects and tools by examining the documentation and source code of all projects. Thus, we may missed some projects or tools. According to the interviews with developers, \textit{dwv} and \textit{Weasis} used GitHub Actions.

\section{Principles, Processes, and Methodologies in the Projects}
\label{sec_rq_PPM}
\begin{description}
\item[RQ3.] What principles, processes, and methodologies are used in the development of current software packages?
\end{description}

We answer this question by measuring the qualities and interviewing the developers. This section shows the principles, processes, and methodologies in software testing, documentation, contribution management, project management, and improving software qualities. Table \ref{tab_reference_rq3} shows the sections and tables containing answers to this research question.

\begin{table}[H]
\centering
\begin{tabular}{ll}
\hline
Section or table & Description \\ \hline
Section \ref{fg_correctness_erifiability_scores}, \ref{sec_pain_points_3}, \ref{sec_interview_correctness}, and \ref{sec_interview_reproducibility}; Table \ref{tab_q18_strategies_usability} & Software testing \\
Section \ref{fg_correctness_erifiability_scores} and \ref{sec_interview_documents}; Table \ref{tab_maintainability_docs}, \ref{tab_Visibility/Transparency_docs}, and \ref{tab_artifacts_frequency} & Documentation \\
Section \ref{sec_contribution_pm} & Contribution management \\
Section \ref{sec_contribution_pm} & Project management \\ \hline
\end{tabular}
\caption{\label{tab_reference_rq3}Sections and tables with answers to RQ3}
\end{table}

We identified the use of unit testing in less than half of the 29 projects. On the other hand, the interviewees believed that testing (including usability tests with users) was the top solution to improve \textit{correctness}, \textit{usability}, and \textit{reproducibility}. One pain point in the development process is the lack of access to real-world datasets for testing. The developers' strategies to address it is in Section \ref{sec_pain_points_3}. One threat to \textit{correctness} is: with huge datasets for testing, the tests are expensive and time-consuming. Three interviewees endorsed self tests / automated tests, which may save time for testing.

All 29 projects did not have theory manuals. We identified a road map in the \textit{3D Slicer} project, and no requirements specifications for the rest. Eight of the nine interviewees thought that documentation was essential to their projects. However, they hold the common opinion that their documentation needed improvements. Nearly half of them also believed that the lack of time prevented them from improving the documentation.

\section{Pain Points and Solutions}
\label{sec_rq_pain_points}
\begin{description}
\item[RQ4.] What are the pain points for developers working on research software projects? What aspects of the existing processes, methodologies, and tools do they consider as potentially needing improvement? What changes to processes, methodologies, and tools can improve software development and software quality?
\end{description}

We answer this question by interviewing the developers. The answers to this question, including the pain points and the solutions proposed by the developers, are in Section \ref{sec_interview_pain_points}.

\section{Software Qualities}
\label{sec_rq_qualities}
\begin{description}\item[RQ5.] What is the current status of the following software qualities for the projects? What actions have the developers taken to address them?\end{description}

This section includes our answers from the qualities measurements and interviews with the developers. Table \ref{tab_reference_rq4} shows the sections and tables containing answers to this research question.

\begin{table}[H]
\centering
\begin{tabular}{ll}
\hline
Section or table & Description \\ \hline
Section \ref{sec_result_installability} & Installability \\
Section \ref{sec_result_correctness_verifiability} and \ref{sec_interview_correctness} & Correctness \& verifiability \\
Section \ref{sec_result_reliability} & Reliability \\
Section \ref{sec_result_robustness} & Robustness \\
Section \ref{sec_result_usability} and \ref{sec_interview_usability}& Usability \\
Section \ref{sec_score_maintainability} and \ref{sec_interview_maintainability} & Maintainability \\
Section \ref{sec_result_reusability} & Reusability \\
Section \ref{sec_result_understandability} and \ref{sec_interview_understandability} & Understandability \\
Section \ref{sec_result_visibility_transparency} & Visibility/transparency \\
Section \ref{sec_interview_reproducibility} & Reproducibility \\ \hline
\end{tabular}
\caption{\label{tab_reference_rq4}Sections and tables with answers to RQ4}
\end{table}

\section{Our Ranking versus the Community Ratings}
\label{sec_rq_comparison}
\begin{description}
\item[RQ6.] How does software designated as high quality by this methodology compare with top-rated software by the community?
\end{description}

We answer this question by grading the qualities of the software, collecting ratings from the MI community, then conducting two comparisons:
\begin{itemize}
\item comparing our ranking with the community ratings on GitHub, such as GitHub stars, number of forks, and number of people watching the projects (Section \ref{sec_ranking_vs_github});
\item comparing top-rated software designated by our methodology with the ones recommended by our domain experts (Section \ref{sec_designation_vs_experts}).
\end{itemize}

\subsection{Our Ranking versus the GitHub Popularity}
\label{sec_ranking_vs_github}

Table \ref{tab_ranking_vs_GitHub} shows our ranking to the 29 MI projects, and their GitHub metrics if applicable. As mentioned in Section \ref{sec_score_maintainability}, 24 projects used GitHub. Since GitHub repositories have different creation dates, we collect the number of months each stayed on GitHub, and calculate the average number of new stars, people watching, and forks per 12 months. The method of getting the creation date is described in Section \ref{sec_empirical_measurements}, and we obtained these metrics in July, 2021.

In Table \ref{tab_ranking_vs_GitHub}, we used the average number of new stars per 12 months as an indicator of the GitHub popularity, and listed the items in the descending order of this number. We ordered the non-GitHub items by our ranking. Generally speaking, most of the top-ranking MI software projects also received greater attention and popularity on GitHub. Between our ranking and the GitHub stars-per-year ranking, four of the top five software projects are the same.

Project \textit{dwv} was popular on GitHub, but we ranked it low. As mentioned in Section \ref{sec_result_installability}, we failed to build it locally, and used the test version on its websites for the measurements. We followed the instructions and tried to run the command ``yarn run test" locally, which did not work. In addition, the test version did not detect a broken DICOM file and displayed a blank image as described in Section \ref{sec_result_robustness}. We might underestimate the scores for \textit{dwv} due to uncommon technical issues. We also ranked \textit{DICOM Viewer} much lower than its popularity. As mentioned in Section \ref{sec_result_installability} it depended on the NextCloud platform that we could not successfully install. Thus, we might underestimate the scores of its \textit{surface reliability} and \textit{surface robustness}. We weighted all qualities equally, which is not likely to be the case with all users. As a result, some projects with high popularity may not perform well in all qualities.

\begingroup
\renewcommand{\arraystretch}{0.85}
\begin{table}[H]
\centering
\begin{tabular}{lllll}
\hline
Software & Our ranking & Stars/yr & Watches/yr & Forks/yr \\ \hline
3D Slicer & 1 & 284 & 19 & 128 \\
OHIF Viewer & 3 & 277 & 19 & 224 \\
dwv & 23 & 124 & 12 & 51 \\
ImageJ & 2 & 84 & 9 & 30 \\
ParaView & 5 & 67 & 7 & 28 \\
Horos & 12 & 49 & 9 & 18 \\
Papaya & 17 & 45 & 5 & 20 \\
Fiji & 4 & 44 & 5 & 21 \\
DICOM Viewer & 29 & 43 & 6 & 9 \\
INVESALIUS 3 & 10 & 40 & 4 & 17 \\
Weasis & 6 & 36 & 5 & 19 \\
dicompyler & 26 & 35 & 5 & 14 \\
OsiriX Lite & 9 & 34 & 9 & 24 \\
MRIcroGL & 18 & 24 & 3 & 3 \\
GATE & 25 & 19 & 6 & 26 \\
Ginkgo CADx & 14 & 19 & 4 & 6 \\
BioImage Suite Web & 8 & 18 & 5 & 7 \\
Drishti & 27 & 16 & 4 & 4 \\
Slice:Drop & 20 & 10 & 2 & 5 \\
ITK-SNAP & 15 & 9 & 1 & 4 \\
medInria & 7 & 7 & 3 & 6 \\
SMILI & 13 & 3 & 1 & 2 \\
MatrixUser & 28 & 2 & 0 & 0 \\
MicroView & 16 & 1 & 1 & 1 \\
Gwyddion & 11 & n/a & n/a & n/a \\
XMedCon & 19 & n/a & n/a & n/a \\
DicomBrower & 21 & n/a & n/a & n/a \\
AMIDE & 22 & n/a & n/a & n/a \\
3DimViewer & 24 & n/a & n/a & n/a \\ \hline
\end{tabular}
\caption{\label{tab_ranking_vs_GitHub}Software ranking versus GitHub metrics}
\end{table}
\endgroup

\subsection{Designated Top Software versus the Domain Experts' Recommendation}
\label{sec_designation_vs_experts}

As shown in Section \ref{sec_mi_software_selection}, our domain experts recommended a list of top software with 12 software products. Table \ref{tab_top_software_vs_experts} and \ref{tab_experts_vs_top_software} compare the top 12 software projects ranked by our methodology with the ones from the domain experts.

All of the top 4 software from the domain experts are among the top 12 ones ranked by our methodology. 3 of the top 4 on both lists are the same ones: \textit{3D Slicer}, \textit{ImageJ}, and \textit{Fiji}. \textit{3D Slicer} is also the top 1 of both rankings.

As mentioned in Section \ref{sec_mi_software_selection}, six of the recommended software packages did not have visualization as the primary function, so we did not include them on our final list.

\begin{table}[H]
\centering
\begin{tabular}{lll}
\hline
Our ranking & Assessed software & Domain experts \\ \hline
1 & 3D Slicer & 1 \\
2 & ImageJ & 3 \\
3 & OHIF Viewer & n/a \\
4 & Fiji & 4 \\
5 & ParaView & n/a \\
6 & Weasis & n/a \\
7 & medInria & n/a \\
8 & BioImage Suite Web & n/a \\
9 & OsiriX Lite & n/a \\
10 & INVESALIUS 3 & n/a \\
11 & Gwyddion & n/a \\
12 & Horos & 2 \\ \hline
\end{tabular}
\caption{\label{tab_top_software_vs_experts}Top software by our ranking versus domain experts' recommendation}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{lll}
\hline
Our ranking & Recommended software & Domain experts \\ \hline
1 & 3D Slicer & 1 \\
12 & Horos & 2 \\
2 & ImageJ & 3 \\
4 & Fiji & 4 \\
n/a & AFNI & 5 \\
n/a & FSL & 6 \\
n/a & Freesurfer & 7 \\
18* & Mricron & 8 \\
17** & Mango & 9 \\
n/a & Tarquin & 10 \\
n/a & Diffusion Toolkit & 11 \\
n/a & MRItrix & 12 \\ \hline
\end{tabular}
\caption{\label{tab_experts_vs_top_software}Domain experts' recommendation versus our ranking}
\end{table}

* \textit{MRIcron} development had moved to \textit{MRIcroGL}, as mentioned in Section \ref{sec_mi_software_selection}. Thus, we measured and ranked \textit{MRIcroGL} at 18.

** We included \textit{Mango} in the initial list, but removed it because it was not OSS. \textit{Papaya} is a the web version OSS of \textit{Mango}. We measured and ranked \textit{Papaya} at 17.

\section{Threats to Validity}
\label{sec_threats_to_validity}

This section lists all the potential threats to validity in our research. The definitions of common validity are \cite{AmpatzoglouEtAl2019}\cite{ZhouEtAl2016}:
\begin{description}
\item[Construct Validity:] ``Deﬁnes how eﬀectively a test or experiment measures up to its claims. This aspect deals with whether or not the researcher measures what is intended to be measured" \cite{AmpatzoglouEtAl2019}.
\item[Internal Validity:] ``This aspect relates to the examination of causal relations. Internal validity examines whether an experimental treatment/condition makes a diﬀerence or not, and whether there is evidence to support the claim" \cite{AmpatzoglouEtAl2019}.
\item[External Validity:] ``Define the domain to which a study's findings can be
generalized" \cite{ZhouEtAl2016}.
\item[Conclusion Validity:] ``Demonstrate that the operations of a study such as the data collection procedure can be repeated, with the same results" \cite{ZhouEtAl2016}.
\end{description}

\noindent We categorize and present the threats to validity in the following subsections.

\subsection{Threats to Construct Validity}
\begin{itemize}
\item We compared nine software qualities for 29 software packages, so we could only spend a limited time on each of them. As a result, our assessments may have missed something relevant.
\item Our ranking is partly based on surface (shallow) measurement, which may not fully reveal the underlying qualities.
\end{itemize}

\subsection{Threats to Internal Validity}
\begin{itemize}
\item It was not practical to ask each development team for every piece of information. We collected much information - such as artifacts and funding situations of software - by ourselves. There may be cases that we missed some information.
\item As mentioned in Section \ref{ch_interview}, one interviewee was too busy to participate in a full interview, so he provided a version of written answers to us. Since we did not have the chance to explain our questions or ask him follow-up questions, there is a possibility of misinterpretation of the questions or answers.
\item As mentioned in Section \ref{sec_result_installability}, we could not install or build \textit{dwv}, \textit{GATE}, and \textit{DICOM Viewer}. We used a deployed online version for \textit{dwv}, a VM version for \textit{GATE}, but no alternative for \textit{DICOM Viewer}. We might underestimate their rank due to uncommon technical issues.
\end{itemize}

\subsection{Threats to External Validity}
\begin{itemize}
\item We interviewed eight teams, which is a good proportion of the 29. However, there is still a risk that they might not well represent the whole MI software community.
\item Our ranking gave all qualities equal weight, which may not be the case with all users. Thus, it may not represent the popularity of software among users.
\item The number of GitHub stars, watches, and forks are not perfect measures of popularity, but they are what we had available.
\end{itemize}

\subsection{Threats to Conclusion Validity}
\begin{itemize}
\item We used the grading template in Appendix \ref{ap_grading_template} to guide our measurements. Our impressions of the software - such as user experience - were factors in deciding some scores. Thus, there is a risk that some scores may be subjective and biased.
\end{itemize}
