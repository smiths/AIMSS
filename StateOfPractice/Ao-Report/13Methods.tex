\chapter{Methods}
\label{ch_methods}

We designed a general process for evaluating the state of the practice of domain-specific software, that we instantiate to SC software for specific scientific domains.

Our method involves: 1) choosing a software domain (Section \ref{sec_domain_selection}); 2) collecting and filtering software packages (Section \ref{sec_software_selection}); 3) grading the selected software (Section \ref{sec_grading_software}); 4) interviewing development teams for further information (Section \ref{sec_interview_methods}).

Details of how we applied the method on the MI domain are expanded upon in Section \ref{sec_applying_method}.

\section{Domain Selection}
\label{sec_domain_selection}
Our methods are generic, but our scope only included scientific domains due to the objective of our research, and thus we have only applied this method to scientific domains.

When choosing a candidate domain, we prefer a domain with a large number of active OSS. This is because we aim to finalize a list of 30 software packages \cite{SmithEtAl2021} in the domain after screening out the ones not meeting our requirements, such as the ones without recent updates or certain functions. Thus the quantity of final-decision packages may not meet our initial expectation if we do not have enough software candidates to choose from. Another reason is that our grading method works much better for OSS. Moreover, we need the domain to have an active community developing and using SC software, making it easier to conduct interviews with developers.

With an adequate number of software packages in a selected domain, we may still find the software products with various purposes and fall into different sub-categories. So we should ask one question to ourselves - do we prefer a group of software all providing similar functions and features, or do we aim to cross-compare several sub-sections within the same domain? With that answered, we can better determine a favored domain.

Another aspect to consider is the team carrying out the research, more specifically, the domain experts - if there is any - in the team. In our team, researchers in the software engineering field usually lead the projects, and experts working in scientific domains support them. Having domain experts in the team provides significant benefits in selecting software packages and designing interview questions.

\section{Software Product Selection}
\label{sec_software_selection}

The process of selecting software packages contains two steps: i) identify software candidates in the chosen domain, ii) filter the list according to needs \cite{SmithEtAl2021}.

\subsection{Identify Software Candidates}
\label{sec_identify_software_candidates}
We can find candidate software in publications of the domain. Another source is to search various websites, such as \hyperlink{https://github.com/}{GitHub}, \hyperlink{https://swmath.org/}{swMATH} and the Google search results for software recommendation articles. Meanwhile, we should also include the suggested ones from domain experts \cite{SmithEtAl2021}.

\subsection{Filter the Software List}
\label{sec_filter_software_list}
The goal is to build a software list with a length of about 30 \cite{SmithEtAl2021}.

The only ``mandatory'' requirement is that the software must be OSS, as defined in Section \ref{sec_open_source_software}. The reason is that to evaluate all aspects of some software qualities, the source code should be accessible.

The other factors to filter the list can be optional and we should consider them according to the number of software candidates and the objectives of the research project.

One of the factors is the functions and purposes of the software. For example, we can choose a group of software with similar functions, so that the cross-comparison is between each individual of them. On the other hand, if our objective is to compare sub-categories in the domain, we should select candidates from each of the categories.

The empirical measurement tools listed in Section \ref{sec_empirical_measurements} have limitations that we can only apply them on projects using \hyperlink{https://git-scm.com/}{Git} as the version control tool, so we prefer software with Git. Some manual steps in empirical measurement depend on a few metrics of GitHub, which makes projects held on GitHub more favored \cite{SmithEtAl2021}.

Some of the OSS projects may experience a lack of recent maintenance. So we can eliminate packages that have not been updated for a long time and unless they are still popular and highly recommended by the users in the domain \cite{SmithEtAl2021}.

No matter we set what standards to filter the packages, with domain experts in the team, we should value their opinions. For example, if a software package is not OSS and has not been updated for a long while, the domain experts may still identify it as a popular and widely used product. In this case, perhaps it is valuable to keep this software on the list.

\section{Grading Software}
\label{sec_grading_software}

We grade the selected software using a template (Section \ref{sec_grading_template}) and a specific empirical method (Section \ref{sec_empirical_measurements}). Some technical details for the measurements are in Section \ref{sec_technical_details}.

\subsection{Grading Template}
\label{sec_grading_template}
The full grading template can be found in Appendix \ref{ap_grading_template}. The template contains 101 questions that we use for grading software products.

We use the first section of the template to collect some general information about the software, such as the name, purpose, platform, programming language, publications about the software, the first release and the most recent change date, website, and source code repository of the product, etc. Information in this section helps us to understand the projects better and may be useful for further analysis, but it does not directly affect the grading scores for the packages.

We designed the next 9 sections in the template for the 9 software qualities mentioned in Section \ref{sec_software_quality}. For each quality, we ask several questions and the typical answers are among the choices of ``yes'', ``no'', ``n/a'', ``unclear'', a number, a string, a date, a set of strings, etc. The last question of each quality is asking for an overall score between 1 and 10 based on all the previous questions of this section, which is also the grading score for this quality. For some qualities, the empirical measurement sections on the template also affect this grading score.

All the last 3 sections on the template are about the empirical measurements. We use two command-line software tools git\_stats and scc to extract information about the source code from the project repositories. For projects held on GitHub, we manually collect additional metrics, such as the stars of the GitHub repository, and the numbers of open and closed pull requests. Section \ref{sec_empirical_measurements} presents more details of how these empirical measurements affect software quality grading scores.

\subsection{Empirical Measurements}
\label{sec_empirical_measurements}

We use two command-line tools for the empirical measurements. One is GitStats that generates statistics for git repositories and display outputs in the format of web pages \cite{Gieniusz2019}; the other one is Sloc Cloc and Code (as known as scc) \cite{Boyter2021}, aiming to count the lines of code, comments, etc.

Both tools can measure the number of text-based files in a git repository, as well as lines of text in these files. Based on our experience, most text-based files in a software project repository contain programming source code and developers use them to compile and build software products, and a minority of these files are instructions and other documents. So we roughly regard the lines of text in text-based files as lines of programming code. The two tools usually generate similar but not identical results as for the above measurements. From our understanding, this minor difference is because of the different techniques that they use to detect if a file is a text-based or binary file.

Additionally, we also manually collect some information for projects held on GitHub, such as the numbers of stars, forks, people watching this repository, open pull request, and closed pull request.

These empirical measurements help us from two aspects. Firstly, with more statistical details, we can get an overview of a project faster and more accurately. For example, the number of commits over the last 12 months shows how active this project has been during this period, and the number of stars and forks may reveal its popularity. Secondly, the results may affect our decisions regarding the grading scores for some software qualities. For example, if the percentage of comment lines is low, we might want to double-check the understandability of the code, and if the ratio of open versus closed pull requests is high, perhaps we should pay more attention to the maintainability of the project.

\subsection{Technical Details}
\label{sec_technical_details}
To test the software on a ``clean'' system, we created a new virtual machine (VM) for each software and only installed the necessary dependencies before measuring. All 29 VM were created on the same computer. We only start the measuring of the next software after finishing a current one, and after grading each software, the VM is destroyed.

Generally speaking, we spend about two hours grading one software, unless there are technical issues and more time is needed to resolve them. In most of the situation, we finish all the measurements for one software on the same day.

\section{Interview Methods}
\label{sec_interview_methods}

\subsection{Interviewee Selection}
For a software list with a length of roughly 30, we aim to interview about 10 development teams of these projects. Interviewing multiple individuals from each team should give us more comprehensive information about a project, but if there are difficulties in finding multiple willing participants, a single engineer well knowing the project can also be sufficient.

Ideally, we select projects after the grading measurements and prefer the projects with higher overall scores. However, if we do not find enough participants, we should also contact all teams on the list.

We try to find the contacts of the teams on the websites related to the software, such as the official web pages, repository websites, publication websites, and bio pages of the teams' institutions. For each candidate, we send at most two emails asking for their support and participation before receiving any replies.

\subsection{Interview Question Selection}

We have a list of 20 questions to guide our interviews with the development teams, which can be found in Appendix \ref{ap_interview}.

Some of the questions are about the background of the software, the development teams, the interviewees, and the way they organize the projects. We also ask about their understandings of the users. Another part of the interview questions focuses on the major difficulties the team experience currently or in the past, and the solutions that they have found or will try in the future. We also discuss with interviewees the importance of documents to their projects and the current situations of these documents. One more proportion of the questions are about several specific software qualities, such as maintainability, understandability, usability, and reproducibility.

The interviews are supposed to be semi-structured based on the question list and we also ask follow-up questions when necessary. Based on our experience, the interviewees usually bring up some interesting ideas that we do not expect and we can expand on these topics for a few more details.

\subsection{Interview Process}
Since the members of the development teams are likely to be based around the world, so we organize these interviews as virtual meetings online with \hyperlink{https://zoom.us/}{Zoom}. After receiving consent from the interviewees, we also record our discussions to better transcribe them.

\section{An Example of Applying the Method}
\label{sec_applying_method}

This section shows how we applied our method in Section \ref{ch_methods} on the Medical Imaging (MI) domain.

Based on the principles in Section \ref{sec_domain_selection}, we selected the MI domain, because there are numerous software products and a great number of professionals use and/or develop such software in this domain. We decided to focus on MI software with the viewing function, so we needed a large number of software candidates in the domain. Being able to include MI domain experts in our team also made this domain more preferred.

By using the method in Section \ref{sec_identify_software_candidates}, we identified 48 MI software projects as the candidates from publications \cite{Bjorn2017} \cite{Bruhschwein2019} \cite{Haak2015}, online articles related to the domain \cite{Emms2019} \cite{Hasan2020} \cite{Mu2019}, forum discussions related to the domain \cite{Samala2014}, etc.

Among the 48 software packages, there were several ones that we could not find their source code, such as MicroDicom, Aliza, and jivex, etc. These packages are likely to be freeware defined in Section \ref{sec_freeware} and not OSS. So following guidelines in Section \ref{sec_filter_software_list} we removed them from the list. We focused on the MI software that could work as a MI viewer. Some of the software on the list were tool kits or libraries for other software to use as dependencies but not for end-users to view medical images, such as VTK, ITK, and dcm4che, etc. We also eliminated these from the list. After that, there were 29 software products left on the list. We still preferred projects using git and GitHub and being updated recently, but did not apply another filtering since the number of packages was already below 30. However, 27 out of the 29 software packages on the final list used git, and 24 of which were held on GitHub. Furthermore, 27 packages had the latest updates after the year 2018, and 23 after 2020.

Then we followed the steps in Section \ref{sec_grading_software} to measure and grade the software. 27 out of the 29 packages can be installed on two or three different operating systems such as Windows, macOS, and Linux, and 5 of them are browser-based, making them platform-independent. However, in the interest of time, we only performed the measurements for each project by installing it on one of the platforms, most likely Windows.

Going through the interview process in Section \ref{sec_interview_methods}, we started with the teams with higher scores on our list, and eventually contacted all of them. There are developers/architects from 8 teams having participated in our interviews so far. Before contacting any interviewee candidate, we received ethics clearance from the McMaster University Research Ethics Board (Appendix \ref{ap_ethics}).
