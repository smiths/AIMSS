\chapter{Conclusions}
\label{ch_conclusions}

We analyzed the state of the practice for SC software in the MI domain. To better achieve our goals in Section \ref{ch_intro}, we proposed six research questions, that we list in Section \ref{sec_research_questions}.

Our methods in Section \ref{ch_methods} form a general process to evaluate domain-specific software, that we apply on specific SC domains. As mentioned in Section \ref{sec_applying_method}, following this process, we chose the MI domain, identified 48 SC software candidates in it, then selected 29 of them to our final list. Section \ref{ch_results} lists our measurements to nine software qualities for the 29 projects, and Section \ref{ch_interview} contains our interviews with eight of the 29 teams, discussing their development process and five software qualities.

With the measurement and interview results in Section \ref{ch_results} and \ref{ch_interview}, we reveal some current status of SC software development and qualities in the MI domain. The interview results also show some merits, drawbacks, and pain points within the development process. With these findings, we answer our six research questions in Section \ref{ch_answers}. In addition, Section \ref{ch_recommendations} presents our recommendations on SC software development, such as the ones on improving software qualities and easing pain points during development.

With learnings from this project, we summarized recommendations for the future state of the practice assessments:
\begin{itemize}
	\item we can further automate the measurements on the grading temple in Appendix \ref{ap_grading_template}. For example, with automation scripts and a GitHub API, we may save significant time on retrieving the GitHub metrics;
	\item the grading standard can be more explicit. For example, we can explicitly define scores for each item in the grading temple.
	\item we can improve some interview questions. For example, in question 14, ``Do you think improving this process can tackle the current problem?" is a yes-or-no question, which is not informative enough. As mentioned in Section \ref{sec_contribution_pm}, most interviewees ignored it. We can change it to ``By improving this process, what current problems can be tackled?"
	\item we can better organize the interview questions. Since we use audio conversion tools to transcribe the answers, we should aim to make the transcription easier to read. For example, we can order them together for questions about the five software qualities and compose a similar structure for each.
	\item we can mark the follow-up interview questions with keywords. For example, say ``this is a follow-up question" every time asking one. Thus, we record this sentence in the transcription, and it will be much easier to distinguish the follow-up questions from the 20 designed questions.
\end{itemize}
