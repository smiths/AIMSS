\documentclass{article}

\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{hyperref}
\hypersetup{%
	colorlinks = true,
	linkcolor  = black
}


\title{Project Notes - Quality Measurement}

\author{Peter Michalski}

\date{}


\begin{document}

\maketitle


\newpage

\tableofcontents
\addtocontents{toc}{\protect\thispagestyle{empty}}

~\newpage
\section{Variabilities}

\begin{enumerate}
	\item category
	\item dimensions
	\item computational model
	\item decomposition technique
	\item exception check
	\item language
	\item license
	\item parallel
	\item turbulent
	\item complex geometry
	\item non-Newtonian fluids
	\item dependencies
\end{enumerate}

~\newpage
\section{Quality Measurement}
\subsection{Robustness}
\subsubsection{Definition}
Software possesses the characteristic of robustness if it behaves ``reasonably'' in two situations: i) when it encounters circumstances not anticipated in the requirements specification; and ii) when the assumptions in its requirements specification are violated.

\subsubsection{Metrics}
Using CRASH criteria: (DOI: 10.1007/978-3-642-29032-9\_16)
\begin{enumerate}
	\item How does the system react when random but valid input is provided?\\
	
	Inject random input at interface (automated testing (selenium?) - generate list of valid input and observe if CRASH)
	\item How does the system react when invalid input is provided?\\
	
	Use invalid input at interface (testing - generate list of invalid input and observe if CRASH)
	\item How does the system react when required libraries are not provided?\\
	
	Not provide necessary libraries (find if external libraries are needed, not provide, observe if CRASH)
	\item How does the system react when directories and/or files are mutated?\\
	
	Removing directories (mutation) (for example removal of output directory - find directories for mutation, remove, observe if CRASH)
	
	\item How does the system react when run on OS and/or hardware that is outside of the requirement specification?\\
	
	Using OS and hardware outside of those in the reqs: (find req OS and hardware, make list outside of reqs, attempt to run the software and observe if CRASH)
\end{enumerate}
\subsubsection{Notes}
DOI: 10.1007/978-3-642-29032-9\_16:\\ 
The robustness failures are typically classified according to the CRASH criteria [540]: Catastrophic (the whole system crashes or reboots), Restart (the application has to be restarted), Abort (the application terminates abnormally), Silent (invalid operation is performed without error signal), and Hindering (incorrect error code is returned–note that returning a proper error code is considered as robust operation). The measure of robustness can be given as the ratio of test cases that exposes robustness faults, or, from the system’s point of view, as the number of robustness faults exposed by a given test suite.\\ 

~\newpage

\subsection{Performance}
\subsubsection{Definition}
The degree to which a system or component accomplishes its designated functions within given constraints, such as speed (database response times, for instance), throughput (transactions per second), capacity (concurrent usage loads), and timing (hard real-time demands).

\subsubsection{Metrics}

\begin{enumerate}
	\item How does the application compare in CPU utilization compared to other solutions running on the same hardware?\\
	
	Measure average CPU utilization of each application using the same hardware.
	\item What is the average memory usage of the application?\\
	
	Measure the average memory usage of the application using the same hardware.
	\item How many concurrent usage loads can the system handle?\\
	
	Run multiple instances of the application. 
	\item What is the average amount of time that the system needs to provide a solution?\\
	
	Measure the average amount of time that each application takes to provide a solution.
	\item How many errors does the system experience in X hours of run-time?\\
	
	Run automated tests and count the number of run-time errors.
	\item How accurate, on average, are the results of the solution?\\
	
	Measure the average error of the solution when compared to a pseudo oracle.
\end{enumerate}

\subsubsection{Notes}

~\newpage

\subsection{Maintainability}
\subsubsection{Definition}
The effort with which a software system or component can be modified to:

\begin{enumerate}
	\item correct faults
	\item improve performance or other
	attributes
	\item satisfy new requirements
\end{enumerate}

\subsubsection{Metrics}
\begin{enumerate}
	\item Open and closed issues on Git\\
	
	Count the ratio of open to closed issues on Git
	\item Average time to close issues\\
	
	Calculate the average time taken to close an issue on Git
	\item Update frequency\\
	
	Count average number of days between major update releases?
	\item Number of maintainers\\
	
	Count how many maintainers have worked on this project.\\
\end{enumerate}

Metrics for Assessing a Software System's Maintainability (Oman, Hagemeister):
\begin{enumerate}
\item Age\\

Count number of months since release.
\item Size\\

Count thousands of non commented source statements (TNCSS)
\item Stability 

Calculate: Stability = 1 - Change Factor when CF is ( e$\land$number of months * average percentage change of lines of code in number of months)
\item Defect Intensity\\

Calculate: Defect intensity =  e$\land$number of months * average percentage of defective lines of code per month
\item Subjective Product Appraisals\\

Measure: Subjective Product Appraisal = 5 point (very low to very high) for language complexity, application complexity, requirements volatility, product dependencies, complexity of build, installation complexity, intensity of product use, efficiency of the software system
\item Modularity\\

Measure: number of modules, average module size
\item Consistency\\

Measure: std deviation of module size (TNCSS)
\item Global Data Types\\

Measure: the number of global data types divided by the total number of defined data types
\item Global Data Structures\\

Measure: the number of global data structures divided by the total number of defined data structures
\item Data Type Consistency\\

Calculate: 1 - percentage of data structures that undergo type conversion during assignment operations
\item I/O Complexity\\

Measure: the number of lines of code devoted to I/O divided by TNCSS
\item Local Data Types\\

Calculate: the number of local data types divided by the total number of defined data types averaged over all modules
\item Local Data Types\\

Calculate: the number of local data types divided by the total number of defined data types averaged over all modules
\item Local Data Structures\\

Calculate: the number of local data structures divided by the total number of defined data structures averaged over all modules
\item Initialization Integrity\\

Calculate: percentage of variables initialized prior to use averaged over all modules
\item Overall Formatting\\

Calculate: percentage of blank lines in the whole program, percentage of modules with blank lines
\item Commenting\\

Calculate: percentage of comment lines in program, percentage of modules with header comments
\item Statement Formatting\\

Calculate: percentage of uncrowded statements (no more than one statement per line) per module averaged over all modules
\item Intramodule Commenting\\ 

Calculate: percentage lines of comments in module, averaged over all modules
\item Subjective Evaluation of Document Descriptiveness\\

Categorize based on accuracy, consistency, unambiguous
\item Subjective Evaluation of Document Completeness\\ 

Categorize based on extent of document set, contents
\item Subjective Evaluation of Document Correctness\\ 

Categorize based on traceability, verifiability
\item Subjective Evaluation of Document Readability\\

Categorize based on organization, accessibility via indices and table of contents, consistency of the writing style, typography, and comprehensibility
\item Subjective Evaluation of Document Modifiability\\

Categorize based on document set redundancies
\end{enumerate}
\subsubsection{Notes}


Metrics for Assessing a Software System's Maintainability (Oman, Hagemeister):\\
Software system metrics divided into 3 categories:\\

1. server maturity attributes (age since release, size, stability, maintenance intensity, defect intensity, reliability, reuse, subjective product appraisals)\\

2. source code (control structure, information structure, typography and naming and commenting) - each of these is broken into system and component subcategories\\

system control structure: modularity, complexity, consistency, nesting, control coupling, encapsulation, module reuse, control flow consistency\\

component control structure: complexity, use of structured constructs, use of unconditional branching, nesting, span of control structures, cohesion\\

system information structure: global data types, global data structures, system coupling, data flow consistency, data type consistency, nesting, I/O complexity, I/O integrity\\

component information structure: local data types, local data structures, data coupling, initialization integrity, span of data \\

system typography, naming and commenting: overall program formatting, overall program commenting, module separation, naming, symbols and case\\

component typography, naming and commenting: statement formatting, vertical spacing, horizontal spacing, intramodule commenting\\

3. supporting documentation (abstraction, physical attributes)\\
 
supporting documentation abstraction: descriptiveness appraisals, completeness appraisals, correctness appraisals\\

supporting documentation physical attributes: readability appraisals, modifiability appraisals\\

  
~\newpage

\subsection{Reusability}
\subsubsection{Definition}
The extent to which a software component can be used with or without adaptation in a problem solution other than the one for which it was originally developed.

\subsubsection{Metrics}
\noindent Poulin (non-direct source):
\begin{enumerate}
	\item Size\\
	
	Count lines of source code.
	\item Structure\\
	
	Count links to other modules.
	\item Documentation\\
	
	Subjective rating on a scale of 1 to 10.
	\item Language\\
	
	Subjective rating based on how common the language is.
	\item Simple interfaces\\
	
	Count the average number of passed variables to modules.
	\item Few calls to other modules\\
	
	Count the average number of calls to other modules.
	\item Cyclomatic complexity\\
	
	Measure cyclomatic complexity
	\item Self descriptiveness\\
	
	Measure average percentage of comment lines in module
	\item Number of problem reports\\
	
	Measure average number of outstanding defects in the module.
\end{enumerate}

\noindent Chidamber and Kemerer:
\begin{enumerate}
	\item Methods per class\\
	
	Measure the average number of methods defined in each class (want to keep low).
	\item Number of children\\
	
	Measure the number of immediate child classes derived from a base class (wan this to be high). 
	\item Coupling between object classes\\
	
	Measure the average number of methods or variables per class that are used by other classes (want to keep this low).
\end{enumerate}

\noindent Software reusability metrics estimation: Algorithms, models and optimization techniques (Padhy):\\ 
\begin{enumerate}
	\item Methods per class (WMC)\\
	
	Measure the number of methods per class.
	\item Depth of inheritance tree (DIT)\\
	
	Measure the highest length from the rood to the node.
	\item Number of children (NOC)\\
	
	Measure the average total number of total number of sub-classes.

	\item Method complexity (MC)\\
	
	Measure the average total number of methods present in a class.
\end{enumerate}

\noindent A new reusability metric for object-oriented software (Barnard):\\ 
\begin{enumerate}
	\item Coupling (calls to foreign classes)
	\item Number of methods
	\item Number of attributes
	\item Meaningful name
	\item Documentation
	\item Lines of code
	\item Comment lines
	\item Depth of inheritance
	\item Number of children
	\item Cohesion 
	\item Class variables
	\item Complexity of data structure
	\item Interface parameters
	\item Calls to foreign classes
	\item Calls to library classes
	\item Access (public, protected, private)
\end{enumerate}

\noindent Reusability Index: A Measure for Assessing Software Assets Reusability (Ampatzoglou):\\ 
\begin{enumerate}
	\item Number of classes in a module
	\item Number of properties
	\item Interface complexity
	\item Documentation quality
	\item Number of open bugs
\end{enumerate}


\subsubsection{Notes}
\noindent Measuring Software Reusability (Poulin)\\  

Taxonomy of reusability metrics:\\
1. Empirical methods:\\
Module oriented (complexity based, size based, reliability based), 
Component oriented\\

2.Qualitative methods:\\
Module oriented (style guidelines), Component oriented (certification guidelines, quality guidelines)\\

\noindent Chidamber and Kemerer object-oriented metrics (https://www.aivosto.com/project/help/pm-oo-ck.html):\\
The Chidamber and Kemerer metrics suite originally consists of 6 metrics calculated for each class: WMC, DIT, NOC, CBO, RFC and LCOM1. The original suite has later been amended by RFC´, LCOM2, LCOM3 and LCOM4 by other authors.\\ 

\noindent Software reusability metrics estimation: Algorithms, models and optimization techniques (Padhy)\\ 

\noindent A new reusability metric for object-oriented software (Barnard)\\ 

\noindent Reusability Index: A Measure for Assessing Software Assets Reusability (Ampatzoglou) 

~\newpage
\subsection{Portability}
\subsubsection{Definition}
Effort required to transfer a program between system environments (including hardware and software).\\
\subsubsection{Metrics}
Same as installability?
\begin{enumerate}
	\item OS platforms\\
	
	Measure the average time (or steps taken) to port the program between a list of OS environments.
	\item OS versions\\
	
	Measure the average time (or steps taken) to port the program between a list of OS versions.
	\item Hardware platforms\\
	
	Measure the average time (or steps taken) to port the program between a list of hardware platforms.
	
	\item Number of external libraries needed\\
	
	Count the number of external libraries needed to set up the environment.
	
\end{enumerate}

\subsubsection{Notes}
compare effort to port to effort to redevelop\\ 

determine if the system can be ported: ram, processor, resolution, OS, browser\\ 

\noindent Issues in the Specification and Measurement of Software Portability (Mooney):\\
The term portability
 refers to the ability of a software
 unit to be ported (to a given environment).
 A program is
 portable if and to the degree that the cost of porting is less than
 the cost of redevelopment. A software unit would be perfectly
 portable if it could be ported at zero cost, but this is never
 possible in practice.
 Instead, a software unit may be
 characterized by its degree ofportability,
 which is a function of
 the porting and development costs, with respect to a specific
 target environment.\\   
 
 Some of these issues have been examined by the author
 [Mooney 931. This work has proposed as a metric the degree of
 portability
 of a software unit with respect to a target
 environment, defined as
 DPfsu) = 1 - (Cport(su,q) I Crd$\&$req.e2)).\\
 This metric relates portability to a ratio between the cost
 of porting (which depends on the properties of the existing
 software unit and on the target environment), and the cost of
 redevelopment (which depends on the requirements and the
 target environment). A series of experiments is underway to
 refine and validate this metric and to determine how to measure
 or estimate Cpoyf and Crdev.
 One study by Sheets [94]
 suggests that the metric can be badly skewed by secondary
 elements such as inadequate documentation for the existing
 software.\\
  
  The principal types of portability usually considered are binary portability (porting the executable form) and source portability (porting the source language representation). Binary portability offers several advantages, but is possible only across strongly similar environments.Source portability assumes availability of source code, but provides opportunities to adapt a software unit to a wide range of environments.  In this paper we consider only source portability.The porting process has two principal components which we call transportation and adaptation.  Transportation is physical movement;  this may not be trivial since compatible media must be used and various types of representation conversion may be required.  Adaptation is any modification that must be performed on the original version;  we take this to mean both mechanical translation such as by language processors, and manual modification by humans\\ 

\noindent Designing a Measurement Method for the Portability Non-functional Requirement (Talib):\\
 
\noindent https://www.softwaretestinghelp.com/what-is-portability-testing/:\\

ISO 9126 breaks down portability testing: installability, compatability, adapatability, and replaceability.\\ 

check Installability, Adaptability, Replaceability, Compatibility or Coexistence\\

Installability: validate OS reqs, memory and RAM reqs, clear installation and uninstallation procedures, additional prerequisites\\

Adapatability: hardware and software dependency, language dependency and communication system\\



\end{document}