\documentclass[letterpaper,cleveref]{lipics-v2019}

\usepackage{natbib}
\usepackage{booktabs}
\usepackage{amsmath,amsthm}
\usepackage{hyperref}

\usepackage{hyperref}
\hypersetup{
colorlinks=true,       % false: boxed links; true: colored links
linkcolor=red,          % color of internal links (change box color with
%linkbordercolor)
citecolor=blue,       % color of links to bibliography
filecolor=magenta,   % color of file links
urlcolor=cyan           % color of external links
}

%% Comments
\newif\ifcomments\commentstrue

\ifcomments
\newcommand{\authornote}[3]{\textcolor{#1}{[#3 ---#2]}}
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}
\else
\newcommand{\authornote}[3]{}
\newcommand{\todo}[1]{}
\fi

\newcommand{\wss}[1]{\authornote{blue}{SS}{#1}} %Spencer Smith
\newcommand{\jc}[1]{\authornote{red}{JC}{#1}} %Jacques Carette
\newcommand{\oo}[1]{\authornote{magenta}{OO}{#1}} %Olu Owojaiye
\newcommand{\pmi}[1]{\authornote{green}{PM}{#1}} %Peter Michalski
\newcommand{\ad}[1]{\authornote{cyan}{AD}{#1}} %Ao Dong

\newcommand{\notdone}[1]{\textcolor{red}{#1}}
\newcommand{\done}[1]{\textcolor{black}{#1}}

%\oddsidemargin 0mm
%\evensidemargin 0mm
%\textwidth 160mm
%\textheight 200mm

\theoremstyle{definition}
\newtheorem{defn}{Definition}

\title{Methodology for Assessing the State of the Practice for Domain X} 
\author{Spencer Smith}{McMaster University, Canada}{smiths@mcmaster.ca}{}{}
\author{Jacques Carette}{McMaster University, Canada}{carette@mcmaster.ca}{}{}
\author{Olu Owojaiye}{McMaster University, Canada}{owojaiyo@mcmaster.ca}{}{}
\author{Peter Michalski}{McMaster University, Canada}{michap@mcmaster.ca}{}{}
\author{Ao Dong}{McMaster University, Canada}{donga9@mcmaster.ca}{}{}

\authorrunning{Smith et al.}  \Copyright{Spencer Smith and Jacques Carette and
Olu Owojaiye and Peter Michalski and Ao Dong}

\date{\today}

\hideLIPIcs
\nolinenumbers

\begin{document}
\maketitle

\begin{abstract}
	...
\end{abstract}

\tableofcontents

\section{Introduction} \label{SecIntroduction}

Purpose and scope of the document.  \wss{Needs to be filled in.  Should
	reference the overall research proposal, and the ``state of the practice''
	exercise in particular.  Reference questions we are trying to answer.}

\section{Research Questions}

In general questions:

\begin{enumerate}
\item Comparison between domains
\item How to measure qualities
\item How does the quality compare for projects with the most resources to those
  with the fewest?
\item What skills/knowledge are needed by future developers?
\item How can the development process be improved?
\item What are the common pain points?
\end{enumerate}

For each domain questions''

\begin{enumerate}
\item Best examples within the domain
\item What software artifacts?
\item What are the pain points?
\item Any advice on what can be done about the pain points?
\end{enumerate}

Measure the effort invested and the reward.  Related to sustainability.

Collect the data and see what conclusions follow.  For an individual domain,
between domains.  The process isn't so much about ranking the software as it is
about looking at the software closely and see what conclusions arise.  The
measurements are intended to force scrutiny, from different perspectives.

\section{Overview of Steps in Assessing Quality of the Domain Software}

\begin{enumerate}
\item Start with state of practice research questions.\pmi{add link to above section}
\item Identify the domain.  (Provide criteria on a candidate domain.) \pmi{as an example what criteria? Do we need to add a section below explaining this?}
\item \emph{Domain Expert}: Create a top ten list of software packages in the domain. \pmi{The details of this will be specified in the Meeting Agenda, so link to that section}
\item Brief the Domain Expert on the overall objective, research proposal, research questions, measurement template, survey for short list projects, usability tests, performance benchmarks, maintainability experiments.\pmi{Link this to Meeting Agenda, and create a small summary below; Reword or remove anything from this sentence?}
\item Identify broad list of candidate software packages in the domain. (Section~\ref{SecIdentifyCandSoft}) \pmi{To be completed}
\item Preliminary filter of software packages list. \pmi{Add section below; Judge on professionalism, available documentation, status, availability of source code}
\item \emph{Domain Expert}: Review domain software list. \pmi{any comments on this? Does this need a section below? Link to appropriate section of Meeting Agenda?}
\item Domain Analysis. (Section~\ref{SecDomainAnalysis})\pmi{To be completed}
\item \emph{Domain Expert}: Vet domain analysis.\pmi{This is currently not listed in Meeting Agenda and was part of the original Steps in Assessing Quality list. We need to add it to the Meeting Agenda and link it from here}
\item Gather source code and documentation for each prospective software package.
\item Collect Empirical measures. \pmi{To do: Link to below and edit the section}
\item Measure using ``shallow'' measurement template. \pmi{Add section below and link to actual template}
\item Use AHP process to rank the software packages. \pmi{Link to below section (which needs to be edited) outlining the tool we used, and link the below section to the AHP README file}
\item Identify a short list of five software packages for deeper exploration according to the AHP rankings of the shallow measurements.
\item \emph{Domain Expert}: vet AHP ranking and short list.\pmi{Currently not listed in Meeting Agenda, add it and link to it? ...or is this redundant considering they have made a top ten list and have already vetted the original prospective list (both of these latter Domain Expert actions are listed in the Meeting Agenda)?}
\item With short list:
\begin{enumerate}
\item Survey developers \pmi{To do: Link to survey; Add brief below?}
\item Usability experiments \pmi{Link to section below; link to any other document?}
\item Performance benchmarks\pmi{note: this is still in consideration}
\item Maintainability experiments\pmi{note: this is still in consideration}
\end{enumerate}
\item Rank short list using pairwise comparison of shortlist results.\pmi{Add section below? Link to another doc?}
\item Compare shallow and deep measures and ranking.\pmi{Add section below? Link to another doc? Anything specific to consider when comparing?}
\item Document answers for research questions.
\end{enumerate}

\wss{The domain expert is involved in multiple steps in the process.  How best
  to get their feedback?  The domain experts are busy and are unlikely to devote
  significant time to the project.  We need to quickly get to the point.  Maybe
  something around task based inspection?  Directed interview?}

\section{How to Identify Candidate Software} \label{SecIdentifyCandSoft}
\pmi{outline here how to find such a list}
\begin{enumerate}
\item Must have viewable source code.
\item Ideally have a git repository. \pmi{Not all of mine had this..}
\end{enumerate}

\section{Domain Analysis} \label{SecDomainAnalysis}

Commonality analysis.  Follow as for mesh generator (likely with less detail).
Final result will be tables of commonalities, variabilities and parameters of
variation.

Commonality analysis document Steps:
\begin{enumerate}
\item Introduction
\item Overview of Domain
\item Add Commonalities - Split into simulation, input, output, and
  nonfunctional requirements
\item Add Variabilities - Split into simulation, input, output, system
  constraints, and nonfunctional requirements
\item Add Parameters of Variation - Split into simulation, input, output, system
  constraints, and nonfunctional requirements
\item Add Terminology, Definitions, Acronyms
\end{enumerate}

Commonality analysis for Lattice Boltzmann Solvers can be found
\href{run:../Peter-Notes/Commonality-Analysis-LB-Systems.pdf}{here}.

\section{Empirical Measures}

\subsection{Raw Data}
Measures that can be extracted from on-line repos.

\ad{Still at brainstorm stage.}
\begin{itemize}
\item number of contributors
\item number of watches
\item number of stars
\item number of forks
\item number of clones
\item number of commits
\item number of total/code/document files
\item lines of total/logical/comment code
\item lines/pages of documents (can pdf be extracted?)
\item number of total/open/closed/merged pull requests
\item number of total/open/closed issues
\item number of total/open/closed issues with assignees
\end{itemize}

Instead of only focus on the current status of the above numbers, we may find
the time history of them to be more valuable. For example, the number of
contributors over time, the number of lines of code over time, the number of
open issues over time, etc.

\subsection{Processed Data}
Metrics that can be calculated from the raw data.

\ad{Still at brainstorm stage.}
\begin{itemize}
\item percentage of total/open/closed issues with assignees -
Visibility/Transparency
\item lines of new code produced per person-month - Productivity
\item lines/pages of new documents produced per person-month - Productivity
\item number of issues closed per person-month - Productivity
\item percentage of comment lines in the code - maintainability \ad{Not Ao's
qualities}
\end{itemize}

In the above calculations, a month can be determined to be 30 days.

\subsection{Tool Tests}
\ad{This section is currently a note of unorganized contents. Most parts will beremoved or relocated.}

\ad{This citation needs to be deleted later. It's here because my compiler
doesn't work with 0 citations}
\cite{Emms2019}

Most tests were done targeting to the repo of 3D Slicer
\href{https://github.com/tomgi/git_stats}{GitHub repo}

\subsubsection{git-stats}
\href{https://github.com/tomgi/git_stats}{GitHub repo}

Test results:
\href{http://git-stats-slicer.ao9.io/}{http://git-stats-slicer.ao9.io/} the
results are output as webpages, so I hosted for you to check. Data can be
downloaded as spreadsheets.

\subsubsection{git-of-theseus}
\href{https://github.com/erikbern/git-of-theseus}{GitHub repo}

Test results: It took about 100 minutes for one repo on a 8 core 16G ram Linux
machine. It only outputs graphs.

\subsubsection{hercules}
\href{https://github.com/src-d/hercules}{GitHub repo}

Test results: this one seems to be promising, but the installation is
complicated with various errors.

\subsubsection{git-repo-analysis}
\href{https://github.com/larsxschneider/git-repo-analysis}{GitHub repo}

\subsubsection{HubListener}
\href{https://github.com/pjmc-oliveira/HubListener}{GitHub repo}

The data that HubListener can extract.

Raw:
\begin{itemize}
\item Number of Files
\item Number of Lines
\item Number of Logical Lines
\item Number of Comments
\end{itemize}

Cyclomatic:
\href{https://www.geeksforgeeks.org/cyclomatic-complexity/}{Intro}
\begin{itemize}
\item Cyclomatic Complexity
\end{itemize}
 
Halstead:
\href{https://www.geeksforgeeks.org/software-engineering-halsteads-software-metrics/}{Intro}
\begin{itemize}
\item Halstead Effort
\item Halstead Bugs
\item Halstead Length
\item Halstead Difficulty
\item Halstead Time
\item Halstead Vocabulary
\item Halstead Volume
\end{itemize}

Test results: HubListener works well on the repo of itself, but it did not work
well on some other repos.

\subsubsection{gitinspector}
\href{https://github.com/ejwa/gitinspector}{GitHub repo}

Test results: it doesn't work well. Instead of creating output results, it
prints the results directly in the console.

\section{User Experiments}
\subsection{Usability Experimental Procedure}

\subsection {Procedure}
\begin {enumerate}
%\item Select a list of projects
%\item Write research questions
%\item Prepare logistic details(location, time table, recording setup, moderators etc)
%\item Prepare survey questions
%\item Design the tasks for the study subjects to perform (tasks can be defined based on user category, at least one task to modify the software)
%\item Select participants

\item Survey participants to collect pre-experiment data
\item Participants perform tasks
\item Observe the study subjects (take notes, record sessions(OBS screen recorder), watch out for body languages and verbal cues)
\item Survey the study subjects to collect feedback (post experiment interview)
\item Prepare experiment report
\item Perform pairwise comparison analysis
\item Prepare analysis report
\end {enumerate}


%\subsection{Nielsen’s Heuristics}
%\begin{enumerate}
%	\item Consistency: Check if standards and conventions in product design are followed
% \begin{itemize}
% 	\item Sequences of actions (skill acquisition).
% 	\item Color (categorization).
% 	\item Layout and position (spatial consistency).
% 	\item Font, capitalization (levels of organization).
% 	\item Terminology (delete, del, remove, rm) and language (words, phrases).
% 	\item Standards (e.g.,	blue underlined text for unvisited hyper links).
% \end{itemize}
%\item Visibility: Users should be informed about what is going on with the system through appropriate feedback and display of information e.g. 
% \begin{itemize}
% \item What is the current state of the system?
% \item What can be done at current state?
% \item Where can users go?
% \item What change is made after an action?
% \end{itemize}
%\item Match between system and world. The image of the system perceived by users should match the model the users have about the system. 
%\begin {itemize}
%\item User model matches system image.
%\item Actions provided by the system should match actions performed by users.
%\item Objects on the system should match objects of the task.
%\end {itemize}
%\item Minimalist: Any extraneous information is a distraction and a slow-down. 
%\begin{itemize}
%\item Less is more.
%\item Simple is not equivalent to abstract and general.
%\item Simple is efficient.
%\item  Progressive levels of detail.
%\end{itemize}
%\item Minimize memory load. Users should not be required to memorize a lot of information to carry out tasks. Memory load reduces users’ capacity to carry out the main tasks. 
%\begin{itemize}
%\item Recognition vs. recall (e.g., menu vs. commands).
%\item Externalize information through visualization.
%\item Perceptual procedures.
%\item Hierarchical structure.
%\item Default values.
%\item Concrete examples (DD/MM/YY, e.g., 10/20/99).
%\item Generic rules and actions (e.g., drag objects).
%\item Intuitive procedure
%\end{itemize}
%\item Informative feedback. Users should be given prompt and informative feedback about their actions. 
%\begin{itemize}
%\item Information that can be directly perceived, interpreted, and evaluated.
%\item Levels of feedback (novice and expert).
%\item Concrete and specific, not abstract and general.
%\item Response time:\\
%-0.1 s for instantaneously reacting;\\
%-1.0 s for uninterrupted flow of thought;\\
%-10 s for the limit of attention.
%\end{itemize}
%%\item Flexibility and efficiency. Users always learn and users are always different. Give users the flexibility of creating customization and shortcuts to accelerate their performance. 
%%\begin{itemize}
%%	\item Shortcuts for experienced users.
%%	\item Shortcuts or macros for frequently used operations.
%%	\item Skill acquisition through chunking.
%%	\item Examples:
%%	- Abbreviations, function keys, hot keys, command keys, macros, aliases, templates, type-ahead, bookmarks, hot links, history, default values, etc.
%%\end{itemize}
%\item Good error messages. 
%\begin {itemize}
%\item The messages should be informative enough such that users can understand the nature of errors, learn from errors, and recover from errors. 
%\item Phrased in clear language, avoid obscure codes. Example of obscure code: “system crashed, error code 147.”
%\item Precise, not vague or general. Example of general comment: “Cannot open document.”
%\item Constructive.
%\item Polite. Examples of impolite message: “illegal user action,” “job aborted,” “system was crashed,” “fatal error,” etc.
%\end {itemize}
%\item Prevent errors. It is always better to design interfaces that prevent errors from happening in the first place.
%\begin {itemize} 
%\item Interfaces that make errors impossible.
%\item Avoid modes (e.g., vi, text wrap). Or use informative feedback, e.g., different sounds.
%\item Execution error vs. evaluation error.
%\item Various types of slips and mistakes.
%\end {itemize}
%\item Clear closure. Every task has a beginning and an end. Users should be clearly notified about the completion of a task. 
%\begin {itemize}
%\item Clear beginning, middle, and end.
%\item Complete 7-stages of actions.
%\item Clear feedback to indicate goals are achieved and current stacks of goals can be released. Examples of good closures include many dialogues.
%\end {itemize}
%\item Reversible actions. 
%\begin {itemize}
%\item Users should be allowed to recover from errors. Reversible actions also encourage exploratory learning. 
%\item At different levels: a single action, a subtask, or a complete task.
%\item Multiple steps.
%\item Encourage exploratory learning.
%\item Prevent serious errors.
%\end {itemize}
%\item Use users’ language. The language should be always presented in a form understandable by the intended users.
%\begin{itemize}
%\item Use standard meanings of words.
%\item Specialized language for specialized group.
%\item User defined aliases.
%\item Users’ perspective. Example: “we have bought four tickets for you” (bad) vs. “you bought four tickets” (good).
%\end {itemize}
%\item Help and documentation. Always provide help when needed. 
%\begin {itemize}
%\item Context-sensitive help.
%\item Four types of help.\\
%-task-oriented;\\
%-alphabetically ordered;\\
%-semantically organized;\\
%-search.
%\item Help embedded in contents.
%\end {itemize}
%\item Users in control. Do not give users that impression that they are controlled by the systems. 
%\begin {itemize}
%\item Users are initiators of actors, not responders to actions.
%\item Avoid surprising actions, unexpected outcomes, tedious sequences of actions, etc.
%\end {itemize}
%\end{enumerate}

\subsection{Task selection criteria}
**The task selection will be determined with the aid of the domain expert attached to any of the selected projects.\\ 
**The domain expert will be asked to consider the below criteria when defining a task. \\
**Domain experts will also be asked to identify what background knowledge is necessary for the suggested tasks - Novice, Intermediate, Advanced
\begin {enumerate}
\item Collectively all tasks should not take no more than 2 hours.
\item Selected tasks should reflect common use cases of  the software.
\item Include tasks that require a set of sequential or hierarchical steps to be completed 

\end {enumerate} 


\subsection{Usability Questionnaire}
Two sources of standardized usability questionnaire we could use.

- \url{https://www.usabilitest.com/sus-pdf-generator}- 20-29 - SUS.\\
- \url{https://uiuxtrend.com/pssuq-post-study-system-usability-questionnaire/} - PSSUQ


\section{Analytic Hierarchy Process}

Describe process.  Domain expert review.

\section{Quality Specific Measures}

\subsection{\notdone{Installability} \oo{owner}}

\subsection{\notdone{Correctness} \oo{owner}}

\subsection{\notdone{Verifiability/Testability} \oo{owner}}

\subsection{\notdone{Validatability} \oo{owner}}

\subsection{\notdone{Reliability} \oo{owner}}

\subsection{\notdone{Robustness} \pmi{owner}}

\subsection{\notdone{Performance} \pmi{owner}}

\subsection{\notdone{Usability} \jc{owner}} 

\subsection{\notdone{Maintainability} \pmi{owner}}

\subsection{\notdone{Reusability} \pmi{owner}}

\subsection{\notdone{Portability} \pmi{owner}}

\subsection{\notdone{Understandability} \jc{owner}}

\subsection{\notdone{Interoperability} \ad{owner}}

\subsection{\notdone{Visibility/Transparency} \ad{owner}}

\subsection{\notdone{Reproducibility} \wss{owner}}

\subsection{\notdone{Productivity} \ad{owner}}

\subsection{\notdone{Sustainability} \wss{owner}}

\subsection{\notdone{Completeness} \ad{owner}}

\subsection{\notdone{Consistency} \ad{owner}}

\subsection{\notdone{Modifiability} \jc{owner}}

\subsection{\notdone{Traceability} \jc{owner}}

\subsection{\notdone{Unambiguity} \wss{owner}}

\subsection{\notdone{Verifiability} \wss{owner}}

\subsection{\notdone{Abstract} \wss{owner}}

\section{Using Data to Rank Family Members}

Describe AHP process (or similar).

\appendix
\section{Appendix}
\subsection{Survey for the Selected Projects}
\ad{Several questions are borrowed from \href{https://gitlab.cas.mcmaster.ca/smiths/pub/-/blob/master/Jegatheesan2016.pdf}{Jegatheesan2016}, and needed to be cited later.}
\subsubsection{Information about the developers and users}
\begin{enumerate}
\item Interviewees' current position/title? degrees?
\item Interviewees' contribution to/relationship with the software?
\item Length of time the interviewee has been involved with this software?
\item How large is the development group?
\item What is the typical background of a developer?
\item How large is the user group?
\item What is the typical background of a user?
\end{enumerate}

\subsubsection{Information about the software}

\begin{enumerate}
\item \ad{General} What is the most important software quality(ies) to your work? (set of selected qualities plus "else")
\item \ad{General} Are there any examples where the documentation helped? If yes, how it helped. ({yes$^*$, no})
\item \ad{General} Is there any documentation you feel you should produce and do not? If yes, what is it and why? ({yes$^*$, no})
\item \ad{Completeness} Do you address any of your quality concerns using documentation? If yes, what are the qualities and the documents. ({yes$^*$, no})
\item \ad{Visibility/Transparency} Is there a certain type of development methodologies used during the development? (\{Waterfall, Scrum, Kanban, else\})
\item \ad{Visibility/Transparency} Is there a clearly defined development process? If yes, what is it. (\{yes$^*$, no\})
\item \ad{Visibility/Transparency} Are there any project management tools used during the development? If yes, what are they. (\{yes$^*$, no\})
\item \ad{Visibility/Transparency} Going forward, will your approach to documentation of requirements and design
change? If not, why not. (\{yes, no$^*$\})
\item \ad{Correctness and Verifiability} During the process of development, what tools or techniques are used to build confidence of correctness? (string)
\item \ad{Correctness and Verifiability} Do you use any tools to support testing? If yes, what are they. (e.g. unit testing tools, regression testing suites) (\{yes$^*$, no\})
\item \ad{Correctness and Verifiability} Is there any document about the requirements specifications of the program? If yes, what is it. (\{yes$^*$, no\})
\item \ad{Portability} Do you think that portability has been achieved? If yes, how? (\{yes$^*$, no\})
\item \ad{Maintainability} How was maintainability considered in the design? (string)
\item \ad{Maintainability} What is the maintenance type? (set of \{corrective, adaptive, perfective,
unclear\})
\item \ad{Reusability} How was reusability considered in the design? (string)
\item \ad{Reusability} Are any portions of the software used by another package? If yes, how they are used. ({yes$^*$, no})
\item \ad{Reproducibility} Is reproducibility important to you? ({yes$^*$, no})
\item \ad{Reproducibility} Do you use tools to help reproduce previous software results? If yes, what are they. (e.g. version control, configuration management) ({yes$^*$, no})
\item \ad{Completeness} Is any of the following documents used during the development? ({yes$^*$, no})
\item \ad{General} Will this experience influence how you develop software? Do you see yourself maintaining the same level of documentation, tool support as you go forward? (string)
\begin{itemize}
\item Module Guide
\item Module Interface Specification
\item Verification and Validation Plan
\item Verification and Validation Report
\end{itemize}
\end{enumerate}

\newpage

\bibliographystyle {plainnat}
\bibliography {../../CommonFiles/ResearchProposal}

\end{document}