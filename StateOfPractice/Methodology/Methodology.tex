\documentclass[letterpaper,cleveref]{lipics-v2019}

\usepackage{natbib}
\usepackage{booktabs}
\usepackage{amsmath,amsthm}
\usepackage{hyperref}

\usepackage{hyperref}
\hypersetup{
colorlinks=true,       % false: boxed links; true: colored links
linkcolor=red,          % color of internal links (change box color with
%linkbordercolor)
citecolor=blue,       % color of links to bibliography
filecolor=magenta,   % color of file links
urlcolor=cyan           % color of external links
}

%% Comments
\newif\ifcomments\commentstrue

\ifcomments
\newcommand{\authornote}[3]{\textcolor{#1}{[#3 ---#2]}}
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}
\else
\newcommand{\authornote}[3]{}
\newcommand{\todo}[1]{}
\fi

\newcommand{\wss}[1]{\authornote{blue}{SS}{#1}} %Spencer Smith
\newcommand{\jc}[1]{\authornote{red}{JC}{#1}} %Jacques Carette
\newcommand{\oo}[1]{\authornote{magenta}{OO}{#1}} %Olu Owojaiye
\newcommand{\pmi}[1]{\authornote{green}{PM}{#1}} %Peter Michalski
\newcommand{\ad}[1]{\authornote{cyan}{AD}{#1}} %Ao Dong

\newcommand{\notdone}[1]{\textcolor{red}{#1}}
\newcommand{\done}[1]{\textcolor{black}{#1}}

%\oddsidemargin 0mm
%\evensidemargin 0mm
%\textwidth 160mm
%\textheight 200mm

\theoremstyle{definition}
\newtheorem{defn}{Definition}

\title{Methodology for Assessing the State of the Practice for Domain X} 
\author{Spencer Smith}{McMaster University, Canada}{smiths@mcmaster.ca}{}{}
\author{Jacques Carette}{McMaster University, Canada}{carette@mcmaster.ca}{}{}
\author{Olu Owojaiye}{McMaster University, Canada}{owojaiyo@mcmaster.ca}{}{}
\author{Peter Michalski}{McMaster University, Canada}{michap@mcmaster.ca}{}{}
\author{Ao Dong}{McMaster University, Canada}{donga9@mcmaster.ca}{}{}

\authorrunning{Smith et al.}  \Copyright{Spencer Smith and Jacques Carette and
Olu Owojaiye and Peter Michalski and Ao Dong}

\date{\today}

\hideLIPIcs
\nolinenumbers

\begin{document}
\maketitle

\begin{abstract}
	...
\end{abstract}

\tableofcontents

\section{Introduction} \label{SecIntroduction}

Purpose and scope of the document.  \wss{Needs to be filled in.  Should
	reference the overall research proposal, and the ``state of the practice''
	exercise in particular.  Reference questions we are trying to answer.}

\section{Research Questions}\label{ResearchQuestions}

In general questions:

\begin{enumerate}
\item Comparison between domains
\item How to measure qualities
\item How does the quality compare for projects with the most resources to those
  with the fewest?
\item What skills/knowledge are needed by future developers?
\item How can the development process be improved?
\item What are the common pain points?
\end{enumerate}

For each domain questions''

\begin{enumerate}
\item Best examples within the domain
\item What software artifacts?
\item What are the pain points?
\item Any advice on what can be done about the pain points?
\end{enumerate}

Measure the effort invested and the reward.  Related to sustainability.

Collect the data and see what conclusions follow.  For an individual domain,
between domains.  The process isn't so much about ranking the software as it is
about looking at the software closely and see what conclusions arise.  The
measurements are intended to force scrutiny, from different perspectives.

\section{Overview of Steps in Assessing Quality of the Domain Software}

\begin{enumerate}
\item Start with state of practice research questions. (Section~\ref{ResearchQuestions}) \pmi{To be completed}
\item Identify the domain. (Section~\ref{SecIdentifyDomain}) \pmi{To be reviewed}
\item \emph{Domain Experts}: Create a top ten list of software packages in the domain. (\href{run:Meeting Agenda with Domain Experts.pdf}{Meeting Agenda with Domain Experts})
\item Brief the Domain Experts on the overall objective, research proposal, research questions, measurement template, survey for short list projects, usability tests, performance benchmarks, maintainability experiments. (\href{run:Meeting Agenda with Domain Experts.pdf}{Meeting Agenda with Domain Experts})
\item Identify broad list of candidate software packages in the domain. (Section~\ref{SecIdentifyCandSoft}) \pmi{To be reviewed}
\item Preliminary filter of software packages list. (Section~\ref{SecInitialFilter}) \pmi{To be reviewed}
\item \emph{Domain Experts}: Review domain software list. (\href{run:Meeting Agenda with Domain Experts.pdf}{Meeting Agenda with Domain Experts})
\item Domain Analysis. (Section~\ref{SecDomainAnalysis})\pmi{To be completed}
\item \emph{Domain Experts}: Vet domain analysis. (\href{run:Meeting Agenda with Domain Experts.pdf}{Meeting Agenda with Domain Experts}) \pmi{This was part of the original Steps in Assessing Quality list. We need to add it to the Meeting Agenda}
\item Gather source code and documentation for each prospective software package.
\item Collect empirical measures. (Section~\ref{SecEmpiricalMeasures}) \pmi{To be completed - go over how to clean this up in meeting, or through an issue}
\item Measure using ``shallow'' measurement template. (Section~\ref{SecShallowMeasure}) \pmi{To be completed}
\item Use AHP process to rank the software packages. (Section~\ref{SecAHP}) \pmi{To be completed}
\item Identify a short list of top software packages, typically four to six, for deeper exploration according to the AHP rankings of the shallow measurements.
\item \emph{Domain Experts}: Vet AHP ranking and short list. (\href{run:Meeting Agenda with Domain Experts.pdf}{Meeting Agenda with Domain Experts}) \pmi{This was part of the original Steps in Assessing Quality list. We need to add it to the Meeting Agenda}
\item With short list:
\begin{enumerate}
\item Survey developers (\href{run:Questions to Developers.pdf}{Questions to Developers})
\item Usability experiments (\href{run:User Experiments.pdf}{User Experiments})
\item Performance benchmarks\pmi{note: this is still in consideration}
\item Maintainability experiments\pmi{note: this is still in consideration}
\end{enumerate}
\item Rank short list. (Section~\ref{SecRankShortList}) \pmi{To be completed}
\item Document answers for research questions.
\end{enumerate}

\wss{The domain expert is involved in multiple steps in the process.  How best
  to get their feedback?  The domain experts are busy and are unlikely to devote
  significant time to the project.  We need to quickly get to the point.  Maybe
  something around task based inspection?  Directed interview?}


\section{How to Identify the Domain} \label{SecIdentifyDomain}
\begin{enumerate}	
	\item The domain must fall within the research software scope.
	\item The domain must be specific.
	\item The domain must be well understood.
	\item There must be a community of people studying the domain.
	\item There must to be a variety of software solutions for the domain.
	\item These software solutions must have a user community.
\end{enumerate}


\section{How to Identify Candidate Software} \label{SecIdentifyCandSoft}
The candidate software can be found through search engine queries and domain related publications. The candidate software should have the following properties:
\begin{enumerate}
	\item Major function(s) must fall within the identified domain.
	\item Must have viewable source code.
	\item Ideally have a git repository or ability to gather emprical measures found in Section \ref{SecEmpiricalMeasures}.
\end{enumerate}

\section{How to Initially Filter the Software List} \label{SecInitialFilter}
The initial list of candidate software should be filtered using the following properties:
\begin{enumerate}
	\item Organization - The software and any related documentation should appear to be easy to gather and understand.
	\item Available documentation - The purpose of the software and the installation and usage procedures should appear to be moderately clear or easy to find.
	\item Status - The software cannot be marked as incomplete or in an initial development phase.
\end{enumerate}


Copies of both the initial and filtered lists should be kept for traceability purposes.

\section{Domain Analysis} \label{SecDomainAnalysis}

Commonality analysis.  Follow as for mesh generator (likely with less detail).
Final result will be tables of commonalities, variabilities and parameters of
variation.

Commonality analysis document Steps:
\begin{enumerate}
\item Introduction
\item Overview of Domain
\item Add Commonalities - Split into simulation, input, output, and
  nonfunctional requirements
\item Add Variabilities - Split into simulation, input, output, system
  constraints, and nonfunctional requirements
\item Add Parameters of Variation - Split into simulation, input, output, system
  constraints, and nonfunctional requirements
\item Add Terminology, Definitions, Acronyms
\end{enumerate}

Commonality analysis for Lattice Boltzmann Solvers can be found
\href{run:../Peter-Notes/Commonality-Analysis-LB-Systems.pdf}{here}.

\section{Empirical Measures} \label{SecEmpiricalMeasures}

\subsection{Raw Data}
Measures that can be extracted from on-line repos.

\ad{Still at brainstorm stage.}
\begin{itemize}
\item number of contributors
\item number of watches
\item number of stars
\item number of forks
\item number of clones
\item number of commits
\item number of total/code/document files
\item lines of total/logical/comment code
\item lines/pages of documents (can pdf be extracted?)
\item number of total/open/closed/merged pull requests
\item number of total/open/closed issues
\item number of total/open/closed issues with assignees
\end{itemize}

Instead of only focus on the current status of the above numbers, we may find
the time history of them to be more valuable. For example, the number of
contributors over time, the number of lines of code over time, the number of
open issues over time, etc.

\subsection{Processed Data}
Metrics that can be calculated from the raw data.

\ad{Still at brainstorm stage.}
\begin{itemize}
\item percentage of total/open/closed issues with assignees -
Visibility/Transparency
\item lines of new code produced per person-month - Productivity
\item lines/pages of new documents produced per person-month - Productivity
\item number of issues closed per person-month - Productivity
\item percentage of comment lines in the code - maintainability \ad{Not Ao's
qualities}
\end{itemize}

In the above calculations, a month can be determined to be 30 days.

\subsection{Tool Tests}
\ad{This section is currently a note of unorganized contents. Most parts will beremoved or relocated.}

\ad{This citation needs to be deleted later. It's here because my compiler
doesn't work with 0 citations}
\cite{Emms2019}

Most tests were done targeting to the repo of 3D Slicer
\href{https://github.com/tomgi/git_stats}{GitHub repo}

\subsubsection{git-stats}
\href{https://github.com/tomgi/git_stats}{GitHub repo}

Test results:
\href{http://git-stats-slicer.ao9.io/}{http://git-stats-slicer.ao9.io/} the
results are output as webpages, so I hosted for you to check. Data can be
downloaded as spreadsheets.

\subsubsection{git-of-theseus}
\href{https://github.com/erikbern/git-of-theseus}{GitHub repo}

Test results: It took about 100 minutes for one repo on a 8 core 16G ram Linux
machine. It only outputs graphs.

\subsubsection{hercules}
\href{https://github.com/src-d/hercules}{GitHub repo}

Test results: this one seems to be promising, but the installation is
complicated with various errors.

\subsubsection{git-repo-analysis}
\href{https://github.com/larsxschneider/git-repo-analysis}{GitHub repo}

\subsubsection{HubListener}
\href{https://github.com/pjmc-oliveira/HubListener}{GitHub repo}

The data that HubListener can extract.

Raw:
\begin{itemize}
\item Number of Files
\item Number of Lines
\item Number of Logical Lines
\item Number of Comments
\end{itemize}

Cyclomatic:
\href{https://www.geeksforgeeks.org/cyclomatic-complexity/}{Intro}
\begin{itemize}
\item Cyclomatic Complexity
\end{itemize}
 
Halstead:
\href{https://www.geeksforgeeks.org/software-engineering-halsteads-software-metrics/}{Intro}
\begin{itemize}
\item Halstead Effort
\item Halstead Bugs
\item Halstead Length
\item Halstead Difficulty
\item Halstead Time
\item Halstead Vocabulary
\item Halstead Volume
\end{itemize}

Test results: HubListener works well on the repo of itself, but it did not work
well on some other repos.

\subsubsection{gitinspector}
\href{https://github.com/ejwa/gitinspector}{GitHub repo}

Test results: it doesn't work well. Instead of creating output results, it
prints the results directly in the console.

\section{Measure Using Shallow Measurement Template} \label{SecShallowMeasure}
This step...
The measurement template can be found \href{run:Combined_MeasurementTemplate_EmpiricalMeasures.xlsx}{here}.
\begin{enumerate}
	\item Step 1..
\end{enumerate}

\section{Analytic Hierarchy Process} \label{SecAHP}

Describe process. Domain experts review. Outline the tool we used. The README file of the tool can be found \href{run:../AHP2020/LBM/README.txt}{here}.

\section{Rank Short List} \label{SecRankShortList}
Rank using pairwise comparison of short list results.
\begin{enumerate}
	\item Compare with respect to usability
	\item ...
\end{enumerate}

\section{Quality Specific Measures}

\subsection{\notdone{Installability} \oo{owner}}

\subsection{\notdone{Correctness} \oo{owner}}

\subsection{\notdone{Verifiability/Testability} \oo{owner}}

\subsection{\notdone{Validatability} \oo{owner}}

\subsection{\notdone{Reliability} \oo{owner}}

\subsection{\notdone{Robustness} \pmi{owner}}

\subsection{\notdone{Performance} \pmi{owner}}

\subsection{\notdone{Usability} \jc{owner}} 

\subsection{\notdone{Maintainability} \pmi{owner}}

\subsection{\notdone{Reusability} \pmi{owner}}

\subsection{\notdone{Portability} \pmi{owner}}

\subsection{\notdone{Understandability} \jc{owner}}

\subsection{\notdone{Interoperability} \ad{owner}}

\subsection{\notdone{Visibility/Transparency} \ad{owner}}

\subsection{\notdone{Reproducibility} \wss{owner}}

\subsection{\notdone{Productivity} \ad{owner}}

\subsection{\notdone{Sustainability} \wss{owner}}

\subsection{\notdone{Completeness} \ad{owner}}

\subsection{\notdone{Consistency} \ad{owner}}

\subsection{\notdone{Modifiability} \jc{owner}}

\subsection{\notdone{Traceability} \jc{owner}}

\subsection{\notdone{Unambiguity} \wss{owner}}

\subsection{\notdone{Verifiability} \wss{owner}}

\subsection{\notdone{Abstract} \wss{owner}}

\section{Using Data to Rank Family Members}

Describe AHP process (or similar).

\appendix
\section{Appendix}
\subsection{Survey for the Selected Projects}
\ad{Several questions are borrowed from \href{https://gitlab.cas.mcmaster.ca/smiths/pub/-/blob/master/Jegatheesan2016.pdf}{Jegatheesan2016}, and needed to be cited later.}
\subsubsection{Information about the developers and users}
\begin{enumerate}
\item Interviewees' current position/title? degrees?
\item Interviewees' contribution to/relationship with the software?
\item Length of time the interviewee has been involved with this software?
\item How large is the development group?
\item What is the typical background of a developer?
\item How large is the user group?
\item What is the typical background of a user?
\end{enumerate}

\subsubsection{Information about the software}

\begin{enumerate}
\item \ad{General} What is the most important software quality(ies) to your work? (set of selected qualities plus "else")
\item \ad{General} Are there any examples where the documentation helped? If yes, how it helped. ({yes$^*$, no})
\item \ad{General} Is there any documentation you feel you should produce and do not? If yes, what is it and why? ({yes$^*$, no})
\item \ad{Completeness} Do you address any of your quality concerns using documentation? If yes, what are the qualities and the documents. ({yes$^*$, no})
\item \ad{Visibility/Transparency} Is there a certain type of development methodologies used during the development? (\{Waterfall, Scrum, Kanban, else\})
\item \ad{Visibility/Transparency} Is there a clearly defined development process? If yes, what is it. (\{yes$^*$, no\})
\item \ad{Visibility/Transparency} Are there any project management tools used during the development? If yes, what are they. (\{yes$^*$, no\})
\item \ad{Visibility/Transparency} Going forward, will your approach to documentation of requirements and design
change? If not, why not. (\{yes, no$^*$\})
\item \ad{Correctness and Verifiability} During the process of development, what tools or techniques are used to build confidence of correctness? (string)
\item \ad{Correctness and Verifiability} Do you use any tools to support testing? If yes, what are they. (e.g. unit testing tools, regression testing suites) (\{yes$^*$, no\})
\item \ad{Correctness and Verifiability} Is there any document about the requirements specifications of the program? If yes, what is it. (\{yes$^*$, no\})
\item \ad{Portability} Do you think that portability has been achieved? If yes, how? (\{yes$^*$, no\})
\item \ad{Maintainability} How was maintainability considered in the design? (string)
\item \ad{Maintainability} What is the maintenance type? (set of \{corrective, adaptive, perfective,
unclear\})
\item \ad{Reusability} How was reusability considered in the design? (string)
\item \ad{Reusability} Are any portions of the software used by another package? If yes, how they are used. ({yes$^*$, no})
\item \ad{Reproducibility} Is reproducibility important to you? ({yes$^*$, no})
\item \ad{Reproducibility} Do you use tools to help reproduce previous software results? If yes, what are they. (e.g. version control, configuration management) ({yes$^*$, no})
\item \ad{Completeness} Is any of the following documents used during the development? ({yes$^*$, no})
\item \ad{General} Will this experience influence how you develop software? Do you see yourself maintaining the same level of documentation, tool support as you go forward? (string)
\begin{itemize}
\item Module Guide
\item Module Interface Specification
\item Verification and Validation Plan
\item Verification and Validation Report
\end{itemize}
\end{enumerate}

\newpage

\bibliographystyle {plainnat}
\bibliography {../../CommonFiles/ResearchProposal}

\end{document}